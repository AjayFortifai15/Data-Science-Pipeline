{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92eeb286-5fe4-4e52-9a91-46544d6baf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import joblib\n",
    "import logging\n",
    "from typing import List\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import logging\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "# Set options to show full DataFrame output\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "\t\t\tdatabase='baldota-dev-db',\n",
    "\t\t\tuser='fortifai_ng_user_ro',\n",
    "\t\t\tpassword='user@123!',\n",
    "\t\t\tport='5432',\n",
    "            sslmode=\"require\"\n",
    "\t\t)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "            SELECT table_name \n",
    "            FROM information_schema.tables \n",
    "        \"\"\")\n",
    "tables = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "\n",
    "line_item_keys = [\"DocumentNo\",\"InvItem\",\"Purch.Doc.\",\"Item\", \"Amount\", \"BlR\", \"Central Contract\", \"Central Contract Item\", \"CoCd\", \"D/C\", \"FIn\",\n",
    "    \"GR/IR Clrg\", \"Indicator for Differential Invoicing\", \"Material\", \"Plnt\",\n",
    "     \"OUn\", \"Quantity\", \"OPUn\", \"Qty in OPUn\", \"Reference\", \"SAA\", \"Supplier\",\n",
    "    \"Tax Jur.\", \"Year\", \"Year.1\"\n",
    "]\n",
    "line_item_values = [\"accounting_doc_no\",\"doc_line_item_no\",\"purch_doc_no\", \"purch_doc_item_no\",\"amt_doc_curr\", \"block_reason_field\", \"central_contract\",\n",
    "                    \"central_contract_item_no\",\"company_code\", \"debit_credit_flag\", \"final_inv_flag\",\n",
    "    \"ext_gr_ir_clrg_flag\", \"diff_invoicing_flag\", \n",
    "    \"material_no\", \"plant\", \"order_uom\", \"quantity\", \"po_uom\",\n",
    "    \"po_qty_order_uom\", \"ref_doc_no\", \"acct_assgnmt_seq_no\", \"vendor_or_creditor_acct_no\",\n",
    "    \"tax_jurisdiction_code\", \"fiscal_year\", \"ref_doc_fiscal_year\"\n",
    "]\n",
    "\n",
    "\n",
    "# Step 2: Create the dictionary\n",
    "my_dict_line_item = dict(zip(line_item_keys, line_item_values))\n",
    "\n",
    "# Step 3: Print the result\n",
    "print(\"Line items key with baldota: mapped values SAP fixed\",my_dict_line_item)\n",
    "\n",
    "header_keys= [\"Doc. No.\",\n",
    "    \"Bline Date\", \"CoCd\", \"Crcy\", \"Del.Costs\", \"Doc. Date\",  \"Doc.Header Text\",\n",
    "    \"Entry Dte\", \"Exch.rate\", \"G/L\", \"Gross Amnt\", \"I\", \"IV cat\", \"InR.Ref.no\", \"Inv. Pty\",\n",
    "    \"PBk\", \"PM\", \"PayT\", \"Payer\", \"Paymt Ref.\", \"Reference\", \"Rel.\", \"Rvrsd by\",\n",
    "    \"St\", \"TCode\", \"Time\", \"Type\", \"User Name\"\n",
    "]\n",
    "header_values=[\"accounting_doc_no\",\"baseline_date\", \"company_code\", \"currency\", \"unplanned_dlvry_costs\", \"doc_date\",\n",
    "     \"doc_header_text\", \"doc_entry_date\", \"exchange_rate\", \"gl_account\",\n",
    "    \"gross_inv_amt_doc_curr\", \"post_inv_flag\", \"logistics_inv_verif_orig_type\", \"txn_invoice_no\",\n",
    "    \"vendor_or_creditor_acct_no\", \"house_bank_short_key\", \"pymnt_method\", \"pymnt_terms\",\n",
    "    \"payee_or_payer_name\", \"assignment_no\", \"ref_doc_no\", \"sap_release\", \"reversal_doc_no\",\n",
    "    \"invoice_doc_status\", \"txn_code\", \"entry_time\", \"doc_type\", \"username\"\n",
    "]\n",
    "# Step 2: Create the dictionary\n",
    "my_header_dict = dict(zip(header_keys, header_values))\n",
    "\n",
    "# Step 3: Print the result\n",
    "print(\"Header key with baldota: mapped values SAP fixed\",my_header_dict)\n",
    "\n",
    "\n",
    "#tables = [row[0] for row in cur.fetchall()]\n",
    "#p2p_line_item_invoice_data\n",
    "for table in tables:\n",
    "    if table == 'invoice_receipt_items':\n",
    "        p2p_line_item_invoice_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{table}\", conn)\n",
    "        \n",
    "df_line_item_invoice_data=p2p_line_item_invoice_data.copy()\n",
    "# there are na values in accounting_doc_no--> dropping\n",
    "df_line_item_invoice_data= df_line_item_invoice_data.dropna(subset=['accounting_doc_no'])\n",
    "\n",
    "# convert to str to maintain\n",
    "df_line_item_invoice_data['accounting_doc_no'] = df_line_item_invoice_data['accounting_doc_no'].astype(float).astype('int64').astype(str)\n",
    "df_line_item_invoice_data['purch_doc_no'] = df_line_item_invoice_data['purch_doc_no'].astype(float).astype('int64').astype(str)\n",
    "\n",
    "### bring values for item number to orginal form, 10.0 to 00010 etc\n",
    "df_line_item_invoice_data['purch_doc_item_no'] = df_line_item_invoice_data['purch_doc_item_no'].fillna(0).astype(float).astype(int).astype(str).str.zfill(5)\n",
    "# convert doc_line_item_no to str\n",
    "df_line_item_invoice_data['doc_line_item_no'] = df_line_item_invoice_data['doc_line_item_no'].fillna(0).astype(float).astype(int).astype(str)\n",
    "\n",
    "\n",
    "\n",
    "##main data using the mapped columns from baldota\n",
    "df_line_item=df_line_item_invoice_data[line_item_values]\n",
    "## to merge with label data later on\n",
    "df_line_item['base_id']=df_line_item[\"accounting_doc_no\"] + \"^\" + df_line_item[\"doc_line_item_no\"]\n",
    "# adding src to line item data columns to distinguish with header column data\n",
    "df_line_item_renamed = df_line_item.rename(columns={col: f\"{col}_src\" for col in df_line_item.columns})\n",
    "\n",
    "\n",
    "#p2p_header_invoice_data\n",
    "for table in tables:\n",
    "    if table == 'invoice_receipt_header':\n",
    "        p2p_header_invoice_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{table}\", conn)\n",
    "df_header_invoice_data=p2p_header_invoice_data.copy()\n",
    "\n",
    "# there are na values in accounting_doc_no--> dropping\n",
    "df_header_invoice_data= df_header_invoice_data.dropna(subset=['accounting_doc_no'])\n",
    "\n",
    "# str maintain\n",
    "df_header_invoice_data['accounting_doc_no'] = df_header_invoice_data['accounting_doc_no'].astype(float).astype('int64').astype(str)\n",
    "## converting all column values dtype to string except for ingestion_timestamp :: will be updating dtype for values in code when those are required\n",
    "#df_header_po_data.loc[:, df_header_po_data.columns != 'ingestion_timestamp'] = df_header_po_data.loc[:, df_header_po_data.columns != 'ingestion_timestamp'].astype(str)\n",
    "##main data using the mapped columns from baldota\n",
    "df_header=df_header_invoice_data[header_values]\n",
    "# adding hpd to header data columns to distinguish with line_item column data\n",
    "df_header_renamed = df_header.rename(columns={col: f\"{col}_hpd\" for col in df_header.columns})\n",
    "\n",
    "merged_df_before_label=pd.merge(df_line_item_renamed,df_header_renamed,left_on='accounting_doc_no_src',right_on='accounting_doc_no_hpd',how='outer')\n",
    "merged_df_before_label.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## rule label output\n",
    "#line_item=pd.read_excel(\"baldota rule label data/Line Item Transaction Summary.xlsx\")\n",
    "line_item=pd.read_csv(\"baldota rule label data/Line Item Transaction Summary 24_07.csv\")\n",
    "## taking output po label at line item\n",
    "invoice_line_item_label=line_item[line_item['stage']=='Invoice']\n",
    "\n",
    "## merging main data df with label data df\n",
    "merged_df_after_label=pd.merge(merged_df_before_label,invoice_line_item_label[['base_id','rule_ids','rft_by_engine']],left_on='base_id_src',right_on='base_id',how=\"outer\")\n",
    "\n",
    "### adding false to rft_by_engine where rft by engine in null nan\n",
    "df=merged_df_after_label.copy()\n",
    "null_count = df['rft_by_engine'].isna().sum()\n",
    "none_str_count = (df['rft_by_engine'] == 'None').sum()\n",
    "\n",
    "total_missing = null_count + none_str_count\n",
    "print(f\"Total missing values in 'rft_by_engine': {total_missing}\")\n",
    "\n",
    "# Step 2: Replace NaN and 'None' with False\n",
    "df['rft_by_engine'] = df['rft_by_engine'].replace('None', pd.NA)\n",
    "df['rft_by_engine'] = df['rft_by_engine'].fillna(False)\n",
    "\n",
    "\n",
    "# Convert dates\n",
    "df[\"baseline_date_hpd\"] = pd.to_datetime(df[\"baseline_date_hpd\"], errors='coerce')\n",
    "df[\"doc_date_hpd\"] = pd.to_datetime(df[\"doc_date_hpd\"], errors='coerce')\n",
    "df[\"doc_entry_date_hpd\"] = pd.to_datetime(df[\"doc_entry_date_hpd\"], errors='coerce')\n",
    "\n",
    "# drop all rows where all values are Nan\n",
    "df = df.dropna(axis=1, how='all')\n",
    "#df.info()\n",
    "df['purch_doc_mapping']=df[\"purch_doc_no_src\"] + \"^\" + df[\"purch_doc_item_no_src\"]\n",
    "df_final = df.rename(columns={col: f\"{col}_invoice\" for col in df.columns})\n",
    "#df_final.info()\n",
    "\n",
    "## there are 175 invoice that have data in header but not in line item\n",
    "\n",
    "# Step 1: Get unique rule IDs from all rows (comma-separated strings)\n",
    "rule_sets = df_final['rule_ids_invoice'].dropna().apply(lambda x: [r.strip() for r in x.split(',')])\n",
    "unique_rules = sorted(set(r for sublist in rule_sets for r in sublist))\n",
    "\n",
    "# Step 2: Create columns for each rule ID with 1 if present, else 0\n",
    "for rule in unique_rules:\n",
    "    df_final[rule] = df_final['rule_ids_invoice'].apply(lambda x: int(rule in x.split(',')) if pd.notna(x) else 0)\n",
    "\n",
    "df_final.to_pickle(\"invoice_output.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FortifEnv",
   "language": "python",
   "name": "fortifenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
