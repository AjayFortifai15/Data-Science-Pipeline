{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16bf424-0086-42f8-8bb8-ad975f77de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import joblib\n",
    "import logging\n",
    "from typing import List\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import logging\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set options to show full DataFrame output\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import joblib, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from typing import Any, Dict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "\n",
    "def price_variance_data_cleaning(df_updated):\n",
    "    # ============================================\n",
    "    # Price Variance Metrics from markdown \"updated_evidence\"\n",
    "    # Handles: both sections present, one present, or none (No Risk)\n",
    "    # ============================================\n",
    "    \n",
    "    # ---------------- CONFIG ----------------\n",
    "    EVIDENCE_COL = \"updated_evidence_text\"\n",
    "    SLIGHT_THRESHOLD_PCT = 0.0   # |Δ%| < 3% → \"slightly above/below\"\n",
    "    ABOUT_SAME_PCT = 0.0         # |Δ%| < 0.5% → \"about the same\"\n",
    "    EPS = 0.0\n",
    "    \n",
    "    # ---------------- HELPERS ----------------\n",
    "    _keep_num = re.compile(r\"[^\\d\\.\\-]\")  # keep digits, dot, minus\n",
    "    \n",
    "    def to_float(s):\n",
    "        if s is None or (isinstance(s, float) and math.isnan(s)):\n",
    "            return np.nan\n",
    "        t = str(s).strip()\n",
    "        if not t:\n",
    "            return np.nan\n",
    "        t = _keep_num.sub(\"\", t)  # strip currency, commas, spaces\n",
    "        if t in (\"\", \"-\", \".\", \"-.\", \".-\"):\n",
    "            return np.nan\n",
    "        try:\n",
    "            return float(t)\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    def _strip_bold(cell):\n",
    "        c = str(cell).strip()\n",
    "        if c.startswith(\"**\") and c.endswith(\"**\"):\n",
    "            return c[2:-2].strip()\n",
    "        return c\n",
    "    \n",
    "    def _norm(s):\n",
    "        return re.sub(r\"\\s+\", \" \", str(s)).strip().lower()\n",
    "    \n",
    "    def _split_md_row(line):\n",
    "        # split a markdown row like: \"| a | b | c |\"\n",
    "        parts = [p.strip() for p in line.strip().split(\"|\")]\n",
    "        # first and last are empty due to leading/trailing pipes\n",
    "        parts = [p for p in parts if p != \"\"]\n",
    "        return [_strip_bold(p) for p in parts]\n",
    "    \n",
    "    def _find_section_lines(lines, startswith_any):\n",
    "        \"\"\"\n",
    "        Find the line index where a section header occurs (matches any string in startswith_any, case-insensitive).\n",
    "        Returns index or -1.\n",
    "        \"\"\"\n",
    "        candidates = [s.lower() for s in startswith_any]\n",
    "        for i, ln in enumerate(lines):\n",
    "            raw = _strip_bold(ln)\n",
    "            if any(_norm(raw).startswith(_norm(c)) for c in candidates):\n",
    "                return i\n",
    "        return -1\n",
    "    \n",
    "    def _collect_table_lines(lines, start_idx):\n",
    "        \"\"\"\n",
    "        From the line after start_idx, collect consecutive table lines that start with '|'.\n",
    "        Stops when a non-table line is encountered.\n",
    "        \"\"\"\n",
    "        tbl = []\n",
    "        i = start_idx + 1\n",
    "        while i < len(lines) and lines[i].lstrip().startswith(\"|\"):\n",
    "            tbl.append(lines[i])\n",
    "            i += 1\n",
    "        return tbl\n",
    "    \n",
    "    def _parse_md_table(table_lines):\n",
    "        \"\"\"\n",
    "        Parse a markdown table (as lines starting with '|') into a DataFrame.\n",
    "        Returns empty DataFrame if cannot parse.\n",
    "        \"\"\"\n",
    "        if not table_lines:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "        # Expect header, separator, then rows\n",
    "        # Find the first header line (has at least two columns)\n",
    "        header_idx = -1\n",
    "        for k, ln in enumerate(table_lines[:3]):  # header usually within first 3 lines\n",
    "            if \"|\" in ln:\n",
    "                cols = _split_md_row(ln)\n",
    "                if len(cols) >= 2:\n",
    "                    header_idx = k\n",
    "                    break\n",
    "        if header_idx == -1 or header_idx + 1 >= len(table_lines):\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "        header = _split_md_row(table_lines[header_idx])\n",
    "        # Skip separator row if present (---)\n",
    "        data_start = header_idx + 1\n",
    "        if re.search(r\"-{3,}\", table_lines[data_start]):\n",
    "            data_start += 1\n",
    "    \n",
    "        rows = []\n",
    "        for ln in table_lines[data_start:]:\n",
    "            if not ln.lstrip().startswith(\"|\"):\n",
    "                break\n",
    "            row = _split_md_row(ln)\n",
    "            if len(row) < len(header):\n",
    "                # pad if row is short\n",
    "                row = row + [\"\"] * (len(header) - len(row))\n",
    "            rows.append(row[:len(header)])\n",
    "    \n",
    "        if not rows:\n",
    "            return pd.DataFrame(columns=header)\n",
    "    \n",
    "        df = pd.DataFrame(rows, columns=header)\n",
    "    \n",
    "        # Normalize common column names\n",
    "        rename_map = {}\n",
    "        for c in df.columns:\n",
    "            lc = _norm(c)\n",
    "            if lc in (\"po → item\", \"po -> item\", \"po → item \", \"po item\"):\n",
    "                rename_map[c] = \"po_item\"\n",
    "            elif lc == \"vendor\":\n",
    "                rename_map[c] = \"vendor\"\n",
    "            elif lc in (\"price\", \"unit price\"):\n",
    "                rename_map[c] = \"price\"\n",
    "            elif lc in (\"qty\", \"quantity\"):\n",
    "                rename_map[c] = \"qty\"\n",
    "            elif \"Δ vs current\" in lc or \"delta vs current\" in lc or \"change vs current\" in lc:\n",
    "                rename_map[c] = \"delta_vs_current_pct\"\n",
    "            elif \"variance value/unit\" in lc:\n",
    "                rename_map[c] = \"variance_value_per_unit\"\n",
    "            elif lc == \"variance value\":\n",
    "                rename_map[c] = \"variance_value\"\n",
    "            elif lc == \"date\":\n",
    "                rename_map[c] = \"date\"\n",
    "            elif lc == \"material\":\n",
    "                rename_map[c] = \"material\"\n",
    "            elif lc == \"text\":\n",
    "                rename_map[c] = \"text\"\n",
    "            elif lc == \"pr\":\n",
    "                rename_map[c] = \"pr\"\n",
    "            elif \"deletion indicator\" in lc or lc == \"deletion\":\n",
    "                rename_map[c] = \"deletion_indicator\"\n",
    "        if rename_map:\n",
    "            df = df.rename(columns=rename_map)\n",
    "    \n",
    "        # Coerce numerics\n",
    "        if \"price\" in df.columns:\n",
    "            df[\"price_num\"] = df[\"price\"].map(to_float)\n",
    "        if \"qty\" in df.columns:\n",
    "            df[\"qty_num\"] = df[\"qty\"].map(to_float)\n",
    "        if \"variance_value_per_unit\" in df.columns:\n",
    "            df[\"variance_per_unit_num\"] = df[\"variance_value_per_unit\"].map(to_float)\n",
    "        if \"variance_value\" in df.columns:\n",
    "            df[\"variance_value_num\"] = df[\"variance_value\"].map(to_float)\n",
    "    \n",
    "        # Standardize deletion_indicator\n",
    "        if \"deletion_indicator\" in df.columns:\n",
    "            df[\"del_ind_norm\"] = df[\"deletion_indicator\"].fillna(\"\").astype(str).str.strip().str.upper()\n",
    "        else:\n",
    "            df[\"del_ind_norm\"] = \"\"\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    def _parse_section2_kv(md_lines):\n",
    "        \"\"\"\n",
    "        Parse Section 2 Key Description table (Field | Value) → dict\n",
    "        \"\"\"\n",
    "        # locate section 2 header\n",
    "        idx2 = _find_section_lines(\n",
    "            md_lines,\n",
    "            startswith_any=[\n",
    "                \"**2. Key Description\",     # exact from sample\n",
    "                \"2. Key Description\",       # fallback\n",
    "            ],\n",
    "        )\n",
    "        if idx2 == -1:\n",
    "            return {}\n",
    "    \n",
    "        tbl_lines = _collect_table_lines(md_lines, idx2)\n",
    "        df2 = _parse_md_table(tbl_lines)\n",
    "        if df2.empty or not set(df2.columns) >= {\"Field\", \"Value\"}:\n",
    "            return {}\n",
    "    \n",
    "        kv = {}\n",
    "        for _, r in df2.iterrows():\n",
    "            k = _strip_bold(r[\"Field\"])\n",
    "            v = r[\"Value\"]\n",
    "            kv[k] = v\n",
    "        return kv\n",
    "    \n",
    "    def _parse_section_A(md_lines):\n",
    "        idxA = _find_section_lines(\n",
    "            md_lines,\n",
    "            startswith_any=[\n",
    "                \"**A. Price Variance\", \"A. Price Variance\",\n",
    "                \"**A.\", \"A.\"\n",
    "            ],\n",
    "        )\n",
    "        if idxA == -1:\n",
    "            return pd.DataFrame()\n",
    "        tblA = _collect_table_lines(md_lines, idxA)\n",
    "        return _parse_md_table(tblA)\n",
    "    \n",
    "    def _parse_section_B(md_lines):\n",
    "        idxB = _find_section_lines(\n",
    "            md_lines,\n",
    "            startswith_any=[\n",
    "                \"**B. Price Variance\", \"B. Price Variance\",\n",
    "                \"**B.\", \"B.\"\n",
    "            ],\n",
    "        )\n",
    "        if idxB == -1:\n",
    "            return pd.DataFrame()\n",
    "        tblB = _collect_table_lines(md_lines, idxB)\n",
    "        return _parse_md_table(tblB)\n",
    "    \n",
    "    def _verdict(delta_pct):\n",
    "        if pd.isna(delta_pct):\n",
    "            return \"insufficient data\"\n",
    "        ap = abs(delta_pct)\n",
    "        if ap < ABOUT_SAME_PCT:\n",
    "            return \"about the same\"\n",
    "        if ap < SLIGHT_THRESHOLD_PCT:\n",
    "            return \"slightly higher\" if delta_pct > 0 else \"slightly lower\"\n",
    "        return \"higher\" if delta_pct > 0 else \"lower\"\n",
    "    \n",
    "    def _currency_prefix(s):\n",
    "        # best-effort: detect e.g. \"INR\" from a string like \"INR 85.03\"\n",
    "        if not isinstance(s, str):\n",
    "            return \"\"\n",
    "        m = re.match(r\"\\s*([A-Za-z]{3,})\\b\", s.strip())\n",
    "        return (m.group(1) + \" \") if m else \"\"\n",
    "    \n",
    "    def compute_price_variance_from_evidence(df, evidence_col=EVIDENCE_COL):\n",
    "        out_cols = [\n",
    "            \"current_qty\",\n",
    "            \"current_unit_price\",\n",
    "            \"currency_hint\",\n",
    "            \"avg_unit_price_same_vendor\",\n",
    "            \"avg_unit_price_cross_vendor\",\n",
    "            \"avg_unit_price_simple_all\",\n",
    "            \"n_rows_same_vendor\",\n",
    "            \"n_rows_cross_vendor\",\n",
    "            \"delta_per_unit\",\n",
    "            \"delta_pct\",\n",
    "            \"variance_value_total\",\n",
    "            \"verdict\",\n",
    "            \"status\",\n",
    "            \"used_sections\",\n",
    "        ]\n",
    "        for c in out_cols:\n",
    "            df[c] = np.nan\n",
    "    \n",
    "        df[\"verdict\"] = None\n",
    "        df[\"status\"] = None\n",
    "        df[\"used_sections\"] = None\n",
    "        df[\"currency_hint\"] = \"\"\n",
    "    \n",
    "        for idx, text in df[evidence_col].fillna(\"\").astype(str).items():\n",
    "            txt = text.strip()\n",
    "            if not txt:\n",
    "                df.at[idx, \"status\"] = \"missing evidence\"\n",
    "                continue\n",
    "    \n",
    "            # Fast path: No Risk message\n",
    "            if \"no risk for this line item\" in txt.lower():\n",
    "                df.at[idx, \"status\"] = \"no risk (as per evidence)\"\n",
    "                continue\n",
    "    \n",
    "            lines = [ln.rstrip(\"\\n\") for ln in txt.splitlines() if ln.strip() != \"\" or ln.startswith(\"|\")]\n",
    "    \n",
    "            # ---------- Section 2 (current qty & unit price; deletion indicator; currency) ----------\n",
    "            kv = _parse_section2_kv(lines)\n",
    "            if not kv:\n",
    "                df.at[idx, \"status\"] = \"missing section 2\"\n",
    "                # keep going—we might still get averages, but delta will be NaN without current price/qty\n",
    "    \n",
    "            # Deletion check for current line\n",
    "            cur_del = str(kv.get(\"Deletion Indicator\", \"\")).strip().upper() if kv else \"\"\n",
    "            if cur_del == \"L\":\n",
    "                df.at[idx, \"status\"] = \"current item deleted (L) — skipped\"\n",
    "                continue\n",
    "    \n",
    "            # Quantity / Unit & Unit Price e.g. \"490 L @ INR 85.03\"\n",
    "            cur_qty = np.nan\n",
    "            cur_unit_price = np.nan\n",
    "            cur_currency_hint = \"\"\n",
    "            if kv:\n",
    "                qpu = kv.get(\"Quantity / Unit & Unit Price\", \"\") or kv.get(\"Qty / Unit & Unit Price\", \"\")\n",
    "                if qpu:\n",
    "                    # try to capture quantity and unit price\n",
    "                    # patterns like \"490 L @ INR 85.03\" or \"490 @ 85.03\"\n",
    "                    m = re.search(r\"(?P<qty>[\\d,\\.]+)\\s*[A-Za-z]*\\s*@\\s*(?P<cur>.+)$\", qpu)\n",
    "                    if m:\n",
    "                        cur_qty = to_float(m.group(\"qty\"))\n",
    "                        cur_unit_price = to_float(m.group(\"cur\"))\n",
    "                        cur_currency_hint = _currency_prefix(m.group(\"cur\"))\n",
    "                # If not found, try to look into \"Unit Price\" field directly (if your format ever splits it)\n",
    "                if pd.isna(cur_unit_price):\n",
    "                    up = kv.get(\"Unit Price\", \"\")\n",
    "                    if up:\n",
    "                        cur_unit_price = to_float(up)\n",
    "                        cur_currency_hint = _currency_prefix(up)\n",
    "                # Quantity fallback\n",
    "                if pd.isna(cur_qty):\n",
    "                    q = kv.get(\"Quantity\", \"\")\n",
    "                    if q:\n",
    "                        cur_qty = to_float(q)\n",
    "    \n",
    "            df.at[idx, \"current_qty\"] = cur_qty\n",
    "            df.at[idx, \"current_unit_price\"] = cur_unit_price\n",
    "            df.at[idx, \"currency_hint\"] = cur_currency_hint\n",
    "    \n",
    "            # ---------- Section A & B (compared transactions) ----------\n",
    "            dfA = _parse_section_A(lines)\n",
    "            dfB = _parse_section_B(lines)\n",
    "    \n",
    "            # Filter out deletion flag L\n",
    "            def usable(dfX):\n",
    "                if dfX.empty:\n",
    "                    return dfX\n",
    "                good = dfX.copy()\n",
    "                good = good[(~good[\"price_num\"].isna()) & (~good[\"qty_num\"].isna()) & (good[\"qty_num\"] > 0)]\n",
    "                good = good[good[\"del_ind_norm\"].fillna(\"\") != \"L\"]\n",
    "                return good\n",
    "    \n",
    "            A_use = usable(dfA)\n",
    "            B_use = usable(dfB)\n",
    "    \n",
    "            # counts\n",
    "            nA, nB = len(A_use), len(B_use)\n",
    "            df.at[idx, \"n_rows_same_vendor\"] = nA\n",
    "            df.at[idx, \"n_rows_cross_vendor\"] = nB\n",
    "    \n",
    "            used_secs = []\n",
    "            if nA > 0: used_secs.append(\"A\")\n",
    "            if nB > 0: used_secs.append(\"B\")\n",
    "            df.at[idx, \"used_sections\"] = \",\".join(used_secs) if used_secs else \"none\"\n",
    "    \n",
    "            # Averages (per-section)\n",
    "            avgA = 0.0\n",
    "            if nA > 0:\n",
    "                avgA = (A_use[\"price_num\"] * A_use[\"qty_num\"]).sum() / A_use[\"qty_num\"].sum()\n",
    "            avgB = 0.0\n",
    "            if nB > 0:\n",
    "                avgB = (B_use[\"price_num\"] * B_use[\"qty_num\"]).sum() / B_use[\"qty_num\"].sum()\n",
    "            df.at[idx, \"avg_unit_price_same_vendor\"] = avgA\n",
    "            df.at[idx, \"avg_unit_price_cross_vendor\"] = avgB\n",
    "            \n",
    "            total_avg=0.0\n",
    "            ## Combined averages (weighted + simple) across whichever sections exist\n",
    "            #all_use = pd.concat([A_use, B_use], ignore_index=True) if (nA + nB) > 0 else pd.DataFrame()\n",
    "            #if not all_use.empty:\n",
    "                #w_avg = (all_use[\"price_num\"] * all_use[\"qty_num\"]).sum() / max(all_use[\"qty_num\"].sum(), EPS)\n",
    "                #s_avg = all_use[\"price_num\"].mean()\n",
    "                #df.at[idx, \"avg_unit_price_weighted_all\"] = w_avg\n",
    "                #df.at[idx, \"avg_unit_price_simple_all\"] = s_avg\n",
    "                #df.at[idx, \"n_rows_used\"] = len(all_use)\n",
    "            if avgA > 0.0 or avgB > 0.0:\n",
    "                total_avg = (avgA*nA + avgB*nB)/(nA+nB)\n",
    "                df.at[idx, \"avg_unit_price_simple_all\"] = total_avg\n",
    "            else:\n",
    "                df.at[idx, \"status\"] = (df.at[idx, \"status\"] or \"\") + \" | no comparison rows\"\n",
    "    \n",
    "            # ---------- Deltas & totals ----------\n",
    "            cur_p = df.at[idx, \"current_unit_price\"]\n",
    "            base_avg = df.at[idx, \"avg_unit_price_simple_all\"]  # prefer weighted average\n",
    "            if not pd.isna(cur_p) and not pd.isna(base_avg) and abs(base_avg) > EPS:\n",
    "                delta_per_unit = cur_p - base_avg\n",
    "                delta_pct = (delta_per_unit / base_avg) * 100.0\n",
    "                verdict = _verdict(delta_pct)\n",
    "                df.at[idx, \"delta_per_unit\"] = delta_per_unit\n",
    "                df.at[idx, \"delta_pct\"] = delta_pct\n",
    "                q = df.at[idx, \"current_qty\"]\n",
    "                if not pd.isna(q):\n",
    "                    df.at[idx, \"variance_value_total\"] = delta_per_unit * q\n",
    "                df.at[idx, \"verdict\"] = verdict\n",
    "                if not df.at[idx, \"status\"]:\n",
    "                    df.at[idx, \"status\"] = \"ok\"\n",
    "            else:\n",
    "                # If we can’t compute delta, mark status\n",
    "                if not df.at[idx, \"status\"]:\n",
    "                    df.at[idx, \"status\"] = \"insufficient data for delta\"\n",
    "    \n",
    "        # Optional: pretty summary column\n",
    "        def _fmt_money(v, cur):\n",
    "            if pd.isna(v): return \"NaN\"\n",
    "            # Simple formatting, keeps sign\n",
    "            return f\"{cur}{abs(v):,.2f}\" if cur else f\"{v:,.2f}\"\n",
    "    \n",
    "        def mk_summary(r):\n",
    "            cur = r.get(\"currency_hint\", \"\") or \"\"\n",
    "            cp = r.get(\"current_unit_price\", np.nan)\n",
    "            ap = r.get(\"avg_unit_price_simple_all\", np.nan)\n",
    "            dp = r.get(\"delta_per_unit\", np.nan)\n",
    "            dpp = r.get(\"delta_pct\", np.nan)\n",
    "            tv = r.get(\"variance_value_total\", np.nan)\n",
    "            verdict = r.get(\"verdict\", None) or \"insufficient data\"\n",
    "            used = r.get(\"used_sections\", \"none\")\n",
    "    \n",
    "            if pd.isna(cp) and pd.isna(ap):\n",
    "                return \"No price info found.\"\n",
    "            side = \"higher\" if (not pd.isna(dpp) and dpp > 0) else (\"lower\" if not pd.isna(dpp) else \"unknown\")\n",
    "    \n",
    "            return (\n",
    "                f\"Current Price/Unit: {_fmt_money(cp, cur)} | \"\n",
    "                f\"Avg Price/Unit ({used}): {_fmt_money(ap, cur)} | \"\n",
    "                f\"Δ/Unit: {_fmt_money(dp, cur)} ({'' if pd.isna(dpp) else f'{dpp:+.2f}%'}); \"\n",
    "                f\"→ Current PO price is {verdict}. \"\n",
    "                f\"Total Variance Value: {_fmt_money(tv, cur)}\"\n",
    "            )\n",
    "    \n",
    "        df[\"variance_summary\"] = df[out_cols].apply(mk_summary, axis=1)\n",
    "        return df\n",
    "    \n",
    "    # ---------------- USAGE ----------------\n",
    "    df_updated_2 = compute_price_variance_from_evidence(df_updated.copy(), evidence_col=\"updated_evidence_text\")\n",
    "    # Now you can inspect:\n",
    "    # df[[\n",
    "    #     \"current_qty\",\"current_unit_price\",\"avg_unit_price_same_vendor\",\"avg_unit_price_cross_vendor\",\n",
    "    #     \"avg_unit_price_weighted_all\",\"delta_per_unit\",\"delta_pct\",\"variance_value_total\",\n",
    "    #     \"verdict\",\"status\",\"used_sections\",\"variance_summary\"\n",
    "    # ]]\n",
    "\n",
    "    drop_some=df_updated_2.copy()\n",
    "    df_updated_final=drop_some.drop(columns=[\n",
    "    'current_qty','current_unit_price','currency_hint','avg_unit_price_same_vendor',\n",
    "    'avg_unit_price_cross_vendor','avg_unit_price_weighted_all','avg_unit_price_simple_all',\n",
    "    'n_rows_same_vendor', 'n_rows_cross_vendor', 'n_rows_used',\n",
    "           'delta_per_unit', 'delta_pct', 'variance_value_total', 'verdict',\n",
    "           'status', 'used_sections'],errors=\"ignore\")\n",
    "    return df_updated_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
