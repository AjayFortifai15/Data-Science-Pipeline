{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7193d-18f2-400a-94ba-b6dabebc101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "##############################################################################\n",
    "#  Cell 1 ‚Äî Imports & common paths\n",
    "##############################################################################\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import joblib\n",
    "import logging\n",
    "from typing import List\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import logging\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set options to show full DataFrame output\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import joblib, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from typing import Any, Dict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Feature engineering functions (pure, chainable)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def preprocessing_feature_engineering_prediction(df_final):\n",
    "    def _prep(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Rename key columns, parse dates & fill obvious nulls.\"\"\"\n",
    "        #df = df.rename(columns={k: v for k, v in COL_MAP.items() if k in df.columns})\n",
    "        df[\"purch_doc_date_hpd_po\"] = pd.to_datetime(df[\"purch_doc_date_hpd_po\"], errors=\"coerce\")\n",
    "        df[\"doc_change_date_src_po\"] = pd.to_datetime(df.get(\"doc_change_date_src_po\"), errors=\"coerce\")\n",
    "        df[\"vendor_or_creditor_acct_no_hpd_po\"] = df.get(\"vendor_or_creditor_acct_no_hpd_po\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "        df[\"requester_name_src_po\"] = df.get(\"requester_name_src_po\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "        df[\"exchange_rate_hpd_po\"] = df.get(\"exchange_rate_hpd_po\", 1.0).replace({0: np.nan}).fillna(1.0)\n",
    "        return df\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 2a) Rule‚Äëbased features (rules¬†1,2,3,5)\n",
    "    # -----------------------------------------------------------------------------\n",
    "    \n",
    "    def add_rule_metrics(df: pd.DataFrame,\n",
    "                         split_days: int = 60,\n",
    "                         price_var_days: int = 365) -> pd.DataFrame:\n",
    "        \"\"\"Add features mirroring Baldota P2P rules.\n",
    "    \n",
    "        * vm_count/value = aggregation for **same vendor+material** within *split_days*\n",
    "        * vm_price_var_pct = price deviation (%) vs mean of past *price_var_days*\n",
    "        * mat_vendor_cnt & mat_price_var_pct analogues for material across vendors\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df.sort_values(\"purch_doc_date_hpd_po\", inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        # Initialise\n",
    "        df[\"vm_count_%dd\" % split_days] = 0\n",
    "        df[\"vm_value_%dd\" % split_days] = 0.0\n",
    "        df[\"vm_price_var_pct_%dd\" % price_var_days] = 0.0\n",
    "        df[\"mat_vendor_cnt_%dd\" % split_days] = 0\n",
    "        df[\"mat_price_var_pct_%dd\" % price_var_days] = 0.0\n",
    "    \n",
    "        # Pre‚Äëextract convenient arrays for speed\n",
    "        po_dates = df[\"purch_doc_date_hpd_po\"].values\n",
    "        prices = df.get(\"net_price_doc_curr_src_po\").astype(float).values\n",
    "        vals = df.get(\"gross_val_po_curr_src_po\").astype(float).values\n",
    "    \n",
    "        # --- Same vendor + material group logic ----------------------------------\n",
    "        for (v, m), idx in df.groupby([\"vendor_or_creditor_acct_no_hpd_po\", \"material_no_src_po\"]).groups.items():\n",
    "            d = po_dates[idx]\n",
    "            p = prices[idx]\n",
    "            vls = vals[idx]\n",
    "            for loc, ridx in enumerate(idx):\n",
    "                cur = d[loc]\n",
    "                # split window\n",
    "                win_mask = (d >= cur - np.timedelta64(split_days, \"D\")) & (d <= cur)\n",
    "                df.iat[ridx, df.columns.get_loc(\"vm_count_%dd\" % split_days)] = int(win_mask.sum())\n",
    "                df.iat[ridx, df.columns.get_loc(\"vm_value_%dd\" % split_days)] = float(vls[win_mask].sum())\n",
    "                # price variance window\n",
    "                var_mask = (d >= cur - np.timedelta64(price_var_days, \"D\")) & (d <= cur)\n",
    "                if var_mask.sum() > 1:\n",
    "                    mean_price = p[var_mask].mean()\n",
    "                    if mean_price:\n",
    "                        pct = abs(p[loc] - mean_price) / mean_price * 100\n",
    "                        df.iat[ridx, df.columns.get_loc(\"vm_price_var_pct_%dd\" % price_var_days)] = pct\n",
    "    \n",
    "        # --- Material‚Äëonly group logic -------------------------------------------\n",
    "        for m, idx in df.groupby(\"material_no_src_po\").groups.items():\n",
    "            d = po_dates[idx]\n",
    "            p = prices[idx]\n",
    "            vendors = df.loc[idx, \"vendor_or_creditor_acct_no_hpd_po\"].values\n",
    "            for loc, ridx in enumerate(idx):\n",
    "                cur = d[loc]\n",
    "                win_mask = (d >= cur - np.timedelta64(split_days, \"D\")) & (d <= cur)\n",
    "                df.iat[ridx, df.columns.get_loc(\"mat_vendor_cnt_%dd\" % split_days)] = int(len(set(vendors[win_mask])))\n",
    "                var_mask = (d >= cur - np.timedelta64(price_var_days, \"D\")) & (d <= cur)\n",
    "                if var_mask.sum() > 1:\n",
    "                    mean_price = p[var_mask].mean()\n",
    "                    if mean_price:\n",
    "                        pct = abs(p[loc] - mean_price) / mean_price * 100\n",
    "                        df.iat[ridx, df.columns.get_loc(\"mat_price_var_pct_%dd\" % price_var_days)] = pct\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 2b) Value & process metrics\n",
    "    # -----------------------------------------------------------------------------\n",
    "    \n",
    "    def add_value_and_timing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df[\"conv_factor_p2o\"] = (\n",
    "            df.get(\"p2o_unit_conv_num_src_po\") / df.get(\"p2o_unit_conv_denom_src_po\")\n",
    "        ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "        df[\"conv_factor_o2b\"] = (\n",
    "            df.get(\"o2b_unit_conv_num_src_po\") / df.get(\"o2b_unit_conv_denom_src_po\")\n",
    "        ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "    \n",
    "        df[\"po_change_lag_days\"] = (df.get(\"doc_change_date_src_po\") - df[\"purch_doc_date_hpd_po\"]).dt.days\n",
    "        df[\"base_value\"] = df.get(\"gross_val_po_curr_src_po\") * df.get(\"exchange_rate_hpd_po\")\n",
    "        p95 = df[\"gross_val_po_curr_src_po\"].quantile(0.95)\n",
    "        df[\"high_value_flag\"] = (df[\"gross_val_po_curr_src_po\"] >= p95).astype(int)\n",
    "        return df\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 2c) Behavioural rolling windows\n",
    "    # -----------------------------------------------------------------------------\n",
    "    \n",
    "    def _rolling_stats(df: pd.DataFrame, group_col: str, days: int,\n",
    "                       count_col: str, sum_col: str) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df[count_col] = 0\n",
    "        df[sum_col] = 0.0\n",
    "    \n",
    "        po_dates = df[\"purch_doc_date_hpd_po\"].values\n",
    "        gross_vals = df[\"gross_val_po_curr_src_po\"].values\n",
    "    \n",
    "        for key, idx in df.groupby(group_col).groups.items():\n",
    "            dates = po_dates[idx]\n",
    "            vals = gross_vals[idx]\n",
    "            for loc, ridx in enumerate(idx):\n",
    "                cur = dates[loc]\n",
    "                mask = (dates >= cur - np.timedelta64(days, \"D\")) & (dates <= cur)\n",
    "                df.iat[ridx, df.columns.get_loc(count_col)] = int(mask.sum())\n",
    "                df.iat[ridx, df.columns.get_loc(sum_col)] = float(vals[mask].sum())\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def add_behavioural_stats(df: pd.DataFrame, window_days: int = 30) -> pd.DataFrame:\n",
    "        df = _rolling_stats(df, \"requester_name_src_po\", window_days,\n",
    "                            \"req_po_count_%dd\" % window_days,\n",
    "                            \"req_val_sum_%dd\" % window_days)\n",
    "        df = _rolling_stats(df, \"vendor_or_creditor_acct_no_hpd_po\", window_days,\n",
    "                            \"vendor_po_count_%dd\" % window_days,\n",
    "                            \"vendor_val_sum_%dd\" % window_days)\n",
    "        return df\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 3) Public orchestrator\n",
    "    # -----------------------------------------------------------------------------\n",
    "    \n",
    "    def build_features(raw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"End‚Äëto‚Äëend feature generator (no target leakage).\"\"\"\n",
    "        df = _prep(raw_df)\n",
    "        df = add_rule_metrics(df)\n",
    "        df = add_value_and_timing(df)\n",
    "        df = add_behavioural_stats(df)\n",
    "        return df\n",
    "    def flag_split_po(df):\n",
    "        df = df.copy()\n",
    "        df['split_po_flag'] = 0  # Default 0\n",
    "    \n",
    "        # Apply exclusion filters only for computation\n",
    "        exclude_doc_types = [\"AN\", \"AR\", \"MN\", \"QC\", \"QI\", \"QS\", \"RS\", \"SC\", \"SG\", \"SR\", \"SS\", \"ST\", \"TP\", \"TR\", \"UB\", \"WK\"]\n",
    "    \n",
    "        filtered = df[\n",
    "            (~df['purch_doc_type_hpd_po'].isin(exclude_doc_types)) &\n",
    "            (df['purch_doc_type_hpd_po'].notna()) &\n",
    "            (~(df['po_item_del_flag_src_po'] == 'L')) &\n",
    "            (~df['plant_src_po'].fillna(\"\").astype(str).str.startswith(\"4\")) &\n",
    "            (df['release_indicator_hpd_po'] == 'R') &\n",
    "            (df['gross_val_po_curr_src_po'] >= 10) &\n",
    "            (df['purch_doc_date_hpd_po'].notna()) &\n",
    "            (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "            (df['company_code_src_po'].notna())\n",
    "        ].copy()\n",
    "    \n",
    "        filtered['purch_doc_date_hpd_po'] = pd.to_datetime(filtered['purch_doc_date_hpd_po'])\n",
    "    \n",
    "        def get_group_key(row):\n",
    "            if pd.notna(row['material_no_src_po']):\n",
    "                return f\"{row['vendor_or_creditor_acct_no_hpd_po']}__{row['material_no_src_po']}\"\n",
    "            elif pd.notna(row['short_text_src_po']):\n",
    "                return f\"{row['vendor_or_creditor_acct_no_hpd_po']}__{row['short_text_src_po']}\"\n",
    "            else:\n",
    "                return np.nan\n",
    "    \n",
    "        filtered['split_key'] = filtered.apply(get_group_key, axis=1)\n",
    "        filtered = filtered[filtered['split_key'].notna()].copy()\n",
    "        filtered.sort_values(['split_key', 'purch_doc_date_hpd_po'], inplace=True)\n",
    "    \n",
    "        flagged_po_set = set()\n",
    "    \n",
    "        for key, group in filtered.groupby('split_key'):\n",
    "            dates = group['purch_doc_date_hpd_po'].reset_index(drop=True)\n",
    "            po_nos = group['purch_doc_no_src_po'].reset_index(drop=True)\n",
    "    \n",
    "            for i in range(len(dates)):\n",
    "                date_i = dates[i]\n",
    "                po_i = po_nos[i]\n",
    "                mask = (\n",
    "                    (dates >= date_i - pd.Timedelta(days=14)) &\n",
    "                    (dates <= date_i + pd.Timedelta(days=14)) &\n",
    "                    (po_nos != po_i)\n",
    "                )\n",
    "                if mask.sum() > 0:\n",
    "                    flagged_po_set.add(po_i)\n",
    "    \n",
    "        # Assign flag only to matching rows in original df\n",
    "        df['split_po_flag'] = df['purch_doc_no_src_po'].isin(flagged_po_set).astype(int)\n",
    "        return df\n",
    "    def flag_intra_po_split(df, gross_threshold=10):\n",
    "        df = df.copy()\n",
    "        df['intra_po_split_flag'] = 0  # Default\n",
    "    \n",
    "        df_valid = df[\n",
    "            df['gross_val_po_curr_src_po'].notna() &\n",
    "            df['vendor_or_creditor_acct_no_hpd_po'].notna() &\n",
    "            df['material_no_src_po'].notna() &\n",
    "            df['purch_doc_no_src_po'].notna()\n",
    "        ].copy()\n",
    "    \n",
    "        group_cols = ['purch_doc_no_src_po', 'vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po']\n",
    "        grouped = df_valid.groupby(group_cols)\n",
    "    \n",
    "        flagged_indexes = []\n",
    "    \n",
    "        for _, group in grouped:\n",
    "            total_gross = group['gross_val_po_curr_src_po'].sum()\n",
    "            num_items = len(group)\n",
    "            all_below_threshold = group['gross_val_po_curr_src_po'].all()\n",
    "    \n",
    "            if total_gross >= gross_threshold and num_items > 1 and all_below_threshold:\n",
    "                flagged_indexes.extend(group.index.tolist())\n",
    "    \n",
    "        df.loc[flagged_indexes, 'intra_po_split_flag'] = 1\n",
    "        return df\n",
    "    def flag_multiple_pos_per_pr_item(df):\n",
    "        df = df.copy()\n",
    "        df['multi_po_per_pr_flag'] = 0  # Default\n",
    "    \n",
    "        # Only consider approved POs with valid PR and PR item\n",
    "        df_valid = df[\n",
    "            (df['release_indicator_hpd_po'] == 'R') &\n",
    "            df['pr_no_src_po'].notna() &\n",
    "            df['pr_item_no_src_po'].notna() &\n",
    "            df['purch_doc_no_src_po'].notna()\n",
    "        ][['pr_no_src_po', 'pr_item_no_src_po', 'purch_doc_no_src_po']].drop_duplicates()\n",
    "    \n",
    "        # Count number of unique POs per PR+Item\n",
    "        po_counts = df_valid.groupby(['pr_no_src_po', 'pr_item_no_src_po'])['purch_doc_no_src_po'].nunique()\n",
    "    \n",
    "        # Identify PR+Items linked to more than one PO\n",
    "        multi_po_keys = po_counts[po_counts > 1].index.tolist()\n",
    "    \n",
    "        # Create a set for fast lookup\n",
    "        multi_po_set = set(multi_po_keys)\n",
    "    \n",
    "        # Flag in the main DataFrame\n",
    "        df['multi_po_per_pr_flag'] = df.apply(\n",
    "            lambda row: 1 if (row['pr_no_src_po'], row['pr_item_no_src_po']) in multi_po_set else 0,\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "        return df\n",
    "    def flag_same_vendor_price_increase(df, price_increase_threshold=0.05, months_range=6, flag_column='same_vendor_price_increase_flag'):\n",
    "        df = df.copy()\n",
    "        df[flag_column] = 0\n",
    "    \n",
    "        df_valid = df[\n",
    "            (df['release_indicator_hpd_po'] == 'R') &\n",
    "            (df['material_no_src_po'].notna()) &\n",
    "            (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "            (df['net_price_doc_curr_src_po'].notna()) &\n",
    "            (df['purch_doc_date_hpd_po'].notna())\n",
    "        ].copy()\n",
    "    \n",
    "        df_valid['purch_doc_date_hpd_po'] = pd.to_datetime(df_valid['purch_doc_date_hpd_po'])\n",
    "        df_valid.sort_values(['vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po', 'purch_doc_date_hpd_po'], inplace=True)\n",
    "    \n",
    "        group_cols = ['vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po']\n",
    "        flagged_indices = []\n",
    "    \n",
    "        for _, group in df_valid.groupby(group_cols):\n",
    "            group = group.sort_values('purch_doc_date_hpd_po').reset_index()\n",
    "    \n",
    "            for i in range(1, len(group)):\n",
    "                current_row = group.loc[i]\n",
    "                current_date = current_row['purch_doc_date_hpd_po']\n",
    "                current_price = current_row['net_price_doc_curr_src_po']\n",
    "    \n",
    "                mask = group.loc[:i-1, 'purch_doc_date_hpd_po'] >= current_date - pd.DateOffset(months=months_range)\n",
    "                past_group = group.loc[:i-1][mask]\n",
    "    \n",
    "                if not past_group.empty:\n",
    "                    last_price = past_group['net_price_doc_curr_src_po'].iloc[-1]\n",
    "                    if last_price > 0 and ((current_price - last_price) / last_price) >= price_increase_threshold:\n",
    "                        flagged_indices.append(current_row['index'])\n",
    "    \n",
    "        df.loc[flagged_indices, flag_column] = 1\n",
    "        return df\n",
    "    def flag_diff_vendor_price_variance(df, price_variance_threshold=0.05, months_range=6, flag_column='diff_vendor_price_variance_flag'):\n",
    "        df = df.copy()\n",
    "        df[flag_column] = 0\n",
    "    \n",
    "        df_valid = df[\n",
    "            (df['release_indicator_hpd_po'] == 'R') &\n",
    "            (df['material_no_src_po'].notna()) &\n",
    "            (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "            (df['net_price_doc_curr_src_po'].notna()) &\n",
    "            (df['purch_doc_date_hpd_po'].notna())\n",
    "        ].copy()\n",
    "    \n",
    "        df_valid['purch_doc_date_hpd_po'] = pd.to_datetime(df_valid['purch_doc_date_hpd_po'])\n",
    "        df_valid = df_valid.sort_values(['material_no_src_po', 'purch_doc_date_hpd_po'])\n",
    "    \n",
    "        for material, mat_group in df_valid.groupby('material_no_src_po'):\n",
    "            mat_group = mat_group.sort_values('purch_doc_date_hpd_po').reset_index()\n",
    "    \n",
    "            for i in range(len(mat_group)):\n",
    "                current_row = mat_group.loc[i]\n",
    "                current_date = current_row['purch_doc_date_hpd_po']\n",
    "                current_price = current_row['net_price_doc_curr_src_po']\n",
    "    \n",
    "                past_window = mat_group[\n",
    "                    (mat_group['purch_doc_date_hpd_po'] < current_date) &\n",
    "                    (mat_group['purch_doc_date_hpd_po'] >= current_date - pd.DateOffset(months=months_range))\n",
    "                ]\n",
    "    \n",
    "                vendor_prices = past_window.groupby('vendor_or_creditor_acct_no_hpd_po')['net_price_doc_curr_src_po'].mean()\n",
    "                if not vendor_prices.empty:\n",
    "                    max_price = vendor_prices.max()\n",
    "                    min_price = vendor_prices.min()\n",
    "                    if max_price > 0 and (max_price - min_price) / max_price >= price_variance_threshold:\n",
    "                        df.loc[current_row['index'], flag_column] = 1\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    def final_parsing(df):\n",
    "            # === Parse Date Columns ===\n",
    "        date_cols = [\n",
    "            \"doc_change_date_src_po\",\n",
    "            \"doc_change_date_hpd_po\",\n",
    "            \"purch_doc_date_hpd_po\"\n",
    "        ]\n",
    "        for col in date_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        \n",
    "        # === Feature Engineering ===\n",
    "        \n",
    "        # A. Date Features\n",
    "        df[\"po_doc_age_days\"] = (df[\"doc_change_date_hpd_po\"] - df[\"purch_doc_date_hpd_po\"]).dt.days\n",
    "        df[\"lead_time_po_vs_pr\"] = (df[\"purch_doc_date_hpd_po\"] - df[\"doc_change_date_src_po\"]).dt.days\n",
    "        df[\"po_day_of_week\"] = df[\"purch_doc_date_hpd_po\"].dt.dayofweek\n",
    "        df[\"po_day_of_month\"] = df[\"purch_doc_date_hpd_po\"].dt.day\n",
    "        df[\"po_month\"] = df[\"purch_doc_date_hpd_po\"].dt.month\n",
    "        \n",
    "        # B. Price & Value Features\n",
    "        df[\"price_per_unit\"] = df[\"net_val_po_curr_src_po\"] / df[\"quantity_src_po\"].replace(0, np.nan)\n",
    "        df[\"net_vs_gross_delta\"] = df[\"gross_val_po_curr_src_po\"] - df[\"net_val_po_curr_src_po\"]\n",
    "        df[\"price_variance_percent\"] = (df[\"net_val_po_curr_src_po\"] - df[\"gross_val_po_curr_src_po\"]) / df[\"gross_val_po_curr_src_po\"].replace(0, np.nan)\n",
    "        df[\"outline_agrmt_coverage\"] = df[\"outline_agrmt_tgt_val_doc_curr_src_po\"] / df[\"net_val_po_curr_src_po\"].replace(0, np.nan)\n",
    "        \n",
    "        # C. PO-PR Linkage Flags\n",
    "        df[\"has_pr_link\"] = df[\"pr_no_src_po\"].notna().astype(int)\n",
    "        df[\"has_pr_item_link\"] = df[\"pr_item_no_src_po\"].notna().astype(int)\n",
    "        \n",
    "        # D. Flags & Indicators\n",
    "        binary_cols = [\n",
    "            \"gr_indicator_src_po\", \"gr_invoice_verif_flag_src_po\",\n",
    "            \"inv_receipt_indicator_src_po\", \"release_indicator_hpd_po\",\n",
    "            \"release_status_hpd_po\", \"doc_release_incompl_flag_hpd_po\"\n",
    "        ]\n",
    "        for col in binary_cols:\n",
    "            if col in df.columns:\n",
    "                df[col + \"_flag\"] = df[col].notna().astype(int)\n",
    "        \n",
    "        # E. Missing Signal\n",
    "        critical_cols = [\n",
    "            \"material_type_src_po\", \"material_no_src_po\",\n",
    "            \"vendor_or_creditor_acct_no_hpd_po\", \"gross_val_po_curr_src_po\",\n",
    "            \"net_price_doc_curr_src_po\"\n",
    "        ]\n",
    "        df[\"missing_critical_fields\"] = df[critical_cols].isnull().sum(axis=1)\n",
    "        \n",
    "        # F. Currency & Exchange Rate\n",
    "        df[\"has_exchange_rate\"] = df[\"exchange_rate_hpd_po\"].notna().astype(int)\n",
    "        df[\"log_exchange_rate\"] = np.log1p(df[\"exchange_rate_hpd_po\"].fillna(0))\n",
    "        \n",
    "        # G. Unit Conversion Features\n",
    "        df[\"p2o_conversion_ratio\"] = df[\"p2o_unit_conv_num_src_po\"] / df[\"p2o_unit_conv_denom_src_po\"].replace(0, np.nan)\n",
    "        df[\"o2b_conversion_ratio\"] = df[\"o2b_unit_conv_num_src_po\"] / df[\"o2b_unit_conv_denom_src_po\"].replace(0, np.nan)\n",
    "        \n",
    "        # H. Behavioral Flags\n",
    "        df[\"is_same_vendor_pr_po\"] = (\n",
    "            df[\"vendor_or_creditor_acct_no_hpd_po\"].notna() & df[\"base_id_src_po\"].notna()\n",
    "        ).astype(int)\n",
    "        df[\"has_rfq_status\"] = df[\"rfq_status_hpd_po\"].notna().astype(int)\n",
    "        df[\"purch_group_org_same\"] = (df[\"purch_group_hpd_po\"] == df[\"purch_org_hpd_po\"]).astype(int)\n",
    "        \n",
    "        # I. Rare Category Flagging\n",
    "        rare_cat_cols = [\"purch_doc_type_hpd_po\", \"purch_group_hpd_po\", \"vendor_or_creditor_acct_no_hpd_po\"]\n",
    "        for col in rare_cat_cols:\n",
    "            if col in df.columns:\n",
    "                freq_map = df[col].value_counts(normalize=True)\n",
    "                df[f\"{col}_is_rare\"] = df[col].map(freq_map) < 0.01\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    class Autoencoder(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_1, hidden_2, bottleneck):\n",
    "            super().__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_1, hidden_2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_2, bottleneck)\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(bottleneck, hidden_2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_2, hidden_1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_1, input_dim)\n",
    "            )\n",
    "    \n",
    "        def forward(self, x):\n",
    "            encoded = self.encoder(x)\n",
    "            decoded = self.decoder(encoded)\n",
    "            return decoded\n",
    "    \n",
    "    def auto_encoder_pipeline(df_new):\n",
    "        # ========== Step 1: Load Files Auto Encoder ==========\n",
    "        model_weights_path = \"Po_Invoice_Data/autoencoder_model.pth\"\n",
    "        preprocessor_path = \"Po_Invoice_Data/preprocessor.pkl\"\n",
    "        best_params_path = \"Po_Invoice_Data/best_params.pkl\"\n",
    "        # Load precomputed threshold\n",
    "        threshold_path = \"Po_Invoice_Data/autoencoder_threshold.pkl\"\n",
    "    \n",
    "        preprocessor = joblib.load(preprocessor_path)\n",
    "        best_params = joblib.load(best_params_path)\n",
    "        \n",
    "        # ========== Step 3: Preprocess the New Data ==========\n",
    "        X_new_scaled = preprocessor.transform(df_new)\n",
    "        \n",
    "        # ========== Step 5: Initialize & Load Model ==========\n",
    "        input_dim = X_new_scaled.shape[1]\n",
    "        model = Autoencoder(input_dim, best_params['hidden_1'], best_params['hidden_2'], best_params['bottleneck'])\n",
    "        model.load_state_dict(torch.load(model_weights_path))\n",
    "        model.eval()\n",
    "        \n",
    "        # ========== Step 6: Get Reconstruction Error ==========\n",
    "        X_tensor = torch.tensor(X_new_scaled, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            reconstructed = model(X_tensor)\n",
    "            recon_error = torch.mean((X_tensor - reconstructed) ** 2, dim=1).numpy()\n",
    "        \n",
    "        # ========== Step 7: Apply Threshold to Get Prediction ==========\n",
    "        # Load the same threshold used during training or recompute if needed\n",
    "        # For now, assume same threshold as earlier\n",
    "        threshold =joblib.load(threshold_path) #np.percentile(recon_error, 95)  # or hardcode: threshold = 0.0383\n",
    "        print(f\"üö® Threshold used: {threshold:.4f}\")\n",
    "        \n",
    "        predicted_labels = (recon_error > threshold).astype(int)\n",
    "        \n",
    "        # ========== Step 8: Add to DataFrame and Save ==========\n",
    "        df_new[\"ae_fraud_score\"] = recon_error\n",
    "        df_new[\"ae_predicted_flag\"] = predicted_labels\n",
    "        \n",
    "    \n",
    "        return df_new\n",
    "    \n",
    "    def gbt_pipeline(df):\n",
    "        # === CONFIG ===\n",
    "        MODEL_PATH = Path(\"Po_Invoice_Data/po_gbdt_model.pkl\")\n",
    "        print(\"üì¶ Loading model from:\", MODEL_PATH)\n",
    "        model = joblib.load(MODEL_PATH)\n",
    "    \n",
    "        print(\"üîç Predicting risk probabilities‚Ä¶\")\n",
    "        X = df.copy()\n",
    "        fraud_flag = model.predict(X)\n",
    "        fraud_score = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        print(\"üìù Adding prediction columns‚Ä¶\")\n",
    "        df[\"gbt_fraud_score\"] = fraud_score\n",
    "        df[\"gbt_model_flag\"] = fraud_flag\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    def predict_isolation_forest(new_df: pd.DataFrame):\n",
    "        ARTIFACT_DIR = Path(\"Po_Invoice_Data\")\n",
    "        ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        PREPROC_PKL   = ARTIFACT_DIR / \"iso_preprocessor.pkl\"\n",
    "        MODEL_PKL     = ARTIFACT_DIR / \"isoforest_model.pkl\"\n",
    "        THRESHOLD_NPY = ARTIFACT_DIR / \"isoforest_threshold.npy\"\n",
    "        \"\"\"\n",
    "        Loads saved preprocessor, model, and threshold. Returns scores + flags for new data.\n",
    "        \"\"\"\n",
    "        preproc = joblib.load(PREPROC_PKL)\n",
    "        iso = joblib.load(MODEL_PKL)\n",
    "        threshold = float(np.load(THRESHOLD_NPY)[0])\n",
    "    \n",
    "        out = new_df.copy()\n",
    "        X_enc = preproc.transform(new_df)\n",
    "        scores = iso.score_samples(X_enc)\n",
    "        flags = (scores < threshold).astype(int)  # 1 = anomaly\n",
    "    \n",
    "        #out = new_df.copy()\"iso_fraud_score\", \"iso_predicted_flag\"\n",
    "        out[\"iso_fraud_score\"] = scores\n",
    "        out[\"iso_predicted_flag\"]  = flags\n",
    "        return out\n",
    "    \n",
    "    # === Drop same columns as training ===\n",
    "    columns_to_drop = [\n",
    "        # IDs and references\n",
    "        \"purch_doc_no_src_po\", \"purch_doc_item_no_src_po\", \"pr_no_src_po\", \"pr_item_no_src_po\",\"base_id_src_po\", \n",
    "        \"principal_purch_agrmt_item_no_src_po\", \"principal_purch_agrmt_no_hpd_po\",\n",
    "    \n",
    "        # Text\n",
    "        \"short_text_src_po\", \"requester_name_src_po\", \"resp_vendor_salesperson_hpd_po\",\n",
    "    \n",
    "        # Dates (used to create features)\n",
    "        \"doc_change_date_src_po\", \"doc_change_date_hpd_po\", \"purch_doc_date_hpd_po\",\n",
    "    \n",
    "        # Sparse or incomplete\n",
    "        \"po_item_del_flag_src_po\", \"doc_release_incompl_flag_hpd_po\", \"control_indicator_hpd_po\",\n",
    "    \n",
    "        # Replaced with ratios / logs / engineered\n",
    "        \"p2o_unit_conv_denom_src_po\", \"p2o_unit_conv_num_src_po\",\n",
    "        \"o2b_unit_conv_denom_src_po\", \"o2b_unit_conv_num_src_po\",\n",
    "        \"gross_val_po_curr_src_po\", \"net_val_po_curr_src_po\",\n",
    "        \"outline_agrmt_tgt_val_doc_curr_src_po\", \"quantity_src_po\",\n",
    "        \"exchange_rate_hpd_po\", \"net_price_doc_curr_src_po\",]\n",
    "    def predict_sub_risks(raw_df) -> pd.DataFrame:\n",
    "        # === Load MLP model and transformers ===\n",
    "        MODEL_PATH = \"Po_Invoice_Data/rf_multilabel_model_sub_risks.pkl\"\n",
    "        MLB_PATH = \"Po_Invoice_Data/mlb.pkl\"\n",
    "        ENCODERS_PATH = \"Po_Invoice_Data/label_encoders.pkl\"\n",
    "        IMPUTER_PATH = \"Po_Invoice_Data/imputer.pkl\"\n",
    "        \n",
    "        model = joblib.load(MODEL_PATH)\n",
    "        mlb = joblib.load(MLB_PATH)\n",
    "        label_encoders = joblib.load(ENCODERS_PATH)\n",
    "        imputer = joblib.load(IMPUTER_PATH)\n",
    "        \n",
    "        # Step 2: Drop unused columns\n",
    "        columns_to_drop_before_preprocessing= [\n",
    "        'rft_by_engine_po','base_id_po', 'rule_ids_po','P2P02067', 'P2P02068', 'P2P02070', 'P2P02072','gbt_fraud_score', 'gbt_model_flag', 'ae_fraud_score',\n",
    "           'ae_predicted_flag',\"iso_fraud_score\", \"iso_predicted_flag\",'risk_score','risk_level',]#,'model_flag' ]\n",
    "        raw_df.drop(columns=[col for col in columns_to_drop_before_preprocessing if col in raw_df.columns], inplace=True)\n",
    "        \n",
    "        print(\"üßÆ Generating features‚Ä¶\")\n",
    "        # Step 3: Feature engineering\n",
    "        df = build_features(raw_df)\n",
    "        df = flag_split_po(df)\n",
    "        df = flag_intra_po_split(df)\n",
    "        df = flag_multiple_pos_per_pr_item(df)\n",
    "        df = flag_same_vendor_price_increase(df, months_range=6, flag_column='same_vendor_price_increase_6m_flag')\n",
    "        df = flag_same_vendor_price_increase(df, months_range=12, flag_column='same_vendor_price_increase_12m_flag')\n",
    "        df = flag_diff_vendor_price_variance(df, months_range=6, flag_column='diff_vendor_price_variance_6m_flag')\n",
    "        df = flag_diff_vendor_price_variance(df, months_range=12, flag_column='diff_vendor_price_variance_12m_flag')\n",
    "        df = final_parsing(df)\n",
    "        df_with_base_id_src_po=df.copy()\n",
    "        df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "    \n",
    "        # Step 4: Encode categorical\n",
    "        categorical_cols = df.select_dtypes(include='object').columns\n",
    "        for col in categorical_cols:\n",
    "            if col in label_encoders:\n",
    "                le = label_encoders[col]\n",
    "                df[col] = df[col].astype(str).fillna(\"Unknown\")\n",
    "                df[col] = le.transform(df[col])\n",
    "            else:\n",
    "                df[col] = 0  # fallback if unseen\n",
    "    \n",
    "        # Step 5: Impute numeric\n",
    "        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        df[numeric_cols] = imputer.transform(df[numeric_cols])\n",
    "        \n",
    "        print(\"üì¶ Loading model from:\", MODEL_PATH)\n",
    "        #  Step 6: Predict\n",
    "        X = df.copy()\n",
    "        \n",
    "        print(\"üîç Predicting risk probabilities‚Ä¶\")\n",
    "        y_pred = model.predict(X)\n",
    "        predicted_sub_risks = mlb.inverse_transform(y_pred)\n",
    "    \n",
    "        # Step 7: Output DataFrame\n",
    "        print(\"üìù Adding prediction columns‚Ä¶\")\n",
    "        df_with_base_id_src_po[\"predicted_risks\"] = predicted_sub_risks\n",
    "        # merge with data on base_id_src_po\n",
    "        #new_df=pd.merge(final_df,df_with_base_id_src_po[['base_id_src_po','Predicted Risks']],on='base_id_src_po')\n",
    "        #new_df.drop_duplicates(inplace=True)\n",
    "        # Save results\n",
    "        #new_df.to_pickle(output_pickle_path)\n",
    "        #print(f\"‚úÖ Predictions saved to {output_pickle_path}\")\n",
    "    \n",
    "        return df_with_base_id_src_po\n",
    "    \n",
    "    def compute_risk_level(row):\n",
    "        gbt = int(row['gbt_model_flag'])\n",
    "        ae  = int(row['ae_predicted_flag'])\n",
    "        iso = int(row['iso_predicted_flag'])\n",
    "    \n",
    "        # 1) All three flag ‚Üí Very High Risk\n",
    "        if gbt == 1 and ae == 1 and iso == 1:\n",
    "            return \"Very High Risk\"\n",
    "        if gbt== 0 and ae == 0 and iso == 0:\n",
    "            return \"No Risk\"\n",
    "        # 2) GBT flags (regardless of others, except the all-three case above) ‚Üí High Risk\n",
    "        if gbt == 1:\n",
    "            return \"High Risk\"\n",
    "    \n",
    "        # 3) GBT=0 and at least one of AE/ISO flags ‚Üí Needs Validation\n",
    "        if gbt == 0 and (ae == 1 or iso == 1):\n",
    "            return \"Needs Validation\"\n",
    "    \n",
    "    \n",
    "    def compute_model_flag(row):\n",
    "        gbt = int(row['gbt_model_flag'])\n",
    "        ae  = int(row['ae_predicted_flag'])\n",
    "        iso = int(row['iso_predicted_flag'])\n",
    "        # 0 only if all three are 0\n",
    "        return 0 if (gbt == 0 and ae == 0 and iso == 0) else 1\n",
    "    \n",
    "    def fraud_weighted_score(amount, prob):\n",
    "        \"\"\"\n",
    "        Score in [0,1] = minmax(amount) * prob_0_1\n",
    "        - amount: array-like or pandas Series (e.g., net_val_po_curr_src_po)\n",
    "        - prob: array-like or pandas Series (e.g., gbt_fraud_score; % or 0‚Äì1)\n",
    "        \"\"\"\n",
    "        a = pd.to_numeric(pd.Series(amount), errors=\"coerce\").astype(float)\n",
    "        p = pd.to_numeric(pd.Series(prob),   errors=\"coerce\").astype(float)\n",
    "    \n",
    "        # normalize prob to [0,1] if it looks like percentage\n",
    "        maxp = np.nanmax(p.values) if len(p) else np.nan\n",
    "        if np.isfinite(maxp) and 1.0 < maxp <= 100.0:\n",
    "            p = p / 100.0\n",
    "        p = p.clip(0.0, 1.0)\n",
    "    \n",
    "        # min-max scale amount\n",
    "        amin, amax = a.min(skipna=True), a.max(skipna=True)\n",
    "        if pd.isna(amin) or pd.isna(amax) or amax == amin:\n",
    "            a_mm = pd.Series(0.0, index=a.index)\n",
    "        else:\n",
    "            a_mm = (a - amin) / (amax - amin)\n",
    "    \n",
    "        # final score\n",
    "        return (0.3*a_mm + 0.7*p).fillna(0.0)\n",
    "    \n",
    "    \n",
    "    def sub_risk_screen(output_df):\n",
    "            # üîπ Step 2: Ensure all values in 'Predicted Risks' are tuples\n",
    "        output_df[\"predicted_risks\"] = output_df[\"predicted_risks\"].apply(lambda x: tuple(x) if not isinstance(x, tuple) else x)\n",
    "        \n",
    "        # üîπ Step 3: Fill empty risks based on model_flag\n",
    "        output_df[\"predicted_risks\"] = output_df.apply(\n",
    "            lambda row: (\"Price Variance Risk\",) if row[\"model_flag\"] == 1 and row[\"predicted_risks\"] in [(), None, np.nan]\n",
    "            else (\"No Risk\",) if row[\"model_flag\"] == 0 and row[\"predicted_risks\"] in [(), None, np.nan]\n",
    "            else row[\"predicted_risks\"],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # üîπ Step 4: Main Risk Scenario\n",
    "        output_df[\"main_risk_scenario\"] = output_df[\"model_flag\"].apply(\n",
    "            lambda x: \"Procurement Risk\" if x == 1 else \"No Risk\"\n",
    "        )\n",
    "        \n",
    "        # üîπ Step 5: Drop existing Sub Risk columns if any (optional cleanup)\n",
    "        output_df.drop(columns=[col for col in output_df.columns if col.startswith(\"sub_risk_\")], errors='ignore', inplace=True)\n",
    "        \n",
    "        # üîπ Step 6: Expand risk tuple to individual columns ‚Äî safe even if only one risk\n",
    "        risk_df = pd.DataFrame(output_df[\"predicted_risks\"].tolist())\n",
    "        \n",
    "        # üîπ Fix mismatched index\n",
    "        risk_df.reset_index(drop=True, inplace=True)\n",
    "        output_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # üîπ Rename dynamically based on max number of sub-risks\n",
    "        risk_df.columns = [f\"sub_risk_{i+1}\" for i in range(risk_df.shape[1])]\n",
    "        \n",
    "        # üîπ Step 7: Safe concat\n",
    "        output_df = pd.concat([output_df, risk_df], axis=1)\n",
    "        return output_df\n",
    "    \n",
    "    def get_impact_ordered(row, col1=\"sub_risk_1\", col2=\"sub_risk_2\"):\n",
    "        s1 = str(row.get(col1, \"\") or \"\")\n",
    "        s2 = str(row.get(col2, \"\") or \"\")\n",
    "        impacts = []\n",
    "    \n",
    "        def add(x):\n",
    "            if x not in impacts:\n",
    "                impacts.append(x)\n",
    "    \n",
    "        # Map contributions from sub_risk_1 (first)\n",
    "        if \"Price Variance Risk\" in s1:\n",
    "            add(\"Profitability\")\n",
    "            add(\"Cash Flow\")\n",
    "        if \"Split PO\" in s1:\n",
    "            add(\"Efficiency\")\n",
    "    \n",
    "        # Then contributions from sub_risk_2 (second)\n",
    "        if \"Price Variance Risk\" in s2:\n",
    "            add(\"Profitability\")\n",
    "            add(\"Cash Flow\")\n",
    "        if \"Split PO\" in s2:\n",
    "            add(\"Efficiency\")\n",
    "    \n",
    "        else:\n",
    "            add('None')\n",
    "    \n",
    "        return impacts\n",
    "    \n",
    "    def split_impact_column(df, impact_col=\"impact\", max_impacts=3):\n",
    "        \"\"\"\n",
    "        Splits a list-based 'Impact' column into separate columns: Impact 1, Impact 2, ..., Impact n.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "            impact_col (str): Name of the column containing the list of impacts.\n",
    "            max_impacts (int): Maximum number of impact columns to create.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with new 'Impact 1'...'Impact n' columns added.\n",
    "        \"\"\"\n",
    "        if impact_col not in df.columns:\n",
    "            raise ValueError(f\"Column '{impact_col}' not found in DataFrame.\")\n",
    "        \n",
    "        # Ensure all rows are lists, then pad with None to match max_impacts\n",
    "        impact_expanded = df[impact_col].apply(\n",
    "            lambda x: (x if isinstance(x, list) else []) + [None] * (max_impacts - len(x))\n",
    "        )\n",
    "        \n",
    "        # Create new columns\n",
    "        new_cols = [f\"impact_{i+1}\" for i in range(max_impacts)]\n",
    "        df[new_cols] = pd.DataFrame(impact_expanded.tolist(), index=df.index)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def preprocess_pipeline(raw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        columns_to_drop = ['rft_by_engine_po','base_id_po', 'rule_ids_po', 'P2P02067', 'P2P02068', 'P2P02070', 'P2P02072']\n",
    "        raw_df.drop(columns=[col for col in columns_to_drop if col in raw_df.columns], errors=\"ignore\", inplace=True)\n",
    "        df = build_features(raw_df)\n",
    "        df = flag_split_po(df)\n",
    "        df = flag_intra_po_split(df)\n",
    "        df = flag_multiple_pos_per_pr_item(df)\n",
    "        df = flag_same_vendor_price_increase(df, months_range=6, flag_column='same_vendor_price_increase_6m_flag')\n",
    "        df = flag_same_vendor_price_increase(df, months_range=12, flag_column='same_vendor_price_increase_12m_flag')\n",
    "        df = flag_diff_vendor_price_variance(df, months_range=6, flag_column='diff_vendor_price_variance_6m_flag')\n",
    "        df = flag_diff_vendor_price_variance(df, months_range=12, flag_column='diff_vendor_price_variance_12m_flag')\n",
    "        df = final_parsing(df)\n",
    "    \n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def predict_rft_flags(po_data_clean) -> pd.DataFrame:#predict_rft_flags( raw_path: Union[str, Path]) -> pd.DataFrame:\n",
    "        print(\"üì• Loading raw PO data‚Ä¶\")\n",
    "        #raw_df = pd.read_pickle(raw_path) if str(raw_path).endswith(\".pkl\") else pd.read_csv(raw_path)\n",
    "        raw_df=po_data_clean\n",
    "        \n",
    "        # Split into non-NaN and NaN base_id_src_po\n",
    "        df_with_id = raw_df[raw_df['base_id_src_po'].notna()].copy()\n",
    "        df_without_id = raw_df[raw_df['base_id_src_po'].isna()].copy()\n",
    "    \n",
    "        ### ==== PROCESS NON-NaN DATA ====\n",
    "        print(\"üßÆ Processing data with base_id_src_po...\")\n",
    "        data_for_gbt = preprocess_pipeline(df_with_id)\n",
    "        data_for_auto = data_for_gbt.copy()\n",
    "        data_for_iso=data_for_gbt.copy()\n",
    "        df_gbt= gbt_pipeline(data_for_gbt)\n",
    "        df_auto= auto_encoder_pipeline(data_for_auto)\n",
    "        df_iso=predict_isolation_forest(data_for_iso)\n",
    "        # üîπ Fix mismatched index\n",
    "        new_df=(df_gbt[[\"base_id_src_po\", \"gbt_fraud_score\", \"gbt_model_flag\"]]\n",
    "        .merge(\n",
    "            df_auto[[\"base_id_src_po\", \"ae_fraud_score\", \"ae_predicted_flag\"]],\n",
    "            on=\"base_id_src_po\",\n",
    "            how=\"outer\"\n",
    "        )\n",
    "        .merge(\n",
    "            df_iso[[\"base_id_src_po\", \"iso_fraud_score\", \"iso_predicted_flag\"]],\n",
    "            on=\"base_id_src_po\",\n",
    "            how=\"outer\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "        gbt_auto_df = pd.merge(df_with_id, new_df, on='base_id_src_po', how='inner').drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "        # Risk levels\n",
    "        gbt_auto_df[\"risk_score\"] = fraud_weighted_score(gbt_auto_df[\"net_val_po_curr_src_po\"], gbt_auto_df[\"gbt_fraud_score\"])\n",
    "        gbt_auto_df['risk_level'] = gbt_auto_df.apply(compute_risk_level, axis=1)\n",
    "        gbt_auto_df['model_flag'] = gbt_auto_df.apply(compute_model_flag, axis=1)\n",
    "    \n",
    "        # Sub-risk prediction\n",
    "        sub_risk_data = predict_sub_risks(gbt_auto_df.copy())\n",
    "        ml_with_all_data_df = pd.merge(\n",
    "            gbt_auto_df, \n",
    "            sub_risk_data[['base_id_src_po', 'predicted_risks']],\n",
    "            on='base_id_src_po',\n",
    "            how='left'\n",
    "        ).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "        result_df_with_id = sub_risk_screen(ml_with_all_data_df)\n",
    "    \n",
    "        ### ==== PROCESS NaN DATA SEPARATELY ====\n",
    "        if not df_without_id.empty:\n",
    "            print(\"üßÆ Processing data WITHOUT base_id_src_po...\")\n",
    "            df_without_id.reset_index(drop=True, inplace=True)\n",
    "            data_for_gbt_nan = preprocess_pipeline(df_without_id)\n",
    "            data_for_auto_nan = data_for_gbt_nan.copy()\n",
    "            data_for_iso_nan=data_for_gbt_nan.copy()\n",
    "    \n",
    "            df_gbt_nan = gbt_pipeline(data_for_gbt_nan)\n",
    "            df_auto_nan = auto_encoder_pipeline(data_for_auto_nan)\n",
    "            df_iso_nan=predict_isolation_forest(data_for_iso_nan)\n",
    "    \n",
    "            df_gbt_nan.reset_index(drop=True, inplace=True)\n",
    "            df_auto_nan.reset_index(drop=True, inplace=True)\n",
    "            df_iso_nan.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "            # Merge manually on index\n",
    "            nan_df = pd.concat([df_without_id, df_gbt_nan[['gbt_fraud_score', 'gbt_model_flag']], df_auto_nan[['ae_fraud_score', 'ae_predicted_flag']],\n",
    "                               df_iso_nan[['iso_fraud_score', 'iso_predicted_flag']]], axis=1)\n",
    "            nan_df[\"risk_score\"] = fraud_weighted_score(nan_df[\"net_val_po_curr_src_po\"], nan_df[\"gbt_fraud_score\"])\n",
    "            nan_df['risk_level'] = nan_df.apply(compute_risk_level, axis=1)\n",
    "            nan_df['model_flag'] = nan_df.apply(compute_model_flag, axis=1)\n",
    "            nan_df['predicted_risks'] = [()] * len(nan_df)  # Empty risks for now\n",
    "    \n",
    "            result_df_without_id = sub_risk_screen(nan_df)\n",
    "        else:\n",
    "            result_df_without_id = pd.DataFrame()\n",
    "    \n",
    "        ### ==== CONCAT FINAL OUTPUT ====\n",
    "        final_result_df = pd.concat([result_df_with_id, result_df_without_id], axis=0).reset_index(drop=True)\n",
    "        final_result_df[\"impact\"] = final_result_df.apply(get_impact_ordered, axis=1)\n",
    "        final_result_df = split_impact_column(final_result_df, impact_col=\"impact\", max_impacts=3)\n",
    "    \n",
    "        return final_result_df\n",
    "\n",
    "    # === Example run ===\n",
    "    if __name__ == \"__main__\":\n",
    "        final_result_df=predict_rft_flags(df_final.copy()) #\"po_output.pkl\")\n",
    "\n",
    "    return final_result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
