{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb4edc-46bf-409c-98ee-2e4e0feba824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ROCK-SOLID LLM REFINEMENT (Ollama gpt-oss:20b) ===========================\n",
    "# Input : df[\"llm_explanation\"]  (your 5-section deterministic text)\n",
    "# Output: df[\"llm_refined_explanation\"] (analyst-style narrative + Evidence verbatim)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import re, time, json, requests, pandas as pd\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential_jitter, retry_if_exception_type\n",
    "from tqdm import tqdm\n",
    "def ai_explanation_part_1(df_updated_final):\n",
    "    # ---------------- CONFIG ----------------\n",
    "    ID_COL        = \"base_id_src_po\"\n",
    "    INPUT_COL     = \"llm_explanation\"\n",
    "    FINAL_INPUT_EVIDENCE=\"updated_evidence_text\"\n",
    "    VARIANCE_COL  = \"variance_summary\" # 5-section text from your deterministic step\n",
    "    OUTPUT_COL    = \"llm_refined_explanation\"\n",
    "    OLLAMA_URL    = \"http://127.0.0.1:11434/api/chat\"\n",
    "    MODEL         = \"llama3:70b\"\n",
    "    \n",
    "    # Generation knobs (conservative)\n",
    "    OPTIONS = {\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1,\n",
    "        \"num_ctx\": 8192,\n",
    "        \"num_predict\": 1400\n",
    "    }\n",
    "    \n",
    "    # Banned terms (post-filter)\n",
    "    BANNED_TERMS = [\n",
    "        r\"\\bfraud\\b\", r\"\\bfraudulent\\b\", r\"\\bscam\\b\", r\"\\bscamming\\b\",\n",
    "        r\"\\bcorruption\\b\", r\"\\bbribe\\b\", r\"\\bbribery\\b\"\n",
    "    ]\n",
    "    HEADER_LINE = \"_\" * 152\n",
    "    # Checkpointing (optional)\n",
    "    CHECKPOINT_EVERY = 0                    # set e.g. 50 to save every 50 rows\n",
    "    CHECKPOINT_PATH  = \"with_llm_refined_explanations_checkpoint_06_09.parquet\"\n",
    "    \n",
    "    # ---------------- SMALL PARSERS (for helpful hints & fallback) ----------------\n",
    "    def _find_block(src: str, title_regex: str, stop_at_next_numbered=True) -> str:\n",
    "        \"\"\"\n",
    "        Returns lines AFTER a numbered heading matching title_regex until the next numbered heading or EOF.\n",
    "        \"\"\"\n",
    "        lines = src.splitlines()\n",
    "        pat = re.compile(rf\"^\\s*\\d+\\.\\s*{title_regex}\\s*:?\\s*$\", re.I)\n",
    "        start = None\n",
    "        for i, ln in enumerate(lines):\n",
    "            if pat.match(ln.strip()):\n",
    "                start = i + 1\n",
    "                break\n",
    "        if start is None:\n",
    "            return \"\"\n",
    "        out = []\n",
    "        for ln in lines[start:]:\n",
    "            if stop_at_next_numbered and re.match(r\"^\\s*\\d+\\.\\s\", ln):\n",
    "                break\n",
    "            out.append(ln)\n",
    "        while out and out[-1].strip() == \"\":\n",
    "            out.pop()\n",
    "        return \"\\n\".join(out)\n",
    "    \n",
    "    def extract_risk_line(src: str) -> str:\n",
    "        ctx = _find_block(src, r\"Context\\s*&\\s*Trigger\")\n",
    "        for ln in ctx.splitlines():\n",
    "            if \"Risk Score\" in ln:\n",
    "                return ln.strip()\n",
    "        return \"\"\n",
    "    \n",
    "    def extract_primary_line(src: str) -> str:\n",
    "        ctx = _find_block(src, r\"Context\\s*&\\s*Trigger\")\n",
    "        for ln in ctx.splitlines():\n",
    "            if \"Primary Risk Scenario\" in ln:\n",
    "                return ln.strip()\n",
    "        return \"\"\n",
    "    \n",
    "    def extract_risk_drivers(src: str) -> list[str]:\n",
    "        blk = _find_block(src, r\"Risk\\s*Drivers\")\n",
    "        drivers = []\n",
    "        for ln in blk.splitlines():\n",
    "            t = ln.strip()\n",
    "            if t.startswith(\"· \") or t.startswith(\"- \"):\n",
    "                drivers.append(t[2:].strip())\n",
    "        return drivers\n",
    "    \n",
    "    def extract_business_impact_line(src: str) -> str:\n",
    "        blk = _find_block(src, r\"Business\\s*Impact\")\n",
    "        # Prefer a “Line Value / Suspicious Value” style bullet if present\n",
    "        for ln in blk.splitlines():\n",
    "            t = ln.strip()\n",
    "            if t and (\"Suspicious\" in t or \"Line Value\" in t or \"Overall Impact\" in t):\n",
    "                return t\n",
    "        # fallback to first non-empty line\n",
    "        for ln in blk.splitlines():\n",
    "            if ln.strip():\n",
    "                return ln.strip()\n",
    "        return \"\"\n",
    "    def source_has_no_risk(source_text: str) -> bool:\n",
    "        return bool(re.search(r\"\\bNo\\s*Risk\\b\", source_text, re.IGNORECASE))\n",
    "    \n",
    "    # ---------------- Price Variance (your exact format) ----------------\n",
    "    def _normalize(s: str) -> str:\n",
    "        if s is None: return \"\"\n",
    "        s = str(s)\n",
    "        s = s.replace(\"—\", \"-\").replace(\"–\", \"-\").replace(\"₹\", \"INR \")\n",
    "        s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "        return s.strip()\n",
    "    \n",
    "    def _first(v: str, patterns: list[str]):\n",
    "        for p in patterns:\n",
    "            m = re.search(p, v, flags=re.I)\n",
    "            if m: return m.group(1).strip()\n",
    "        return None\n",
    "    \n",
    "    def _clean_arrow(s: str | None) -> str | None:\n",
    "        if not s: return None\n",
    "        t = re.sub(r\"^[•\\-\\u2022>\\u2192]+\\s*\", \"\", s.strip())\n",
    "        return f\"• {t}\"\n",
    "    \n",
    "    def build_price_variance_bullets(variance_text: str) -> str:\n",
    "        v = _normalize(variance_text)\n",
    "        curr  = _first(v, [r\"Current\\s*Price\\s*(?:Per\\s*Unit|/Unit)\\s*:\\s*([^|;]+)\"])\n",
    "        avg   = _first(v, [r\"Avg(?:erage)?\\s*Price\\s*(?:Per\\s*Unit|/Unit)(?:\\s*\\([^)]+\\))?\\s*:\\s*([^|;]+)\"])\n",
    "        delta = _first(v, [r\"[Δ∆]\\s*/?\\s*Unit\\s*:\\s*([^|;]+)\", r\"Variance\\s*Value\\s*Per\\s*Unit\\s*:\\s*([^|;]+)\"])\n",
    "        total = _first(v, [r\"Total\\s*Variance\\s*Value\\s*:\\s*([^|;]+)\"])\n",
    "        arrow = _clean_arrow(_first(v, [r\"(→\\s*[^|;]+)\"])) or \"• Current PO price is slightly below/high then average price.\"\n",
    "        return \"\\n\".join([\n",
    "            f\"• Current Price Per Unit: {curr or 'None'}\",\n",
    "            \"• Average Price Per Unit (Same and Different Vendor Compared Transactions from the compared transactions examples): \" + (avg or 'None'),\n",
    "            f\"• Variance Value Per Unit: {delta or 'None'}\",\n",
    "            arrow,\n",
    "            \"• Total Variance Value: \" + (total or 'None') + \" {Value = (Current price per unit - Average price per unit) x Current Qty}\"\n",
    "        ])\n",
    "    \n",
    "    def remove_price_variance_if_present(text: str) -> str:\n",
    "        # NOTE: Kept for reference; currently not used.\n",
    "        pat = re.compile(r'(?is)(?:^|\\r?\\n)\\*{0,2}\\s*Price\\s+Variance\\s+Value\\s*\\*{0,2}\\s*\\r?\\n.*?(?=(?:\\r?\\n'+re.escape(HEADER_LINE)+r')|\\Z)')\n",
    "        return re.sub(pat, \"\", text or \"\").rstrip()\n",
    "    \n",
    "    def insert_price_variance_block(narr: str, variance_block: str) -> str:\n",
    "        \"\"\"\n",
    "        Insert **Price Variance Value** just BEFORE **Recommended Next Steps**.\n",
    "        Works whether that heading is plain or bold.\n",
    "        \"\"\"\n",
    "        hdr = re.compile(r'(?im)^\\s*\\*{0,2}\\s*Recommended\\s+Next\\s+Steps\\s*\\*{0,2}\\s*$', re.M)\n",
    "        m = hdr.search(narr)\n",
    "        block = f\"\\n**Price Variance Value**\\n{variance_block}\\n\"\n",
    "        if m:\n",
    "            pos = m.start()\n",
    "            prefix = narr[:pos]\n",
    "            suffix = narr[pos:]\n",
    "            return prefix + block + suffix + \"\\n\" \n",
    "        return narr + block\n",
    "    \n",
    "    \n",
    "    # ---------------- PROMPT (single change: your desired headings/format) --------\n",
    "    def build_refine_prompt(source_text: str) -> str:\n",
    "        \"\"\"\n",
    "        EXACTLY outputs 3 sections with your headers. NO 'Price Variance Value' section.\n",
    "        LLM must pull values ONLY from SOURCE (copy tokens; don't invent).\n",
    "        \"\"\"\n",
    "        no_risk = source_has_no_risk(source_text)\n",
    "        no_risk_note = (\n",
    "            \"- If the SOURCE indicates 'No Risk', keep it minimal:\\n\"\n",
    "            \"  • Why It Was Flagged: 'SARA has not flagged this transaction.'\\n\"\n",
    "            \"  • Recommended Next Steps: 1–2 light bullets (retain documentation / periodic review).\\n\"\n",
    "        )\n",
    "        return f\"\"\"\n",
    "    You are a senior procurement risk analyst. Use ONLY the SOURCE (verbatim tokens). Do NOT invent data.\n",
    "    Do NOT include any section titled 'Price Variance Value' (the caller inserts it).\n",
    "    For Executive Summary strictly pick up data from Key Description section only. Pick data correcponding to those value asked only.\n",
    "    DO NOT hallucinate. For Risk Flag pick data strictly from Context & Trigger Risk Scenario only.\n",
    "    HARD CONSTRAINTS:\n",
    "    - Avoid legal/accusatory terms (fraud, bribery, etc.).\n",
    "    - Preserve IDs and codes exactly if you reference them.\n",
    "    - If a value is not present in SOURCE, write \"(not available)\".\n",
    "    STRICT OUTPUT FORMAT (exact headers, bullets as shown, no extra sections):\n",
    "    **SARA AI — Explanation**\n",
    "    \n",
    "    **Executive Summary**\n",
    "    • Purchase Order: <'PO <po> / Item <item>' from 'PO / Item & PR Ref' (ignore anything after '&') or '(not available)'>\n",
    "    • Vendor: <'code – name' from 'Vendor no - Name' or '(not available)'>\n",
    "    • Material: <'code — text' from 'Material – Text, Type' (text before comma) or '(not available)'>\n",
    "    • Risk Flag: <from 'Risk Scenario' (include sub-risks if listed) or '(not available)'>\n",
    "    • Line Value: <copy 'Flagged Value: ...' or 'Net / Gross Value' or compute '<currency> <qty*unit_price, 2dp> (~<pct>% of total PO <total, 2dp>)' using ONLY numbers in SOURCE; else '(not available)'>\n",
    "    • Action: <short phrase from Context & Trigger like 'Needs Validation'/'High Risk' etc., else '(not available)'>\n",
    "    \n",
    "    **Why It Was Flagged**\n",
    "    • <bullet 1 grounded strictly in SOURCE (e.g., same-vendor price variance range if present only then else don't make a point )>\n",
    "    • <bullet 2 grounded strictly in SOURCE (e.g., cross-vendor discrepancy if present only then else don't make a point )>\n",
    "    • <bullet 3 grounded strictly in SOURCE (e.g., split POs ±60d if present only then else don't make a point )> \n",
    "    • <bullet 4 grounded strictly in SOURCE (e.g., Split POs PR-based if present only then else don't make a point )>\n",
    "    \n",
    "    **Recommended Next Steps**\n",
    "    1. <specific, actionable step grounded in SOURCE>\n",
    "    2. <specific, actionable step grounded in SOURCE>\n",
    "    3. <specific, actionable step grounded in SOURCE>\n",
    "    (Optionally a 4th if clearly supported by SOURCE.)\n",
    "    \n",
    "    {no_risk_note if no_risk else \"\"}\n",
    "    \n",
    "    SOURCE (read-only; copy tokens; do not reformat)\n",
    "    — BEGIN —\n",
    "    {source_text}\n",
    "    — END —\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    # ---------------- OLLAMA CLIENT (persistent session + retries) ----------------\n",
    "    class OllamaClient:\n",
    "        def __init__(self, base_url: str, model: str, options: dict):\n",
    "            self.base_url = base_url.rstrip(\"/\")\n",
    "            self.model = model\n",
    "            self.options = options or {}\n",
    "            self.sess = requests.Session()\n",
    "            self.sess.headers.update({\"Content-Type\": \"application/json\"})\n",
    "    \n",
    "        def _payload(self, prompt: str) -> dict:\n",
    "            return {\n",
    "                \"model\": self.model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\":\n",
    "     \"Be precise and conservative. Never invent data or numbers. Do not output an Evidence section. \"\n",
    "     \"Do not include numeric risk score in the Executive Summary; classification name may be used. \"\n",
    "     \"If mentioning a PO, include its item number (e.g., 'PO 4000003611 / Item 00010'). \"\n",
    "     \"Avoid legal/accusatory terms (fraud, corruption, etc.).\" },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                \"stream\": False,\n",
    "                \"options\": self.options\n",
    "            }\n",
    "    \n",
    "        @retry(\n",
    "            reraise=True,\n",
    "            stop=stop_after_attempt(3),\n",
    "            wait=wait_exponential_jitter(1, 6),\n",
    "            retry=retry_if_exception_type((requests.RequestException,))\n",
    "        )\n",
    "        def chat(self, prompt: str, timeout: int = 600) -> str:\n",
    "            r = self.sess.post(self.base_url, data=json.dumps(self._payload(prompt)), timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            return (data.get(\"message\", {}) or {}).get(\"content\", \"\") or \"\"\n",
    "    \n",
    "    # ---------------- SANITIZERS & FALLBACKS ----------------\n",
    "    def sanitize_banned_terms(text: str) -> str:\n",
    "        out = text\n",
    "        for pat in BANNED_TERMS:\n",
    "            out = re.sub(pat, \"risk\", out, flags=re.IGNORECASE)\n",
    "        return out\n",
    "    \n",
    "    def ensure_sections_present(text: str, source_fallback: str) -> str:\n",
    "        \"\"\"\n",
    "        Make sure the three headings exist; patch with minimal stubs if the model omitted any.\n",
    "        \"\"\"\n",
    "        t = text.strip()\n",
    "        if \"**SARA AI — Explanation**\" not in t:\n",
    "            t = \"**SARA AI — Explanation**\\n\\n\" + t\n",
    "        if \"**Executive Summary**\" not in t:\n",
    "            t += \"\\n\\n**Executive Summary**\\nSee Evidence.\"\n",
    "        if \"**Why It Was Flagged**\" not in t:\n",
    "            t += \"\\n\\n**Why It Was Flagged**\\n· See Evidence\"\n",
    "        if \"**Recommended Next Steps**\" not in t:\n",
    "            t += \"\\n\\nRecommended Next Steps\\n· Review Evidence with requester\"\n",
    "        # Never allow empty narrative\n",
    "        if not t.strip():\n",
    "            t = \"**SARA AI — Explanation**\\n\\n**Executive Summary**\\nSee Evidence.\\n\\n**Why It Was Flagged**\\n· See Evidence\\n\\n**Recommended Next Step**\\n· Review Evidence\"\n",
    "        return t\n",
    "    \n",
    "    def local_narrative_fallback(source_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Deterministic analyst narrative if the model fails. Grounded in parsed hints.\n",
    "        \"\"\"\n",
    "        risk_line = extract_risk_line(source_text)\n",
    "        primary   = extract_primary_line(source_text)\n",
    "        impact    = extract_business_impact_line(source_text)\n",
    "        drivers   = extract_risk_drivers(source_text)\n",
    "    \n",
    "        exec_bits = []\n",
    "        if risk_line: exec_bits.append(risk_line)\n",
    "        if primary:   exec_bits.append(primary)\n",
    "        if impact:    exec_bits.append(impact)\n",
    "        exec_summary = \" \".join(exec_bits) or \"This transaction was evaluated; see Evidence for details.\"\n",
    "    \n",
    "        why_bullets = []\n",
    "        if drivers:\n",
    "            for d in drivers[:4]:\n",
    "                why_bullets.append(f\"· Driver: {d}\")\n",
    "        if not why_bullets:\n",
    "            why_bullets = [\"· No specific risk drivers listed in the source.\"]\n",
    "    \n",
    "        next_steps = [\n",
    "            \"· Validate price justification and contract terms for this PO line.\",\n",
    "            \"· Cross-check material/vendor pricing against recent comparable lines.\",\n",
    "            \"· Confirm requester acknowledgment of any variance and approval trail.\"\n",
    "        ]\n",
    "    \n",
    "        text = (\n",
    "            \"**SARA AI — Explanation**\\n\\n\"\n",
    "            \"**Executive Summary**\\n\" + exec_summary + \"\\n\\n\"\n",
    "            \"**Why It Was Flagged**\\n\" + \"\\n\".join(why_bullets) + \"\\n\\n\"\n",
    "            \"**Recommended Next Steps**\\n\" + \"\\n\".join(next_steps)\n",
    "        )\n",
    "        return text\n",
    "    # Matchers for all five sections (bold/plain; flexible dashes/spaces)\n",
    "    SECTION_PATTERNS = [\n",
    "        re.compile(r'^\\s*\\*{0,2}\\s*SARA\\s*AI\\s*[—–-]?\\s*Explanation\\s*\\*{0,2}\\s*$', re.I),\n",
    "        re.compile(r'^\\s*\\*{0,2}\\s*Executive\\s+Summary\\s*\\*{0,2}\\s*$', re.I),\n",
    "        re.compile(r'^\\s*\\*{0,2}\\s*Why\\s+It\\s+Was\\s+Flagged\\s*\\*{0,2}\\s*$', re.I),\n",
    "        re.compile(r'^\\s*\\*{0,2}\\s*Price\\s+Variance(?:\\s+Value)?\\s*\\*{0,2}\\s*$', re.I),\n",
    "        re.compile(r'^\\s*\\*{0,2}\\s*Recommended\\s+Next\\s+Steps\\s*\\*{0,2}\\s*$', re.I),\n",
    "    ]\n",
    "    \n",
    "    def add_header_line_before_sections(text: str, header_line: str = HEADER_LINE, times: int = 1) -> str:\n",
    "        \"\"\"\n",
    "        Insert `header_line` (repeated `times`) immediately BEFORE each target section:\n",
    "        - SARA AI — Explanation\n",
    "        - Executive Summary\n",
    "        - Why It Was Flagged\n",
    "        - Price Variance / Price Variance Value\n",
    "        - Recommended Next Steps\n",
    "    \n",
    "        Preserves original content and avoids duplicating a header line if the nearest\n",
    "        non-empty line directly above the heading is already the same header.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\" if text is None else str(text)\n",
    "    \n",
    "        lines = text.splitlines(True)  # keep line endings\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line_wo_nl = lines[i].rstrip(\"\\n\")\n",
    "            if any(p.match(line_wo_nl) for p in SECTION_PATTERNS):\n",
    "                # Look upward past blank lines to the nearest non-empty line\n",
    "                j = i - 1\n",
    "                while j >= 0 and lines[j].strip() == \"\":\n",
    "                    j -= 1\n",
    "                already_has = (j >= 0 and lines[j].strip() == header_line)\n",
    "    \n",
    "                if not already_has:\n",
    "                    insert_block = (header_line + \"\\n\") * times\n",
    "                    lines.insert(i, insert_block)\n",
    "                    i += 1  # move past inserted block\n",
    "            i += 1\n",
    "    \n",
    "        return \"\".join(lines)\n",
    "    # ---------------- DRIVER ----------------\n",
    "    def refine_with_llm(df_in: pd.DataFrame,\n",
    "                        input_col: str = INPUT_COL,\n",
    "                        id_col: str = ID_COL,variance_col: str = VARIANCE_COL,\n",
    "                        output_col: str = OUTPUT_COL,\n",
    "                        checkpoint_every: int = CHECKPOINT_EVERY,\n",
    "                        checkpoint_path: str = CHECKPOINT_PATH,final_evidence:str=FINAL_INPUT_EVIDENCE) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Sends each row's 5-section text to the LLM and writes a polished analyst narrative.\n",
    "        - Never returns NaN (strings only).\n",
    "        - Appends Evidence (verbatim) always.\n",
    "        - Optional checkpointing during long runs.\n",
    "        \"\"\"\n",
    "        df2 = df_in.copy()\n",
    "        outs = []\n",
    "    \n",
    "        client = OllamaClient(OLLAMA_URL, MODEL, OPTIONS)\n",
    "    \n",
    "        for i, row in tqdm(df2.iterrows(), total=len(df2), desc=\"Refining with LLM\"):\n",
    "            main_source=row.get(final_evidence, \"\")\n",
    "            source = str(row.get(input_col, \"\") or \"\").strip()\n",
    "            variance_text = str(row.get(variance_col, \"\") or \"\").strip()\n",
    "            variance_block = build_price_variance_bullets(variance_text)\n",
    "            if not source:\n",
    "                outs.append(\"\")   # keep empty string, never NaN\n",
    "            else:\n",
    "                prompt = build_refine_prompt(source)\n",
    "                try:\n",
    "                    narrative = client.chat(prompt)\n",
    "                    if not narrative.strip():\n",
    "                        narrative = local_narrative_fallback(source)\n",
    "                except Exception:\n",
    "                    narrative = local_narrative_fallback(source)\n",
    "    \n",
    "                # sanitize & validate structure\n",
    "                # 2) Safety: if LLM added price block anyway, strip it; then insert ours\n",
    "                #draft = remove_price_variance_if_present(draft)\n",
    "                if 'NaN' not in variance_block:\n",
    "                    final_doc = insert_price_variance_block(narrative, variance_block)\n",
    "                else:\n",
    "                    final_doc=narrative\n",
    "                # 3) Sanitize banned terms\n",
    "                final_doc = sanitize_banned_terms(final_doc)\n",
    "                #narrative = sanitize_banned_terms(narrative)\n",
    "                final_doc = ensure_sections_present(final_doc, source)\n",
    "                # 4) add header_lines\n",
    "                final_doc = add_header_line_before_sections(final_doc, times=1)  # or times=2 for double lines\n",
    "    \n",
    "                # append Evidence (verbatim)\n",
    "                final_text = final_doc.rstrip() +\"\\n\" + main_source\n",
    "                outs.append(final_text)\n",
    "    \n",
    "            # checkpoint (optional)\n",
    "            if checkpoint_every and (i + 1) % checkpoint_every == 0:\n",
    "                tmp = df2.iloc[: i + 1].copy()\n",
    "                tmp[output_col] = pd.Series(outs, index=tmp.index, dtype=\"string\")\n",
    "                tmp.to_parquet(checkpoint_path, index=False)\n",
    "    \n",
    "        # finalize column (never NaN)\n",
    "        df2[output_col] = pd.Series(outs, index=df2.index, dtype=\"string\").fillna(\"\")\n",
    "        return df2\n",
    "    \n",
    "    # ---------------- HOW TO USE ----------------\n",
    "    #df = pd.read_excel(\"with_llm_explanations_with_risk_level.xlsx\")\n",
    "    #new_df=df[df['risk_level']!='No Risk']\n",
    "    #df_refined = refine_with_llm(new_df)  # add checkpoint_every=50 if you want periodic saves\n",
    "    #df_refined.to_excel(\"with_llm_refined_explanations.xlsx\", index=False)\n",
    "    #df_refined.to_parquet(\"with_llm_refined_explanations.parquet\", index=False)\n",
    "    \n",
    "    # Quick demo on a small slice (uncomment to test):\n",
    "    #demo = new_df.iloc[0:3].copy()\n",
    "    #demo_out = refine_with_llm(demo)\n",
    "    #for bid, out in zip(demo_out.get(ID_COL, pd.Series([None]*len(demo_out))), demo_out[OUTPUT_COL]):\n",
    "         #print(\"\\n\", bid, \"\\n\", out[:15000], \"\\n\", \"—\"*80)\n",
    "    \n",
    "    \n",
    "    #df = pd.read_excel(\"test_06_09_updated.xlsx\")\n",
    "    df = df_updated_final.copy()\n",
    "    new_df=df[df['updated_risk_level']!='No Risk']\n",
    "    df_refined =refine_with_llm(new_df)  # add checkpoint_every=50 if you want periodic saves\n",
    "    #df_refined.to_excel(\"with_llm_refined_explanations_06_09.xlsx\", index=False)\n",
    "    #df_refined.to_parquet(\"with_llm_refined_explanations_06_09.parquet\", index=False)\n",
    "    return df_refined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
