{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca98412-5dff-4978-9d1d-71955bb31f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "##############################################################################\n",
    "#  Cell 1 â€” Imports & common paths\n",
    "##############################################################################\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import joblib\n",
    "import logging\n",
    "from typing import List\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import logging\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set options to show full DataFrame output\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import joblib, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from typing import Any, Dict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "RAW_PO      = Path(\"po_output.pkl\") #(\"../Po_Invoice_Data/po_output_tushar.pkl\")      # raw PO data (input)\n",
    "FEAT_PO     = Path(\"Po_Invoice_Data/po_output_features_df_auto_model.pkl\")         # engineered features\n",
    "MODEL_PKL   = Path(\"Po_Invoice_Data/po_autoencoder_model.pkl\")         # tuned model file\n",
    "#CV_REPORT   = Path(\"Po_Invoice_Data/cv_results.csv\")            # param grid results\n",
    "#SCORING_OUT = Path(\"Po_Invoice_Data/scored_po.pkl\")             # predictions file\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Column standardisation helper\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "COL_MAP: Dict[str, str] = {\n",
    "    # raw_column                          # internal name\n",
    "    \"vendor_or_creditor_acct_no_hpd_po\": \"vendor_id\",\n",
    "    \"material_no_src_po\": \"material_id\",\n",
    "    \"purch_doc_date_hpd_po\": \"po_date\",\n",
    "    \"doc_change_date_src_po\": \"po_change_date\",\n",
    "    \"net_price_doc_curr_src_po\": \"net_price\",\n",
    "    \"gross_val_po_curr_src_po\": \"gross_val\",\n",
    "    \"exchange_rate_hpd_po\": \"exch_rate\",\n",
    "    \"requester_name_src_po\": \"requester\",\n",
    "    # unitâ€‘conversion numerators / denominators\n",
    "    \"p2o_unit_conv_num_src_po\": \"p2o_num\",\n",
    "    \"p2o_unit_conv_denom_src_po\": \"p2o_den\",\n",
    "    \"o2b_unit_conv_num_src_po\": \"o2b_num\",\n",
    "    \"o2b_unit_conv_denom_src_po\": \"o2b_den\",\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Feature engineering functions (pure, chainable)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _prep(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Rename key columns, parse dates & fill obvious nulls.\"\"\"\n",
    "    #df = df.rename(columns={k: v for k, v in COL_MAP.items() if k in df.columns})\n",
    "    df[\"purch_doc_date_hpd_po\"] = pd.to_datetime(df[\"purch_doc_date_hpd_po\"], errors=\"coerce\")\n",
    "    df[\"doc_change_date_src_po\"] = pd.to_datetime(df.get(\"doc_change_date_src_po\"), errors=\"coerce\")\n",
    "    df[\"vendor_or_creditor_acct_no_hpd_po\"] = df.get(\"vendor_or_creditor_acct_no_hpd_po\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"requester_name_src_po\"] = df.get(\"requester_name_src_po\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"exchange_rate_hpd_po\"] = df.get(\"exchange_rate_hpd_po\", 1.0).replace({0: np.nan}).fillna(1.0)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2a) Ruleâ€‘based features (rulesÂ 1,2,3,5)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def add_rule_metrics(df: pd.DataFrame,\n",
    "                     split_days: int = 60,\n",
    "                     price_var_days: int = 365) -> pd.DataFrame:\n",
    "    \"\"\"Add features mirroring Baldota P2P rules.\n",
    "\n",
    "    * vm_count/value = aggregation for **same vendor+material** within *split_days*\n",
    "    * vm_price_var_pct = price deviation (%) vs mean of past *price_var_days*\n",
    "    * mat_vendor_cnt & mat_price_var_pct analogues for material across vendors\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.sort_values(\"purch_doc_date_hpd_po\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Initialise\n",
    "    df[\"vm_count_%dd\" % split_days] = 0\n",
    "    df[\"vm_value_%dd\" % split_days] = 0.0\n",
    "    df[\"vm_price_var_pct_%dd\" % price_var_days] = 0.0\n",
    "    df[\"mat_vendor_cnt_%dd\" % split_days] = 0\n",
    "    df[\"mat_price_var_pct_%dd\" % price_var_days] = 0.0\n",
    "\n",
    "    # Preâ€‘extract convenient arrays for speed\n",
    "    po_dates = df[\"purch_doc_date_hpd_po\"].values\n",
    "    prices = df.get(\"net_price_doc_curr_src_po\").astype(float).values\n",
    "    vals = df.get(\"gross_val_po_curr_src_po\").astype(float).values\n",
    "\n",
    "    # --- Same vendor + material group logic ----------------------------------\n",
    "    for (v, m), idx in df.groupby([\"vendor_or_creditor_acct_no_hpd_po\", \"material_no_src_po\"]).groups.items():\n",
    "        d = po_dates[idx]\n",
    "        p = prices[idx]\n",
    "        vls = vals[idx]\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = d[loc]\n",
    "            # split window\n",
    "            win_mask = (d >= cur - np.timedelta64(split_days, \"D\")) & (d <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(\"vm_count_%dd\" % split_days)] = int(win_mask.sum())\n",
    "            df.iat[ridx, df.columns.get_loc(\"vm_value_%dd\" % split_days)] = float(vls[win_mask].sum())\n",
    "            # price variance window\n",
    "            var_mask = (d >= cur - np.timedelta64(price_var_days, \"D\")) & (d <= cur)\n",
    "            if var_mask.sum() > 1:\n",
    "                mean_price = p[var_mask].mean()\n",
    "                if mean_price:\n",
    "                    pct = abs(p[loc] - mean_price) / mean_price * 100\n",
    "                    df.iat[ridx, df.columns.get_loc(\"vm_price_var_pct_%dd\" % price_var_days)] = pct\n",
    "\n",
    "    # --- Materialâ€‘only group logic -------------------------------------------\n",
    "    for m, idx in df.groupby(\"material_no_src_po\").groups.items():\n",
    "        d = po_dates[idx]\n",
    "        p = prices[idx]\n",
    "        vendors = df.loc[idx, \"vendor_or_creditor_acct_no_hpd_po\"].values\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = d[loc]\n",
    "            win_mask = (d >= cur - np.timedelta64(split_days, \"D\")) & (d <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(\"mat_vendor_cnt_%dd\" % split_days)] = int(len(set(vendors[win_mask])))\n",
    "            var_mask = (d >= cur - np.timedelta64(price_var_days, \"D\")) & (d <= cur)\n",
    "            if var_mask.sum() > 1:\n",
    "                mean_price = p[var_mask].mean()\n",
    "                if mean_price:\n",
    "                    pct = abs(p[loc] - mean_price) / mean_price * 100\n",
    "                    df.iat[ridx, df.columns.get_loc(\"mat_price_var_pct_%dd\" % price_var_days)] = pct\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2b) Value & process metrics\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def add_value_and_timing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"conv_factor_p2o\"] = (\n",
    "        df.get(\"p2o_unit_conv_num_src_po\") / df.get(\"p2o_unit_conv_denom_src_po\")\n",
    "    ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "    df[\"conv_factor_o2b\"] = (\n",
    "        df.get(\"o2b_unit_conv_num_src_po\") / df.get(\"o2b_unit_conv_denom_src_po\")\n",
    "    ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "    df[\"po_change_lag_days\"] = (df.get(\"doc_change_date_src_po\") - df[\"purch_doc_date_hpd_po\"]).dt.days\n",
    "    df[\"base_value\"] = df.get(\"gross_val_po_curr_src_po\") * df.get(\"exchange_rate_hpd_po\")\n",
    "    p95 = df[\"gross_val_po_curr_src_po\"].quantile(0.95)\n",
    "    df[\"high_value_flag\"] = (df[\"gross_val_po_curr_src_po\"] >= p95).astype(int)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2c) Behavioural rolling windows\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _rolling_stats(df: pd.DataFrame, group_col: str, days: int,\n",
    "                   count_col: str, sum_col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[count_col] = 0\n",
    "    df[sum_col] = 0.0\n",
    "\n",
    "    po_dates = df[\"purch_doc_date_hpd_po\"].values\n",
    "    gross_vals = df[\"gross_val_po_curr_src_po\"].values\n",
    "\n",
    "    for key, idx in df.groupby(group_col).groups.items():\n",
    "        dates = po_dates[idx]\n",
    "        vals = gross_vals[idx]\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = dates[loc]\n",
    "            mask = (dates >= cur - np.timedelta64(days, \"D\")) & (dates <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(count_col)] = int(mask.sum())\n",
    "            df.iat[ridx, df.columns.get_loc(sum_col)] = float(vals[mask].sum())\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_behavioural_stats(df: pd.DataFrame, window_days: int = 30) -> pd.DataFrame:\n",
    "    df = _rolling_stats(df, \"requester_name_src_po\", window_days,\n",
    "                        \"req_po_count_%dd\" % window_days,\n",
    "                        \"req_val_sum_%dd\" % window_days)\n",
    "    df = _rolling_stats(df, \"vendor_or_creditor_acct_no_hpd_po\", window_days,\n",
    "                        \"vendor_po_count_%dd\" % window_days,\n",
    "                        \"vendor_val_sum_%dd\" % window_days)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Public orchestrator\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def build_features(raw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Endâ€‘toâ€‘end feature generator (no target leakage).\"\"\"\n",
    "    df = _prep(raw_df)\n",
    "    df = add_rule_metrics(df)\n",
    "    df = add_value_and_timing(df)\n",
    "    df = add_behavioural_stats(df)\n",
    "    return df\n",
    "def flag_split_po(df):\n",
    "    df = df.copy()\n",
    "    df['split_po_flag'] = 0  # Default 0\n",
    "\n",
    "    # Apply exclusion filters only for computation\n",
    "    exclude_doc_types = [\"AN\", \"AR\", \"MN\", \"QC\", \"QI\", \"QS\", \"RS\", \"SC\", \"SG\", \"SR\", \"SS\", \"ST\", \"TP\", \"TR\", \"UB\", \"WK\"]\n",
    "\n",
    "    filtered = df[\n",
    "        (~df['purch_doc_type_hpd_po'].isin(exclude_doc_types)) &\n",
    "        (df['purch_doc_type_hpd_po'].notna()) &\n",
    "        (~(df['po_item_del_flag_src_po'] == 'L')) &\n",
    "        (~df['plant_src_po'].fillna(\"\").astype(str).str.startswith(\"4\")) &\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['gross_val_po_curr_src_po'] >= 10) &\n",
    "        (df['purch_doc_date_hpd_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['company_code_src_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    filtered['purch_doc_date_hpd_po'] = pd.to_datetime(filtered['purch_doc_date_hpd_po'])\n",
    "\n",
    "    def get_group_key(row):\n",
    "        if pd.notna(row['material_no_src_po']):\n",
    "            return f\"{row['vendor_or_creditor_acct_no_hpd_po']}__{row['material_no_src_po']}\"\n",
    "        elif pd.notna(row['short_text_src_po']):\n",
    "            return f\"{row['vendor_or_creditor_acct_no_hpd_po']}__{row['short_text_src_po']}\"\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    filtered['split_key'] = filtered.apply(get_group_key, axis=1)\n",
    "    filtered = filtered[filtered['split_key'].notna()].copy()\n",
    "    filtered.sort_values(['split_key', 'purch_doc_date_hpd_po'], inplace=True)\n",
    "\n",
    "    flagged_po_set = set()\n",
    "\n",
    "    for key, group in filtered.groupby('split_key'):\n",
    "        dates = group['purch_doc_date_hpd_po'].reset_index(drop=True)\n",
    "        po_nos = group['purch_doc_no_src_po'].reset_index(drop=True)\n",
    "\n",
    "        for i in range(len(dates)):\n",
    "            date_i = dates[i]\n",
    "            po_i = po_nos[i]\n",
    "            mask = (\n",
    "                (dates >= date_i - pd.Timedelta(days=14)) &\n",
    "                (dates <= date_i + pd.Timedelta(days=14)) &\n",
    "                (po_nos != po_i)\n",
    "            )\n",
    "            if mask.sum() > 0:\n",
    "                flagged_po_set.add(po_i)\n",
    "\n",
    "    # Assign flag only to matching rows in original df\n",
    "    df['split_po_flag'] = df['purch_doc_no_src_po'].isin(flagged_po_set).astype(int)\n",
    "    return df\n",
    "def flag_intra_po_split(df, gross_threshold=10):\n",
    "    df = df.copy()\n",
    "    df['intra_po_split_flag'] = 0  # Default\n",
    "\n",
    "    df_valid = df[\n",
    "        df['gross_val_po_curr_src_po'].notna() &\n",
    "        df['vendor_or_creditor_acct_no_hpd_po'].notna() &\n",
    "        df['material_no_src_po'].notna() &\n",
    "        df['purch_doc_no_src_po'].notna()\n",
    "    ].copy()\n",
    "\n",
    "    group_cols = ['purch_doc_no_src_po', 'vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po']\n",
    "    grouped = df_valid.groupby(group_cols)\n",
    "\n",
    "    flagged_indexes = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        total_gross = group['gross_val_po_curr_src_po'].sum()\n",
    "        num_items = len(group)\n",
    "        all_below_threshold = group['gross_val_po_curr_src_po'].all()\n",
    "\n",
    "        if total_gross >= gross_threshold and num_items > 1 and all_below_threshold:\n",
    "            flagged_indexes.extend(group.index.tolist())\n",
    "\n",
    "    df.loc[flagged_indexes, 'intra_po_split_flag'] = 1\n",
    "    return df\n",
    "def flag_multiple_pos_per_pr_item(df):\n",
    "    df = df.copy()\n",
    "    df['multi_po_per_pr_flag'] = 0  # Default\n",
    "\n",
    "    # Only consider approved POs with valid PR and PR item\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        df['pr_no_src_po'].notna() &\n",
    "        df['pr_item_no_src_po'].notna() &\n",
    "        df['purch_doc_no_src_po'].notna()\n",
    "    ][['pr_no_src_po', 'pr_item_no_src_po', 'purch_doc_no_src_po']].drop_duplicates()\n",
    "\n",
    "    # Count number of unique POs per PR+Item\n",
    "    po_counts = df_valid.groupby(['pr_no_src_po', 'pr_item_no_src_po'])['purch_doc_no_src_po'].nunique()\n",
    "\n",
    "    # Identify PR+Items linked to more than one PO\n",
    "    multi_po_keys = po_counts[po_counts > 1].index.tolist()\n",
    "\n",
    "    # Create a set for fast lookup\n",
    "    multi_po_set = set(multi_po_keys)\n",
    "\n",
    "    # Flag in the main DataFrame\n",
    "    df['multi_po_per_pr_flag'] = df.apply(\n",
    "        lambda row: 1 if (row['pr_no_src_po'], row['pr_item_no_src_po']) in multi_po_set else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "def flag_same_vendor_price_increase(df, price_increase_threshold=0.05, months_range=6, flag_column='same_vendor_price_increase_flag'):\n",
    "    df = df.copy()\n",
    "    df[flag_column] = 0\n",
    "\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['material_no_src_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['net_price_doc_curr_src_po'].notna()) &\n",
    "        (df['purch_doc_date_hpd_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    df_valid['purch_doc_date_hpd_po'] = pd.to_datetime(df_valid['purch_doc_date_hpd_po'])\n",
    "    df_valid.sort_values(['vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po', 'purch_doc_date_hpd_po'], inplace=True)\n",
    "\n",
    "    group_cols = ['vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po']\n",
    "    flagged_indices = []\n",
    "\n",
    "    for _, group in df_valid.groupby(group_cols):\n",
    "        group = group.sort_values('purch_doc_date_hpd_po').reset_index()\n",
    "\n",
    "        for i in range(1, len(group)):\n",
    "            current_row = group.loc[i]\n",
    "            current_date = current_row['purch_doc_date_hpd_po']\n",
    "            current_price = current_row['net_price_doc_curr_src_po']\n",
    "\n",
    "            mask = group.loc[:i-1, 'purch_doc_date_hpd_po'] >= current_date - pd.DateOffset(months=months_range)\n",
    "            past_group = group.loc[:i-1][mask]\n",
    "\n",
    "            if not past_group.empty:\n",
    "                last_price = past_group['net_price_doc_curr_src_po'].iloc[-1]\n",
    "                if last_price > 0 and ((current_price - last_price) / last_price) >= price_increase_threshold:\n",
    "                    flagged_indices.append(current_row['index'])\n",
    "\n",
    "    df.loc[flagged_indices, flag_column] = 1\n",
    "    return df\n",
    "def flag_diff_vendor_price_variance(df, price_variance_threshold=0.05, months_range=6, flag_column='diff_vendor_price_variance_flag'):\n",
    "    df = df.copy()\n",
    "    df[flag_column] = 0\n",
    "\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['material_no_src_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['net_price_doc_curr_src_po'].notna()) &\n",
    "        (df['purch_doc_date_hpd_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    df_valid['purch_doc_date_hpd_po'] = pd.to_datetime(df_valid['purch_doc_date_hpd_po'])\n",
    "    df_valid = df_valid.sort_values(['material_no_src_po', 'purch_doc_date_hpd_po'])\n",
    "\n",
    "    for material, mat_group in df_valid.groupby('material_no_src_po'):\n",
    "        mat_group = mat_group.sort_values('purch_doc_date_hpd_po').reset_index()\n",
    "\n",
    "        for i in range(len(mat_group)):\n",
    "            current_row = mat_group.loc[i]\n",
    "            current_date = current_row['purch_doc_date_hpd_po']\n",
    "            current_price = current_row['net_price_doc_curr_src_po']\n",
    "\n",
    "            past_window = mat_group[\n",
    "                (mat_group['purch_doc_date_hpd_po'] < current_date) &\n",
    "                (mat_group['purch_doc_date_hpd_po'] >= current_date - pd.DateOffset(months=months_range))\n",
    "            ]\n",
    "\n",
    "            vendor_prices = past_window.groupby('vendor_or_creditor_acct_no_hpd_po')['net_price_doc_curr_src_po'].mean()\n",
    "            if not vendor_prices.empty:\n",
    "                max_price = vendor_prices.max()\n",
    "                min_price = vendor_prices.min()\n",
    "                if max_price > 0 and (max_price - min_price) / max_price >= price_variance_threshold:\n",
    "                    df.loc[current_row['index'], flag_column] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def final_parsing(df):\n",
    "        # === Parse Date Columns ===\n",
    "    date_cols = [\n",
    "        \"doc_change_date_src_po\",\n",
    "        \"doc_change_date_hpd_po\",\n",
    "        \"purch_doc_date_hpd_po\"\n",
    "    ]\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    \n",
    "    # === Feature Engineering ===\n",
    "    \n",
    "    # A. Date Features\n",
    "    df[\"po_doc_age_days\"] = (df[\"doc_change_date_hpd_po\"] - df[\"purch_doc_date_hpd_po\"]).dt.days\n",
    "    df[\"lead_time_po_vs_pr\"] = (df[\"purch_doc_date_hpd_po\"] - df[\"doc_change_date_src_po\"]).dt.days\n",
    "    df[\"po_day_of_week\"] = df[\"purch_doc_date_hpd_po\"].dt.dayofweek\n",
    "    df[\"po_day_of_month\"] = df[\"purch_doc_date_hpd_po\"].dt.day\n",
    "    df[\"po_month\"] = df[\"purch_doc_date_hpd_po\"].dt.month\n",
    "    \n",
    "    # B. Price & Value Features\n",
    "    df[\"price_per_unit\"] = df[\"net_val_po_curr_src_po\"] / df[\"quantity_src_po\"].replace(0, np.nan)\n",
    "    df[\"net_vs_gross_delta\"] = df[\"gross_val_po_curr_src_po\"] - df[\"net_val_po_curr_src_po\"]\n",
    "    df[\"price_variance_percent\"] = (df[\"net_val_po_curr_src_po\"] - df[\"gross_val_po_curr_src_po\"]) / df[\"gross_val_po_curr_src_po\"].replace(0, np.nan)\n",
    "    df[\"outline_agrmt_coverage\"] = df[\"outline_agrmt_tgt_val_doc_curr_src_po\"] / df[\"net_val_po_curr_src_po\"].replace(0, np.nan)\n",
    "    \n",
    "    # C. PO-PR Linkage Flags\n",
    "    df[\"has_pr_link\"] = df[\"pr_no_src_po\"].notna().astype(int)\n",
    "    df[\"has_pr_item_link\"] = df[\"pr_item_no_src_po\"].notna().astype(int)\n",
    "    \n",
    "    # D. Flags & Indicators\n",
    "    binary_cols = [\n",
    "        \"gr_indicator_src_po\", \"gr_invoice_verif_flag_src_po\",\n",
    "        \"inv_receipt_indicator_src_po\", \"release_indicator_hpd_po\",\n",
    "        \"release_status_hpd_po\", \"doc_release_incompl_flag_hpd_po\"\n",
    "    ]\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            df[col + \"_flag\"] = df[col].notna().astype(int)\n",
    "    \n",
    "    # E. Missing Signal\n",
    "    critical_cols = [\n",
    "        \"material_type_src_po\", \"material_no_src_po\",\n",
    "        \"vendor_or_creditor_acct_no_hpd_po\", \"gross_val_po_curr_src_po\",\n",
    "        \"net_price_doc_curr_src_po\"\n",
    "    ]\n",
    "    df[\"missing_critical_fields\"] = df[critical_cols].isnull().sum(axis=1)\n",
    "    \n",
    "    # F. Currency & Exchange Rate\n",
    "    df[\"has_exchange_rate\"] = df[\"exchange_rate_hpd_po\"].notna().astype(int)\n",
    "    df[\"log_exchange_rate\"] = np.log1p(df[\"exchange_rate_hpd_po\"].fillna(0))\n",
    "    \n",
    "    # G. Unit Conversion Features\n",
    "    df[\"p2o_conversion_ratio\"] = df[\"p2o_unit_conv_num_src_po\"] / df[\"p2o_unit_conv_denom_src_po\"].replace(0, np.nan)\n",
    "    df[\"o2b_conversion_ratio\"] = df[\"o2b_unit_conv_num_src_po\"] / df[\"o2b_unit_conv_denom_src_po\"].replace(0, np.nan)\n",
    "    \n",
    "    # H. Behavioral Flags\n",
    "    df[\"is_same_vendor_pr_po\"] = (\n",
    "        df[\"vendor_or_creditor_acct_no_hpd_po\"].notna() & df[\"base_id_src_po\"].notna()\n",
    "    ).astype(int)\n",
    "    df[\"has_rfq_status\"] = df[\"rfq_status_hpd_po\"].notna().astype(int)\n",
    "    df[\"purch_group_org_same\"] = (df[\"purch_group_hpd_po\"] == df[\"purch_org_hpd_po\"]).astype(int)\n",
    "    \n",
    "    # I. Rare Category Flagging\n",
    "    rare_cat_cols = [\"purch_doc_type_hpd_po\", \"purch_group_hpd_po\", \"vendor_or_creditor_acct_no_hpd_po\"]\n",
    "    for col in rare_cat_cols:\n",
    "        if col in df.columns:\n",
    "            freq_map = df[col].value_counts(normalize=True)\n",
    "            df[f\"{col}_is_rare\"] = df[col].map(freq_map) < 0.01\n",
    "\n",
    "    return df\n",
    "\n",
    "##############################################################################\n",
    "#  Cell 3 â€” Create / load features\n",
    "##############################################################################\n",
    "if FEAT_PO.exists():\n",
    "    print(\"âš¡ Loading cached features\")\n",
    "    df = pd.read_pickle(FEAT_PO)\n",
    "else:\n",
    "    print(\"ðŸš§ Generating features â€¦\")\n",
    "    raw_df  = pd.read_pickle(RAW_PO)\n",
    "    columns_to_drop = [\n",
    "    'base_id_po','rule_ids_po','P2P02067','P2P02068','P2P02070','P2P02072']\n",
    "    raw_df.drop(columns=[col for col in columns_to_drop if col in raw_df.columns], inplace=True)\n",
    "    feat_df = build_features(raw_df)\n",
    "    df=flag_split_po(feat_df)\n",
    "    df=flag_intra_po_split(df)\n",
    "    df = flag_multiple_pos_per_pr_item(df)\n",
    "    # Same vendor price jump\n",
    "    df = flag_same_vendor_price_increase(df, months_range=6, flag_column='same_vendor_price_increase_6m_flag')\n",
    "    df = flag_same_vendor_price_increase(df, months_range=12, flag_column='same_vendor_price_increase_12m_flag')\n",
    "    \n",
    "    # Diff vendor price variance\n",
    "    df = flag_diff_vendor_price_variance(df, months_range=6, flag_column='diff_vendor_price_variance_6m_flag')\n",
    "    df = flag_diff_vendor_price_variance(df, months_range=12, flag_column='diff_vendor_price_variance_12m_flag')\n",
    "    df=final_parsing(df)\n",
    "    df.to_pickle(FEAT_PO)\n",
    "    print(\"Feature shape:\", df.shape)\n",
    "\n",
    "##############################################################################\n",
    "#  Cell 4 â€” Fast train (no hyper-parameter search, no RFECV)\n",
    "##############################################################################\n",
    "# =======================\n",
    "# Reproducibility\n",
    "# =======================\n",
    "# =========================\n",
    "# Isolation Forest pipeline\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# Paths to persist artifacts\n",
    "# -------------------------\n",
    "ARTIFACT_DIR = Path(\"Po_Invoice_Data\")\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PREPROC_PKL   = ARTIFACT_DIR / \"iso_preprocessor.pkl\"\n",
    "MODEL_PKL     = ARTIFACT_DIR / \"isoforest_model.pkl\"\n",
    "THRESHOLD_NPY = ARTIFACT_DIR / \"isoforest_threshold.npy\"\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "# ==== Seeds for reproducibility ====\n",
    "seed = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def build_preprocessor(df: pd.DataFrame, drop_cols=\"rft_by_engine_po\"):\n",
    "    \"\"\"ColumnTransformer for mixed dtypes. Keeps OHE sparse to avoid memory blowups.\"\"\"\n",
    "    #target = \"rft_by_engine_po\"\n",
    "    #df[target] = df[target].fillna(0).astype(int)\n",
    "    drop_cols = drop_cols\n",
    "    X = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    num_cols = [c for c in X.columns if X[c].dtype.kind in \"if\"]\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype.kind not in \"if\"]\n",
    "\n",
    "    preproc = ColumnTransformer([\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\",  StandardScaler(with_mean=False)),  # keep sparse compatibility\n",
    "        ]), num_cols),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            # sparse_output=True (default) keeps output CSR; good for memory\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=10))\n",
    "        ]), cat_cols)\n",
    "    ])\n",
    "    return preproc, num_cols, cat_cols\n",
    "\n",
    "def train_isolation_forest(df: pd.DataFrame, target_col: str = \"rft_by_engine_po\"):\n",
    "    \"\"\"\n",
    "    Trains IsolationForest on NORMAL data (y==0), derives threshold from training normals,\n",
    "    evaluates on all data, and saves artifacts.\n",
    "    \"\"\"\n",
    "    # ----- Prepare -----\n",
    "    df = df.copy()\n",
    "    df[target_col] = df[target_col].fillna(0).astype(int)\n",
    "\n",
    "    # columns you don't want in the features (IDs, clear targets, etc.)\n",
    "    drop_cols = [target_col]  # add any other obvious ID fields if needed\n",
    "    preproc, num_cols, cat_cols = build_preprocessor(df, drop_cols=drop_cols)\n",
    "\n",
    "    # Fit preprocessor\n",
    "    X_all = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    X_all_enc = preproc.fit_transform(X_all)\n",
    "\n",
    "    # Split normals/anomalies\n",
    "    y = df[target_col].values\n",
    "    X_norm = X_all_enc[y == 0]\n",
    "\n",
    "#Chosen percentile: 91.5, threshold=0.308220\n",
    "\n",
    "    # ----- Train ISO -----\n",
    "    iso = IsolationForest(random_state= 42,\n",
    "        n_estimators= 685, max_samples=  0.6523068845866853, \n",
    "       max_features= 0.5488360570031919, bootstrap= False, contamination= 'auto',\n",
    "        n_jobs=1\n",
    "    )\n",
    "    iso.fit(X_norm)\n",
    "\n",
    "    # ----- Scores & Threshold -----\n",
    "    # score_samples: higher = more normal, lower = more anomalous\n",
    "    #train_norm_scores = iso.score_samples(X_norm)\n",
    "    # choose a quantile of NORMAL scores as threshold (e.g., 5th percentile)\n",
    "    threshold = 0.310220 #np.quantile(train_norm_scores, 0.05)\n",
    "    thr_normal_space = -threshold\n",
    "\n",
    "    # Evaluate on full set\n",
    "    all_scores = iso.score_samples(X_all_enc)\n",
    "    # predict anomaly if score < threshold\n",
    "    y_pred = (all_scores < thr_normal_space).astype(int)  # 1 = anomaly, 0 = normal (to match your convention)\n",
    "    print(\"\\nðŸ“Š Classification Report (Isolation Forest):\")\n",
    "    print(classification_report(y, y_pred, target_names=[\"No Risk\", \"Fraud\"]))\n",
    "\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    ConfusionMatrixDisplay(cm, display_labels=[\"No Risk\", \"Fraud\"]).plot(cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix â€” Isolation Forest\")\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: visualize score distribution\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(all_scores[y==0], bins=60, alpha=0.7, label=\"Normal (y=0)\")\n",
    "    plt.hist(all_scores[y==1], bins=60, alpha=0.7, label=\"Fraud (y=1)\")\n",
    "    plt.axvline(threshold, linestyle=\"--\", label=f\"Threshold={threshold:.4f}\")\n",
    "    plt.xlabel(\"IsolationForest score (higher = more normal)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Score Distribution â€” Isolation Forest\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ----- Save artifacts -----\n",
    "    joblib.dump(preproc, PREPROC_PKL)\n",
    "    joblib.dump(iso, MODEL_PKL)\n",
    "    np.save(THRESHOLD_NPY, np.array([thr_normal_space], dtype=np.float64))\n",
    "    print(f\"\\nâœ… Saved: {PREPROC_PKL.name}, {MODEL_PKL.name}, {THRESHOLD_NPY.name}\")\n",
    "\n",
    "    # Attach scores/preds back if you want\n",
    "    out = df.copy()\n",
    "    out[\"iso_score\"] = all_scores\n",
    "    out[\"iso_flag\"]  = y_pred\n",
    "    return out, {\"threshold\": float(threshold), \"num_cols\": num_cols, \"cat_cols\": cat_cols}\n",
    "\n",
    "\n",
    "trained_df, meta=train_isolation_forest(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb57db8a-8396-4d57-8703-734b7f394747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef59ab2a-311f-4fbf-97f4-74dddac65a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, joblib, optuna\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ==== Seeds for reproducibility ====\n",
    "seed = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "# df should already be your engineered dataset\n",
    "RAW =FEAT_PO      # <- your already-featured DF or run your feature builder before this\n",
    "df = pd.read_pickle(RAW).copy()\n",
    "# ==== Prep data (use your existing df) ====\n",
    "target = \"rft_by_engine_po\"\n",
    "df[target] = df[target].fillna(0).astype(int)\n",
    "y = df[target].values\n",
    "X = df.drop(columns=[target])\n",
    "\n",
    "num_cols = [c for c in X.columns if X[c].dtype.kind in \"if\"]\n",
    "cat_cols = [c for c in X.columns if X[c].dtype.kind not in \"if\"]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ]), num_cols),\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True, min_frequency=10))\n",
    "    ]), cat_cols)\n",
    "])\n",
    "\n",
    "X_scaled = preprocessor.fit_transform(X)\n",
    "\n",
    "# Hold-out split so the study is stable\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=seed, stratify=y\n",
    ")\n",
    "\n",
    "def eval_with_threshold_sweep(scores, y_true, normal_mask, goal=\"f1\", min_recall=None):\n",
    "    \"\"\"\n",
    "    scores: higher = more anomalous\n",
    "    Sweep thresholds on normal distribution percentiles and return best F1 for fraud.\n",
    "    Optionally enforce a minimum recall for fraud.\n",
    "    \"\"\"\n",
    "    pct_grid = list(np.arange(80, 99.5, 0.5)) + [99.9]\n",
    "    best = {\"score\": -1, \"pct\": None, \"report\": None, \"preds\": None, \"thr\": None}\n",
    "    # If no normals in mask, fallback to global percentiles\n",
    "    base_scores = scores[normal_mask] if normal_mask.any() else scores\n",
    "    for p in pct_grid:\n",
    "        thr = np.percentile(base_scores, p)\n",
    "        preds = (scores > thr).astype(int)  # 1 = fraud\n",
    "        report = classification_report(y_true, preds, output_dict=True, zero_division=0)\n",
    "        fraud_f1 = report[\"1\"][\"f1-score\"]\n",
    "        fraud_recall = report[\"1\"][\"recall\"]\n",
    "        if min_recall is not None and fraud_recall < min_recall:\n",
    "            continue\n",
    "        metric = fraud_f1 if goal == \"f1\" else fraud_recall\n",
    "        if metric > best[\"score\"]:\n",
    "            best = {\"score\": metric, \"pct\": p, \"report\": report, \"preds\": preds, \"thr\": thr}\n",
    "    return best\n",
    "\n",
    "# ==== Optuna objective ====\n",
    "def objective(trial):\n",
    "    # Hyperparams\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 200, 800)\n",
    "    max_samples  = trial.suggest_float(\"max_samples\", 0.5, 1.0)\n",
    "    max_features = trial.suggest_float(\"max_features\", 0.5, 1.0)\n",
    "    bootstrap    = trial.suggest_categorical(\"bootstrap\", [False, True])\n",
    "\n",
    "    # IMPORTANT: contamination only affects IFâ€™s internal score offset.\n",
    "    # We ignore that and choose our own threshold, so set it small or 'auto'.\n",
    "    iso = IsolationForest(\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "        max_features=max_features,\n",
    "        contamination='auto',\n",
    "        bootstrap=bootstrap,\n",
    "        random_state=seed,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    # ==== Train ONLY on normal data ====\n",
    "    X_tr_norm = X_tr[y_tr == 0]\n",
    "    iso.fit(X_tr_norm)\n",
    "\n",
    "    # Scores on test (bigger = more anomalous)\n",
    "    scores = -iso.score_samples(X_te)\n",
    "\n",
    "    # Sweep threshold on normals in the TEST set\n",
    "    normal_mask = (y_te == 0)\n",
    "    best = eval_with_threshold_sweep(\n",
    "        scores, y_te, normal_mask,\n",
    "        goal=\"f1\",              # or \"recall\"\n",
    "        min_recall=None         # or e.g. 0.65 to force higher recall\n",
    "    )\n",
    "    # Maximize fraud F1\n",
    "    return best[\"score\"]\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=seed))\n",
    "study.optimize(objective, n_trials=40, show_progress_bar=False)\n",
    "\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "\n",
    "# ==== Train final model with best params & report ====\n",
    "best_params = study.best_params\n",
    "final_iso = IsolationForest(\n",
    "    random_state=seed, n_jobs=1, contamination='auto', **best_params\n",
    ")\n",
    "final_iso.fit(X_scaled[y == 0])  # fit on all normals\n",
    "\n",
    "final_scores = -final_iso.score_samples(X_scaled)\n",
    "best_final = eval_with_threshold_sweep(final_scores, y, (y == 0), goal=\"f1\")\n",
    "print(f\"Chosen percentile: {best_final['pct']}, threshold={best_final['thr']:.6f}\")\n",
    "print(\"\\nFinal classification report:\\n\",\n",
    "      classification_report(y, best_final[\"preds\"], digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f79a8-d058-43e6-b4d7-240f7c3509fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a61a451-27f3-4bfd-9f5a-14eda0e02621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, joblib, optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ==== Seeds ====\n",
    "seed = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# ==== Load featured data ====\n",
    "df = pd.read_pickle(FEAT_PO).copy()\n",
    "target = \"rft_by_engine_po\"\n",
    "df[target] = df[target].fillna(0).astype(int)\n",
    "y_all = df[target].values\n",
    "X_all = df.drop(columns=[target])\n",
    "\n",
    "num_cols = [c for c in X_all.columns if X_all[c].dtype.kind in \"if\"]\n",
    "cat_cols = [c for c in X_all.columns if X_all[c].dtype.kind not in \"if\"]\n",
    "\n",
    "def make_preprocessor():\n",
    "    return ColumnTransformer([\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\",  StandardScaler(with_mean=False))\n",
    "        ]), num_cols),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True, min_frequency=10))\n",
    "        ]), cat_cols)\n",
    "    ])\n",
    "\n",
    "# Hold-out split (raw, before preprocessing)\n",
    "X_tr_raw, X_te_raw, y_tr, y_te = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=seed, stratify=y_all\n",
    ")\n",
    "\n",
    "def eval_with_threshold_sweep(scores, y_true, normal_mask, goal=\"f1\", min_recall=None):\n",
    "    # scores: larger = more anomalous (we'll pass -score_samples)\n",
    "    pct_grid = list(np.arange(80, 99.5, 0.5)) + [99.9]\n",
    "    best = {\"score\": -1, \"pct\": None, \"report\": None, \"preds\": None, \"thr\": None}\n",
    "    base = scores[normal_mask] if normal_mask.any() else scores\n",
    "    for p in pct_grid:\n",
    "        thr = np.percentile(base, p)\n",
    "        preds = (scores > thr).astype(int)  # 1=fraud\n",
    "        rep = classification_report(y_true, preds, output_dict=True, zero_division=0)\n",
    "        f1_1 = rep[\"1\"][\"f1-score\"]\n",
    "        rec_1 = rep[\"1\"][\"recall\"]\n",
    "        if min_recall is not None and rec_1 < min_recall:\n",
    "            continue\n",
    "        metric = f1_1 if goal == \"f1\" else rec_1\n",
    "        if metric > best[\"score\"]:\n",
    "            best = {\"score\": metric, \"pct\": p, \"report\": rep, \"preds\": preds, \"thr\": thr}\n",
    "    return best\n",
    "\n",
    "# ==== Optuna objective (no leakage) ====\n",
    "def objective(trial):\n",
    "    # Preprocess using TRAIN ONLY\n",
    "    preproc = make_preprocessor()\n",
    "    X_tr = preproc.fit_transform(X_tr_raw)\n",
    "    X_te = preproc.transform(X_te_raw)\n",
    "\n",
    "    # Hyperparams\n",
    "    iso = IsolationForest(\n",
    "        n_estimators=trial.suggest_int(\"n_estimators\", 200, 800),\n",
    "        max_samples=trial.suggest_float(\"max_samples\", 0.5, 1.0),\n",
    "        max_features=trial.suggest_float(\"max_features\", 0.5, 1.0),\n",
    "        contamination='auto',              # we'll pick threshold ourselves\n",
    "        bootstrap=trial.suggest_categorical(\"bootstrap\", [False, True]),\n",
    "        random_state=seed,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Train on normals only\n",
    "    iso.fit(X_tr[y_tr == 0])\n",
    "\n",
    "    # Scores on TEST â†’ negate so higher = more anomalous\n",
    "    scores = -iso.score_samples(X_te)\n",
    "\n",
    "    # Sweep threshold on normals in TEST\n",
    "    best = eval_with_threshold_sweep(scores, y_te, (y_te == 0), goal=\"f1\", min_recall=None)\n",
    "    return best[\"score\"]\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=seed))\n",
    "study.optimize(objective, n_trials=40, show_progress_bar=False)\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "\n",
    "# ==== Train final artifacts on ALL data (no leakage in tuning) ====\n",
    "final_preproc = make_preprocessor()\n",
    "X_all_enc = final_preproc.fit_transform(X_all)\n",
    "\n",
    "best_params = study.best_params\n",
    "final_iso = IsolationForest(\n",
    "    contamination='auto', random_state=seed, n_jobs=-1, **best_params\n",
    ")\n",
    "final_iso.fit(X_all_enc[y_all == 0])\n",
    "\n",
    "# Choose final threshold by sweeping on a fresh hold-out from ALL data\n",
    "# (Or keep the same X_te_raw split used above)\n",
    "_, X_hold_raw, _, y_hold = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=seed, stratify=y_all\n",
    ")\n",
    "X_hold = final_preproc.transform(X_hold_raw)\n",
    "hold_scores = -final_iso.score_samples(X_hold)\n",
    "best_final = eval_with_threshold_sweep(hold_scores, y_hold, (y_hold == 0), goal=\"f1\")\n",
    "thr = float(best_final[\"thr\"])\n",
    "print(f\"Chosen percentile: {best_final['pct']}, threshold={thr:.6f}\")\n",
    "print(\"\\nFinal report on hold-out:\\n\", classification_report(y_hold, best_final[\"preds\"], digits=3))\n",
    "\n",
    "# ==== Save artifacts ====\n",
    "#joblib.dump(final_preproc, \"Po_Invoice_Data/iso_preprocessor.pkl\")\n",
    "#joblib.dump(final_iso, \"Po_Invoice_Data/isoforest_model.pkl\")\n",
    "#np.save(\"Po_Invoice_Data/isoforest_threshold.npy\", np.array([thr], dtype=np.float64))\n",
    "#print(\"âœ… Saved preprocessor, model, threshold.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
