{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894b755-5159-4b7d-9fa2-c8df9c93d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ROCK-SOLID LLM REFINEMENT (Ollama) =======================================\n",
    "# Input : df[\"llm_explanation\"]  (your 5-section deterministic text)\n",
    "# Output: df[\"llm_refined_explanation\"] (analyst-style narrative + Evidence verbatim)\n",
    "# ==============================================================================\n",
    "\n",
    "import os, re, json, requests, pandas as pd\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential_jitter, retry_if_exception_type\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def ai_explanation_part_2(df_refined):\n",
    "    # ---------------- CONFIG ----------------\n",
    "    ID_COL                 = \"base_id_src_po\"\n",
    "    INPUT_COL              = \"llm_explanation\"\n",
    "    FINAL_INPUT_EVIDENCE   = \"updated_evidence_text\"\n",
    "    VARIANCE_COL           = \"variance_summary\"\n",
    "    OUTPUT_COL             = \"llm_refined_explanation\"\n",
    "    OLLAMA_URL             = \"http://127.0.0.1:11434/api/chat\"\n",
    "    MODEL                  = \"llama3:70b\"\n",
    "    \n",
    "    # Generation knobs (conservative)\n",
    "    OPTIONS = {\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1,\n",
    "        \"num_ctx\": 8192,\n",
    "        \"num_predict\": 1400\n",
    "    }\n",
    "    \n",
    "    # Banned terms (post-filter)\n",
    "    BANNED_TERMS = [\n",
    "        r\"\\bfraud\\b\", r\"\\bfraudulent\\b\", r\"\\bscam\\b\", r\"\\bscamming\\b\",\n",
    "        r\"\\bcorruption\\b\", r\"\\bbribe\\b\", r\"\\bbribery\\b\"\n",
    "    ]\n",
    "    HEADER_LINE = \"_\" * 152\n",
    "    \n",
    "    # Checkpointing (optional)\n",
    "    CHECKPOINT_EVERY = 0\n",
    "    CHECKPOINT_PATH  = \"with_llm_refined_explanations_checkpoint_08_09.parquet\"\n",
    "    \n",
    "    # ---------------- SMALL PARSERS (robust for your format) ----------------\n",
    "    def _find_block(src: str, title_regex: str) -> str:\n",
    "        \"\"\"\n",
    "        Return lines AFTER a heading that matches title_regex (numbered and/or bold) until next heading or EOF.\n",
    "        Accepts lines like: **2. Key Description (PO Details)**  or  2. Key Description  or  **Risk Drivers**\n",
    "        \"\"\"\n",
    "        src = src or \"\"\n",
    "        lines = src.splitlines()\n",
    "        # heading: optional **, optional \"n.\", the title_regex, optional \"(...)\", optional **, and only whitespace around\n",
    "        head_pat = re.compile(rf\"^\\s*\\*{{0,2}}\\s*(?:\\d+\\.\\s*)?{title_regex}(?:\\s*\\([^)]+\\))?\\s*\\*{{0,2}}\\s*$\", re.I)\n",
    "        start = None\n",
    "        for i, ln in enumerate(lines):\n",
    "            if head_pat.match(ln.strip()):\n",
    "                start = i + 1\n",
    "                break\n",
    "        if start is None:\n",
    "            return \"\"\n",
    "        # next heading detector (very forgiving)\n",
    "        next_head_pat = re.compile(r\"^\\s*\\*{0,2}\\s*(?:\\d+\\.\\s*)?[A-Z][A-Za-z0-9 &/()\\-]+(?:\\s*&\\s*[A-Z][^)]*)?\\s*\\*{0,2}\\s*$\")\n",
    "        out = []\n",
    "        for ln in lines[start:]:\n",
    "            if next_head_pat.match(ln.strip()):\n",
    "                break\n",
    "            out.append(ln)\n",
    "        while out and out[-1].strip() == \"\":\n",
    "            out.pop()\n",
    "        return \"\\n\".join(out)\n",
    "    \n",
    "    def extract_risk_line(src: str) -> str:\n",
    "        ctx = _find_block(src, r\"Context\\s*&\\s*Trigger\")\n",
    "        for ln in ctx.splitlines():\n",
    "            if \"Risk Score\" in ln:\n",
    "                return ln.strip()\n",
    "        return \"\"\n",
    "    \n",
    "    def extract_primary_line(src: str) -> str:\n",
    "        ctx = _find_block(src, r\"Context\\s*&\\s*Trigger\")\n",
    "        for ln in ctx.splitlines():\n",
    "            if \"Primary Risk Scenario\" in ln:\n",
    "                return ln.strip()\n",
    "        return \"\"\n",
    "    \n",
    "    def extract_risk_drivers(src: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Your drivers are plain lines (not bullets). Accept any non-empty trimmed line.\n",
    "        \"\"\"\n",
    "        blk = _find_block(src, r\"Risk\\s*Drivers\")\n",
    "        drivers = []\n",
    "        for ln in blk.splitlines():\n",
    "            t = ln.strip().lstrip(\"•-*· \").rstrip(\". \")\n",
    "            if t:\n",
    "                drivers.append(t)\n",
    "        return drivers\n",
    "    \n",
    "    def extract_business_impact_line(src: str) -> str:\n",
    "        blk = _find_block(src, r\"Business\\s*Impact\")\n",
    "        for ln in blk.splitlines():\n",
    "            t = ln.strip()\n",
    "            if t and (\"Suspicious\" in t or \"Line Value\" in t or \"Flagged Value\" in t or \"Overall Impact\" in t):\n",
    "                return t\n",
    "        for ln in blk.splitlines():\n",
    "            if ln.strip():\n",
    "                return ln.strip()\n",
    "        return \"\"\n",
    "    \n",
    "    def source_has_no_risk(source_text: str) -> bool:\n",
    "        return bool(re.search(r\"\\bNo\\s*Risk\\b\", source_text or \"\", re.IGNORECASE))\n",
    "    \n",
    "    # ---------------- Price Variance (your exact format) ----------------\n",
    "    def _normalize(s: str) -> str:\n",
    "        if s is None: return \"\"\n",
    "        s = str(s)\n",
    "        s = s.replace(\"—\", \"-\").replace(\"–\", \"-\").replace(\"₹\", \"INR \")\n",
    "        s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "        return s.strip()\n",
    "    \n",
    "    def _first(v: str, patterns: list[str]):\n",
    "        for p in patterns:\n",
    "            m = re.search(p, v, flags=re.I)\n",
    "            if m: return m.group(1).strip()\n",
    "        return None\n",
    "    \n",
    "    def _clean_arrow(s: str | None) -> str | None:\n",
    "        if not s: return None\n",
    "        t = re.sub(r\"^[•\\-\\u2022>\\u2192]+\\s*\", \"\", s.strip())\n",
    "        return f\"• {t}\"\n",
    "    \n",
    "    def build_price_variance_bullets(variance_text: str) -> str:\n",
    "        v = _normalize(variance_text)\n",
    "        curr  = _first(v, [r\"Current\\s*Price\\s*(?:Per\\s*Unit|/Unit)\\s*:\\s*([^|;]+)\"])\n",
    "        avg   = _first(v, [r\"Avg(?:erage)?\\s*Price\\s*(?:Per\\s*Unit|/Unit)(?:\\s*\\([^)]+\\))?\\s*:\\s*([^|;]+)\"])\n",
    "        delta = _first(v, [r\"[Δ∆]\\s*/?\\s*Unit\\s*:\\s*([^|;]+)\", r\"Variance\\s*Value\\s*Per\\s*Unit\\s*:\\s*([^|;]+)\"])\n",
    "        total = _first(v, [r\"Total\\s*Variance\\s*Value\\s*:\\s*([^|;]+)\"])\n",
    "        arrow = _clean_arrow(_first(v, [r\"(→\\s*[^|;]+)\"])) or \"• Current PO price is slightly below/high then average price.\"\n",
    "        return \"\\n\".join([\n",
    "            f\"• Current Price Per Unit: {curr or 'None'}\",\n",
    "            \"• Average Price Per Unit (Same and Different Vendor Compared Transactions from the compared transactions examples): \" + (avg or 'None'),\n",
    "            f\"• Variance Value Per Unit: {delta or 'None'}\",\n",
    "            arrow,\n",
    "            \"• Total Variance Value: \" + (total or 'None') + \" {Value = (Current price per unit - Average price per unit) x Current Qty}\"\n",
    "        ])\n",
    "    \n",
    "    def insert_price_variance_block(narr: str, variance_block: str) -> str:\n",
    "        hdr = re.compile(r'(?im)^\\s*\\*{0,2}\\s*Recommended\\s+Next\\s+Steps\\s*\\*{0,2}\\s*$', re.M)\n",
    "        m = hdr.search(narr)\n",
    "        block = f\"\\n**Price Variance Value**\\n{variance_block}\\n\"\n",
    "        if m:\n",
    "            pos = m.start()\n",
    "            return narr[:pos] + block + narr[pos:] + \"\\n\"\n",
    "        return narr + block\n",
    "    \n",
    "    # ---------------- PROMPT ----------------\n",
    "    def build_refine_prompt(source_text: str) -> str:\n",
    "        no_risk = source_has_no_risk(source_text)\n",
    "        no_risk_note = (\n",
    "            \"- If the SOURCE indicates 'No Risk', keep it minimal:\\n\"\n",
    "            \"  • Why It Was Flagged: 'SARA has not flagged this transaction.'\\n\"\n",
    "            \"  • Recommended Next Steps: 1–2 light bullets (retain documentation / periodic review).\\n\"\n",
    "        )\n",
    "        return f\"\"\"\n",
    "    You are a senior procurement risk analyst. Use ONLY the SOURCE (verbatim tokens). Do NOT invent data.\n",
    "    Do NOT include any section titled 'Price Variance Value' (the caller inserts it).\n",
    "    For Executive Summary strictly pick up data from Key Description section only. Pick data corresponding to those value asked only.\n",
    "    DO NOT hallucinate. For Risk Flag pick data strictly from Context & Trigger Risk Scenario only.\n",
    "    HARD CONSTRAINTS:\n",
    "    - Avoid legal/accusatory terms (fraud, bribery, etc.).\n",
    "    - Preserve IDs and codes exactly if you reference them.\n",
    "    - If a value is not present in SOURCE, write \"(not available)\".\n",
    "    - Give OUTPUT IN THE EXACT FORMAT ONLY. No extra text outside the template.\n",
    "    STRICT OUTPUT FORMAT (exact headers, bullets as shown, no extra sections):\n",
    "    **SARA AI — Explanation**\n",
    "    \n",
    "    **Executive Summary**\n",
    "    • Purchase Order: <'PO <po> / Item <item>' from 'PO / Item & PR Ref' (ignore anything after '&') or '(not available)'>\n",
    "    • Vendor: <'code – name' from 'Vendor no - Name' or '(not available)'>\n",
    "    • Material: <'code — text' from 'Material – Text, Type' (text before comma) or '(not available)'>\n",
    "    • Risk Flag: <from 'Risk Scenario' (include sub-risks if listed) or '(not available)'>\n",
    "    • Line Value: <copy 'Flagged Value: ...' or 'Net / Gross Value' or compute '<currency> <qty*unit_price, 2dp>' using ONLY numbers in SOURCE; else '(not available)'>\n",
    "    • Action: <short phrase from Context & Trigger like 'Needs Validation'/'High Risk' etc., else '(not available)'>\n",
    "    \n",
    "    **Why It Was Flagged**\n",
    "    • <bullet grounded strictly in SOURCE (only if present)>\n",
    "    • <bullet grounded strictly in SOURCE (only if present)>\n",
    "    • <bullet grounded strictly in SOURCE (only if present)>\n",
    "    • <bullet grounded strictly in SOURCE (only if present)>\n",
    "    \n",
    "    **Recommended Next Steps**\n",
    "    1. <specific, actionable step grounded in SOURCE>\n",
    "    2. <specific, actionable step grounded in SOURCE>\n",
    "    3. <specific, actionable step grounded in SOURCE>\n",
    "    \n",
    "    {no_risk_note if no_risk else \"\"}\n",
    "    \n",
    "    SOURCE (read-only; copy tokens; do not reformat)\n",
    "    — BEGIN —\n",
    "    {source_text}\n",
    "    — END —\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    # ---------------- OLLAMA CLIENT (persistent session + retries) ----------------\n",
    "    class OllamaClient:\n",
    "        def __init__(self, base_url: str, model: str, options: dict):\n",
    "            self.base_url = base_url.rstrip(\"/\")\n",
    "            self.model = model\n",
    "            self.options = options or {}\n",
    "            self.sess = requests.Session()\n",
    "            self.sess.headers.update({\"Content-Type\": \"application/json\"})\n",
    "    \n",
    "        def _payload(self, prompt: str) -> dict:\n",
    "            return {\n",
    "                \"model\": self.model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\":\n",
    "     \"Be precise and conservative. Never invent data or numbers. Do not output an Evidence section. \"\n",
    "     \"Do not include numeric risk score in the Executive Summary; classification name may be used. \"\n",
    "     \"If mentioning a PO, include its item number (e.g., 'PO 4000003611 / Item 00010'). \"\n",
    "     \"Avoid legal/accusatory terms (risk language only).\" },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                \"stream\": False,\n",
    "                \"options\": self.options\n",
    "            }\n",
    "    \n",
    "        @retry(\n",
    "            reraise=True,\n",
    "            stop=stop_after_attempt(3),\n",
    "            wait=wait_exponential_jitter(1, 6),\n",
    "            retry=retry_if_exception_type((requests.RequestException,))\n",
    "        )\n",
    "        def chat(self, prompt: str, timeout: int = 600) -> str:\n",
    "            r = self.sess.post(self.base_url, data=json.dumps(self._payload(prompt)), timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            return (data.get(\"message\", {}) or {}).get(\"content\", \"\") or \"\"\n",
    "    \n",
    "    # ---------------- SANITIZERS & LOCAL FALLBACKS --------------------------------\n",
    "    def sanitize_banned_terms(text: str) -> str:\n",
    "        out = text or \"\"\n",
    "        for pat in BANNED_TERMS:\n",
    "            out = re.sub(pat, \"risk\", out, flags=re.IGNORECASE)\n",
    "        return out\n",
    "    \n",
    "    def local_narrative_fallback(source_text: str) -> str:\n",
    "        risk_line = extract_risk_line(source_text)\n",
    "        primary   = extract_primary_line(source_text)\n",
    "        impact    = extract_business_impact_line(source_text)\n",
    "        drivers   = extract_risk_drivers(source_text)\n",
    "    \n",
    "        exec_bits = []\n",
    "        if risk_line: exec_bits.append(risk_line)\n",
    "        if primary:   exec_bits.append(primary)\n",
    "        if impact:    exec_bits.append(impact)\n",
    "        exec_summary = \" \".join(exec_bits) or \"This transaction was evaluated; see Evidence for details.\"\n",
    "    \n",
    "        why_bullets = [f\"· {d}\" for d in (drivers[:4] or [\"No specific risk drivers listed in the source.\"])]\n",
    "    \n",
    "        next_steps = [\n",
    "            \"· Validate price justification and contract terms for this PO line.\",\n",
    "            \"· Cross-check material/vendor pricing against recent comparable lines.\",\n",
    "            \"· Confirm requester acknowledgment of any variance and approval trail.\"\n",
    "        ]\n",
    "    \n",
    "        return (\n",
    "            \"**SARA AI — Explanation**\\n\\n\"\n",
    "            \"**Executive Summary**\\n\" + exec_summary + \"\\n\\n\"\n",
    "            \"**Why It Was Flagged**\\n\" + \"\\n\".join(why_bullets) + \"\\n\\n\"\n",
    "            \"**Recommended Next Steps**\\n\" + \"\\n\".join(next_steps)\n",
    "        )\n",
    "    \n",
    "    # ---------------- SECTION MATCHERS & HEADER LINE INSERTION ---------------------\n",
    "    SECTION_PATTERNS = [\n",
    "        re.compile(r'^\\s*\\*{0,2}\\s*SARA\\s*AI\\s*[—]?\\s*Explanation\\s*\\*{0,2}\\s*$', re.I),\n",
    "        re.compile(r'^\\s*\\*{0,2}\\s*Executive\\s+Summary\\s*\\*{0,2}\\s*$', re.I),\n",
    "        re.compile(r'^\\s*\\*{0,2}\\s*Why\\s+It\\s+Was\\s+Flagged\\s*\\*{0,2}\\s*$', re.I),\n",
    "        re.compile(r'^\\s*\\*{0,2}\\s*Price\\s+Variance(?:\\s+Value)?\\s*\\*{0,2}\\s*$', re.I),\n",
    "        re.compile(r'^\\s*\\*{0,2}\\s*Recommended\\s+Next\\s+Steps\\s*\\*{0,2}\\s*$', re.I),\n",
    "    ]\n",
    "    \n",
    "    def add_header_line_before_sections(text: str, header_line: str = HEADER_LINE, times: int = 1) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\" if text is None else str(text)\n",
    "        lines = text.splitlines(True)\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line_wo_nl = lines[i].rstrip(\"\\n\")\n",
    "            if any(p.match(line_wo_nl) for p in SECTION_PATTERNS):\n",
    "                j = i - 1\n",
    "                while j >= 0 and lines[j].strip() == \"\":\n",
    "                    j -= 1\n",
    "                already_has = (j >= 0 and lines[j].strip() == header_line)\n",
    "                if not already_has:\n",
    "                    lines.insert(i, (header_line + \"\\n\") * times)\n",
    "                    i += 1\n",
    "            i += 1\n",
    "        return \"\".join(lines)\n",
    "    \n",
    "    # ---------------- VALIDATION & REPAIR GUARDRAILS -------------------------------\n",
    "    EXEC_BULLETS = [\n",
    "        r'•\\s*Purchase\\s+Order:\\s*',\n",
    "        r'•\\s*Vendor:\\s*',\n",
    "        r'•\\s*Material:\\s*',\n",
    "        r'•\\s*Risk\\s+Flag:\\s*',\n",
    "        r'•\\s*Line\\s+Value:\\s*',\n",
    "        r'•\\s*Action:\\s*'\n",
    "    ]\n",
    "    \n",
    "    def _has_section(txt: str, title: str) -> bool:\n",
    "        return re.search(rf'(?mi)^\\s*\\*\\*{re.escape(title)}\\*\\*\\s*$', txt or \"\") is not None\n",
    "    \n",
    "    def _section_block(txt: str, title: str,\n",
    "                       next_titles=('Why It Was Flagged','Price Variance','Price Variance Value','Recommended Next Steps')) -> str:\n",
    "        if not txt: return \"\"\n",
    "        pat = re.compile(\n",
    "            rf'(?is)^\\s*\\*\\*{re.escape(title)}\\*\\*\\s*\\n(.*?)(?=\\n\\s*\\*\\*(?:{\"|\".join(map(re.escape,next_titles))})\\*\\*|\\Z)',\n",
    "            re.M\n",
    "        )\n",
    "        m = pat.search(txt)\n",
    "        return m.group(1).strip() if m else \"\"\n",
    "    \n",
    "    def _count_bullets(block: str, bullet='•') -> int:\n",
    "        return len(re.findall(rf'(?m)^\\s*{re.escape(bullet)}\\s+', block or \"\"))\n",
    "    \n",
    "    def _count_numbered(block: str) -> int:\n",
    "        return len(re.findall(r'(?m)^\\s*\\d+\\.\\s+', block or \"\"))\n",
    "    \n",
    "    def is_valid_narrative(txt: str) -> bool:\n",
    "        if not txt: return False\n",
    "        if not (_has_section(txt, \"SARA AI — Explanation\") and\n",
    "                _has_section(txt, \"Executive Summary\") and\n",
    "                _has_section(txt, \"Why It Was Flagged\") and\n",
    "                _has_section(txt, \"Recommended Next Steps\")):\n",
    "            return False\n",
    "    \n",
    "        exec_blk = _section_block(txt, \"Executive Summary\")\n",
    "        if not exec_blk: return False\n",
    "        for rb in EXEC_BULLETS:\n",
    "            if re.search(rb, exec_blk) is None:\n",
    "                return False\n",
    "    \n",
    "        why_blk = _section_block(txt, \"Why It Was Flagged\")\n",
    "        if _count_bullets(why_blk) < 1:\n",
    "            return False\n",
    "    \n",
    "        steps_blk = _section_block(txt, \"Recommended Next Steps\")\n",
    "        if _count_numbered(steps_blk) < 3:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def strip_preamble_noise(txt: str) -> str:\n",
    "        if not txt: return \"\"\n",
    "        m = re.search(r'(?ims)^\\s*\\*\\*SARA\\s*AI\\s*[—\\-–]\\s*Explanation\\*\\*.*', txt)\n",
    "        return m.group(0).strip() if m else txt.strip()\n",
    "    \n",
    "    # ---------------- Deterministic builder (from SOURCE) --------------------------\n",
    "    # Parse Markdown rows like: | label | value |\n",
    "    MD_ROW_PAT = re.compile(r'^\\s*\\|\\s*(?P<label>[^|]+?)\\s*\\|\\s*(?P<val>[^|]*?)\\s*\\|\\s*$', re.M)\n",
    "    \n",
    "    def _md_val(block: str, label_variants: list[str]) -> str | None:\n",
    "        if not block: return None\n",
    "        want = {lv.strip().lower() for lv in label_variants}\n",
    "        for m in MD_ROW_PAT.finditer(block):\n",
    "            lab = (m.group(\"label\") or \"\").strip()\n",
    "            if lab.lower() in (\"field\", \"---\"):   # skip header/separator\n",
    "                continue\n",
    "            if lab.strip().lower() in want:\n",
    "                return (m.group(\"val\") or \"\").strip()\n",
    "        return None\n",
    "    \n",
    "    def _kv_val_anywhere(src: str, label_variants: list[str]) -> str | None:\n",
    "        if not src: return None\n",
    "        for lab in label_variants:\n",
    "            m = re.search(rf'(?mi)^\\s*{re.escape(lab)}\\s*[:|]\\s*(.+?)\\s*$', src)\n",
    "            if m:\n",
    "                return m.group(1).strip()\n",
    "        return None\n",
    "    \n",
    "    def _parse_qty_price(s: str):\n",
    "        # \"5 EA @ INR 144.07\"\n",
    "        if not s: return (None, None, None, None)\n",
    "        m = re.search(r\"([\\d,]+(?:\\.\\d+)?)\\s*([A-Za-z%]+)?\\s*@\\s*([A-Za-z]{3})\\s*([\\d,]+(?:\\.\\d+)?)\", s)\n",
    "        if not m: return (None, None, None, None)\n",
    "        qty  = float(m.group(1).replace(\",\", \"\")) if m.group(1) else None\n",
    "        unit = m.group(2) or None\n",
    "        cur  = m.group(3) or None\n",
    "        ppu  = float(m.group(4).replace(\",\", \"\")) if m.group(4) else None\n",
    "        return qty, unit, ppu, cur\n",
    "    \n",
    "    def _num(s: str) -> float | None:\n",
    "        if not s: return None\n",
    "        m = re.search(r\"([\\d,]+(?:\\.\\d+)?)\", s)\n",
    "        return float(m.group(1).replace(\",\", \"\")) if m else None\n",
    "    \n",
    "    def _fmt_money(cur: str | None, amt: float | None) -> str | None:\n",
    "        if cur and amt is not None: return f\"{cur} {amt:,.2f}\"\n",
    "        if amt is not None: return f\"{amt:,.2f}\"\n",
    "        return None\n",
    "    \n",
    "    def _build_exec_from_source(src: str) -> str:\n",
    "        kd = _find_block(src, r\"Key\\s*Description\")  # accepts \"(PO Details)\" automatically\n",
    "        po_item_pr = _md_val(kd, [\"PO / Item & PR Ref\"]) or _kv_val_anywhere(src, [\"PO / Item & PR Ref\"])\n",
    "        vendor     = _md_val(kd, [\"Vendor no - Name\"])   or _kv_val_anywhere(src, [\"Vendor no - Name\"])\n",
    "        material   = _md_val(kd, [\"Material – Text, Type\", \"Material - Text, Type\", \"Material – Text\"]) \\\n",
    "                     or _kv_val_anywhere(src, [\"Material – Text, Type\", \"Material - Text, Type\", \"Material – Text\"])\n",
    "        qtyprice   = _md_val(kd, [\"Quantity / Unit & Unit Price\"]) or _kv_val_anywhere(src, [\"Quantity / Unit & Unit Price\"])\n",
    "        netgross   = _md_val(kd, [\"Net / Gross Value\", \"Flagged Value\", \"Net Value\", \"Gross Value\"]) \\\n",
    "                     or _kv_val_anywhere(src, [\"Net / Gross Value\", \"Flagged Value\", \"Net Value\", \"Gross Value\"])\n",
    "    \n",
    "        ctx = _find_block(src, r\"Context\\s*&\\s*Trigger\") or src\n",
    "        risk_flag = None\n",
    "        for ln in (ctx or \"\").splitlines():\n",
    "            if \"Risk Scenario\" in ln:\n",
    "                risk_flag = ln.split(\":\", 1)[-1].strip()\n",
    "                break\n",
    "    \n",
    "        action = None\n",
    "        for ln in (ctx or \"\").splitlines():\n",
    "            if \"Risk Score\" in ln and \"→\" in ln:\n",
    "                action = ln.split(\"→\", 1)[-1].strip()\n",
    "                break\n",
    "    \n",
    "        # Purchase Order / Item\n",
    "        po_item = \"(not available)\"\n",
    "        if po_item_pr:\n",
    "            m = re.search(r\"(\\d{8,})\\s*/\\s*(\\d{5})\", po_item_pr)\n",
    "            if m: po_item = f\"PO {m.group(1)} / Item {m.group(2)}\"\n",
    "    \n",
    "        # Material \"code — text(before comma)\"\n",
    "        material_fmt = \"(not available)\"\n",
    "        if material:\n",
    "            m = re.search(r\"(\\d+)\\s*[—\\-]\\s*([^,|]+)\", material)\n",
    "            material_fmt = f\"{m.group(1)} — {m.group(2).strip()}\" if m else material\n",
    "    \n",
    "        # Line Value: prefer provided; else qty*ppu\n",
    "        qty, unit, ppu, cur = _parse_qty_price(qtyprice or \"\")\n",
    "        line_val = None\n",
    "        if netgross:\n",
    "            m = re.search(r\"([A-Za-z]{3})\\s*([\\d,]+(?:\\.\\d+)?)\", netgross)\n",
    "            if m:\n",
    "                line_val = f\"{m.group(1)} {float(m.group(2).replace(',','')):,.2f}\"\n",
    "            else:\n",
    "                ng = _num(netgross)\n",
    "                line_val = _fmt_money(cur, ng) if ng is not None else netgross\n",
    "        if not line_val and (qty is not None and ppu is not None):\n",
    "            line_val = _fmt_money(cur, qty * ppu)\n",
    "    \n",
    "        return \"\\n\".join([\n",
    "            f\"• Purchase Order: {po_item or '(not available)'}\",\n",
    "            f\"• Vendor: {vendor or '(not available)'}\",\n",
    "            f\"• Material: {material_fmt or '(not available)'}\",\n",
    "            f\"• Risk Flag: {risk_flag or '(not available)'}\",\n",
    "            f\"• Line Value: {line_val or '(not available)'}\",\n",
    "            f\"• Action: {action + ' go through evidence for further validation' or '(not available)'}\",\n",
    "        ])\n",
    "    \n",
    "    def _build_why_from_source(src: str) -> str:\n",
    "        ds = extract_risk_drivers(src) or []\n",
    "        if not ds: return \"• No specific risk drivers listed in the source.\"\n",
    "        return \"\\n\".join([f\"• {d}\" for d in ds[:4]])\n",
    "    \n",
    "    def _build_steps_from_source(src: str) -> str:\n",
    "        return \"\\n\".join([\n",
    "            \"1. Validate price justification and contract terms for this PO line.\",\n",
    "            \"2. Cross-check material/vendor pricing against recent comparable lines.\",\n",
    "            \"3. Confirm requester approval trail for any acceptable variance.\"\n",
    "        ])\n",
    "    \n",
    "    def build_doc_from_source(src: str) -> str:\n",
    "        return (\n",
    "            \"**SARA AI — Explanation**\\n\\n\"\n",
    "            \"**Executive Summary**\\n\" + _build_exec_from_source(src) + \"\\n\\n\"\n",
    "            \"**Why It Was Flagged**\\n\" + _build_why_from_source(src) + \"\\n\\n\"\n",
    "            \"**Recommended Next Steps**\\n\" + _build_steps_from_source(src)\n",
    "        )\n",
    "    \n",
    "    def repair_to_format_with_llm(source_text: str, bad_text: str) -> str:\n",
    "        client_fix = OllamaClient(OLLAMA_URL, MODEL, {**OPTIONS, \"temperature\": 0.0})\n",
    "        repair_prompt = f\"\"\"\n",
    "    Reformat STRICTLY into the EXACT template below using ONLY values present in SOURCE.\n",
    "    If a value is missing, write \"(not available)\". No extra text outside the template. No \"Price Variance Value\" section.\n",
    "    \n",
    "    TEMPLATE:\n",
    "    **SARA AI — Explanation**\n",
    "    \n",
    "    **Executive Summary**\n",
    "    • Purchase Order: <value>\n",
    "    • Vendor: <value>\n",
    "    • Material: <value>\n",
    "    • Risk Flag: <value>\n",
    "    • Line Value: <value>\n",
    "    • Action: <value>\n",
    "    \n",
    "    **Why It Was Flagged**\n",
    "    • <value>\n",
    "    • <value>\n",
    "    • <value>\n",
    "    • <value>\n",
    "    \n",
    "    **Recommended Next Steps**\n",
    "    1. <value>\n",
    "    2. <value>\n",
    "    3. <value>\n",
    "    \n",
    "    SOURCE\n",
    "    — BEGIN —\n",
    "    {source_text}\n",
    "    — END —\n",
    "    \n",
    "    CURRENT OUTPUT\n",
    "    — BEGIN —\n",
    "    {bad_text}\n",
    "    — END —\n",
    "    \"\"\".strip()\n",
    "        try:\n",
    "            fixed = client_fix.chat(repair_prompt)\n",
    "            return strip_preamble_noise(fixed)\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    \n",
    "    # ---------------- ENSURE STRUCTURE (no placeholders) ---------------------------\n",
    "    def ensure_sections_present(text: str, source_fallback: str) -> str:\n",
    "        t = (text or \"\").strip()\n",
    "        if not t:\n",
    "            return build_doc_from_source(source_fallback)\n",
    "        has_all = (_has_section(t, \"SARA AI — Explanation\") and\n",
    "                   _has_section(t, \"Executive Summary\") and\n",
    "                   _has_section(t, \"Why It Was Flagged\") and\n",
    "                   _has_section(t, \"Recommended Next Steps\"))\n",
    "        return t if has_all else build_doc_from_source(source_fallback)\n",
    "    \n",
    "    # ---------------- DRIVER -------------------------------------------------------\n",
    "    def refine_with_llm(df_in: pd.DataFrame,\n",
    "                        input_col: str = INPUT_COL,\n",
    "                        id_col: str = ID_COL,\n",
    "                        variance_col: str = VARIANCE_COL,\n",
    "                        output_col: str = OUTPUT_COL,\n",
    "                        checkpoint_every: int = CHECKPOINT_EVERY,\n",
    "                        checkpoint_path: str = CHECKPOINT_PATH,\n",
    "                        final_evidence: str = FINAL_INPUT_EVIDENCE) -> pd.DataFrame:\n",
    "        df2 = df_in.copy()\n",
    "        outs = []\n",
    "    \n",
    "        client = OllamaClient(OLLAMA_URL, MODEL, OPTIONS)\n",
    "    \n",
    "        for i, row in tqdm(df2.iterrows(), total=len(df2), desc=\"Refining with LLM\"):\n",
    "            main_source = str(row.get(final_evidence, \"\") or \"\")\n",
    "            source      = str(row.get(input_col, \"\") or \"\").strip()\n",
    "            if not source and main_source:\n",
    "                source = main_source\n",
    "    \n",
    "            variance_text = str(row.get(variance_col, \"\") or \"\").strip()\n",
    "            variance_block = build_price_variance_bullets(variance_text)\n",
    "    \n",
    "            if not source:\n",
    "                outs.append(\"\")   # keep empty string, never NaN\n",
    "            else:\n",
    "                prompt = build_refine_prompt(source)\n",
    "                try:\n",
    "                    narrative = client.chat(prompt)\n",
    "                except Exception:\n",
    "                    narrative = \"\"\n",
    "    \n",
    "                narrative = strip_preamble_noise(narrative)\n",
    "                if not narrative.strip():\n",
    "                    narrative = local_narrative_fallback(source)\n",
    "    \n",
    "                if not is_valid_narrative(narrative):\n",
    "                    repaired = repair_to_format_with_llm(source, narrative)\n",
    "                    if repaired and is_valid_narrative(repaired):\n",
    "                        narrative = repaired\n",
    "                    else:\n",
    "                        narrative = build_doc_from_source(source)\n",
    "    \n",
    "                final_doc = insert_price_variance_block(narrative, variance_block) if 'NaN' not in variance_block else narrative\n",
    "                final_doc = sanitize_banned_terms(final_doc)\n",
    "                final_doc = ensure_sections_present(final_doc, source)\n",
    "                final_doc = add_header_line_before_sections(final_doc, times=1)\n",
    "    \n",
    "                final_text = final_doc.rstrip() + \"\\n\" + main_source\n",
    "                outs.append(final_text)\n",
    "    \n",
    "            #if checkpoint_every and (i + 1) % checkpoint_every == 0:\n",
    "                #tmp = df2.iloc[: i + 1].copy()\n",
    "                #tmp[output_col] = pd.Series(outs, index=tmp.index, dtype=\"string\")\n",
    "                #tmp.to_parquet(checkpoint_path, index=False)\n",
    "    \n",
    "        df2[output_col] = pd.Series(outs, index=df2.index, dtype=\"string\").fillna(\"\")\n",
    "        return df2\n",
    "    \n",
    "    # ---------------- RE-RUN UNTIL CLEAN (remove 'See Evidence') -------------------\n",
    "    SEE_EVIDENCE_RE = r'(?is)\\bsee\\s*evidence\\b\\.?'\n",
    "    \n",
    "    def _needs_fix_mask(df: pd.DataFrame) -> pd.Series:\n",
    "        col = OUTPUT_COL\n",
    "        if col not in df.columns:\n",
    "            raise KeyError(f\"Expected column '{col}' not found.\")\n",
    "        return df[col].fillna(\"\").str.contains(SEE_EVIDENCE_RE, regex=True)\n",
    "    \n",
    "    def _read_any(path: str) -> pd.DataFrame:\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "        if ext == \".parquet\":\n",
    "            return pd.read_parquet(path)\n",
    "        elif ext in (\".xlsx\", \".xls\"):\n",
    "            return pd.read_excel(path)\n",
    "        elif ext == \".csv\":\n",
    "            return pd.read_csv(path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input extension: {ext}\")\n",
    "    \n",
    "    def _write_any(df: pd.DataFrame, path: str):\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "        if ext == \".parquet\":\n",
    "            df.to_parquet(path, index=False)\n",
    "        elif ext in (\".xlsx\", \".xls\"):\n",
    "            df.to_excel(path, index=False)\n",
    "        elif ext == \".csv\":\n",
    "            df.to_csv(path, index=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported output extension: {ext}\")\n",
    "    \n",
    "    def _update_master_from_fixed(master: pd.DataFrame, fixed: pd.DataFrame) -> None:\n",
    "        if ID_COL not in master.columns or ID_COL not in fixed.columns:\n",
    "            raise KeyError(f\"Both dataframes must contain '{ID_COL}'.\")\n",
    "        if OUTPUT_COL not in fixed.columns:\n",
    "            raise KeyError(f\"Fixed dataframe must contain '{OUTPUT_COL}'.\")\n",
    "        new_map = fixed.set_index(ID_COL)[OUTPUT_COL]\n",
    "        sel = master[ID_COL].isin(new_map.index)\n",
    "        master.loc[sel, OUTPUT_COL] = master.loc[sel, ID_COL].map(new_map)\n",
    "    \n",
    "    def rerun_llm_until_clean(df_refined,max_passes: int = 2):\n",
    "        #df_all = _read_any(in_path)\n",
    "        df_all=df_refined\n",
    "    \n",
    "        for p in range(1, max_passes + 1):\n",
    "            mask = _needs_fix_mask(df_all)\n",
    "            n = int(mask.sum())\n",
    "            print(f\"[Pass {p}/{max_passes}] rows needing re-run: {n}\")\n",
    "            if n == 0:\n",
    "                break\n",
    "    \n",
    "            subset_cols = [ID_COL, INPUT_COL, VARIANCE_COL, FINAL_INPUT_EVIDENCE]\n",
    "            missing = [c for c in subset_cols if c not in df_all.columns]\n",
    "            if missing:\n",
    "                raise KeyError(f\"Missing required column(s) in input: {missing}\")\n",
    "    \n",
    "            subset = df_all.loc[mask].copy()\n",
    "            subset.drop(columns=[OUTPUT_COL], inplace=True)\n",
    "            fixed = refine_with_llm(subset)\n",
    "            _update_master_from_fixed(df_all, fixed)\n",
    "    \n",
    "            post_n = int(_needs_fix_mask(df_all).sum())\n",
    "            print(f\"[Pass {p}] remaining with 'See Evidence': {post_n}\")\n",
    "            if post_n == 0:\n",
    "                break\n",
    "    \n",
    "        #_write_any(df_all, out_path)\n",
    "        print(f\"Saved cleaned file \")\n",
    "        return df_all\n",
    "\n",
    "    fixed = rerun_llm_until_clean(df_refined.copy())\n",
    "    return fixed\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
