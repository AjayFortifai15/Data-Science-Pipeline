{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e2709-e366-4432-8d83-902e710a30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import joblib\n",
    "import logging\n",
    "from typing import List\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import logging\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set options to show full DataFrame output\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import joblib, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from typing import Any, Dict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def db_update(doc_4):\n",
    "    ## staging_db.po_header_lineitem_merged_with_risks truncate\n",
    "    \n",
    "    #conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    #\t\t\tdatabase='baldota-dev-db',\n",
    "    #\t\t\tuser='fortifai_ng_user_ro',\n",
    "    #\t\t\tpassword='user@123!',\n",
    "    #\t\t\tport='5432',\n",
    "    #            sslmode=\"require\"\n",
    "    #\t\t)\n",
    "    \n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_ai_user_rw',\n",
    "    \t\t\tpassword='AIPwd@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"DELETE FROM staging_db.po_header_lineitem_merged_with_risks;\")\n",
    "        conn.commit()\n",
    "    \n",
    "    print(\"✅ Cleared all rows from staging_db.po_header_lineitem_merged_with_risks\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## staging DB data upload\n",
    "    #conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    #\t\t\tdatabase='baldota-dev-db',\n",
    "    #\t\t\tuser='fortifai_ng_user_ro',\n",
    "    #\t\t\tpassword='user@123!',\n",
    "    #\t\t\tport='5432',\n",
    "    #            sslmode=\"require\"\n",
    "    #\t\t)\n",
    "    \n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_ai_user_rw',\n",
    "    \t\t\tpassword='AIPwd@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "                SELECT table_name \n",
    "                FROM information_schema.tables \n",
    "            \"\"\")\n",
    "    tables = [row[0] for row in cur.fetchall()]\n",
    "    \n",
    "    ## table == name of table from transform_db or semantic db\n",
    "    #data= pd.read_sql_query(f\"SELECT * FROM transform_db.{table}\", conn)\n",
    "    #data= pd.read_sql_query(f\"SELECT * FROM semantic_db.{table}\", conn)\n",
    "    \n",
    "    #cur.close()\n",
    "    #conn.close()\n",
    "    #df['risk_summary_object']=\n",
    "    new_df_1=doc_4[doc_4['purch_doc_no_src_po'].notna()].copy()\n",
    "    #df = df.rename(columns=new_names, errors=\"ignore\")\n",
    "    new_df_1 = new_df_1.rename(columns={\n",
    "        \"llm_refined_explanation\": \"risk_summary_object\",\n",
    "    })\n",
    "    import io\n",
    "    import numpy as np\n",
    "    \n",
    "    schema = \"staging_db\"\n",
    "    table  = \"po_header_lineitem_merged_with_risks\"\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT column_name\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_schema=%s AND table_name=%s\n",
    "            ORDER BY ordinal_position\n",
    "        \"\"\", (schema, table))\n",
    "        db_cols = [r[0] for r in cur.fetchall()]\n",
    "    \n",
    "    df_to_insert = new_df_1[db_cols].replace({np.nan: None})  # align cols & fix NaN\n",
    "    \n",
    "    buf = io.StringIO()\n",
    "    df_to_insert.to_csv(buf, index=False, header=False)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    copy_sql = f\"COPY {schema}.{table} ({', '.join(db_cols)}) FROM STDIN WITH CSV\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.copy_expert(copy_sql, buf)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"✅ Inserted {len(df_to_insert)} rows into {schema}.{table}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## transform_db.transaction_details truncate\n",
    "    #conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    #\t\t\tdatabase='baldota-dev-db',\n",
    "    #\t\t\tuser='fortifai_ng_user_ro',\n",
    "    #\t\t\tpassword='user@123!',\n",
    "    #\t\t\tport='5432',\n",
    "    #            sslmode=\"require\"\n",
    "    #\t\t)\n",
    "    \n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_ai_user_rw',\n",
    "    \t\t\tpassword='AIPwd@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"DELETE FROM transform_db.transaction_details;\")\n",
    "        conn.commit()\n",
    "    \n",
    "    print(\"✅ Cleared all rows from transform_db.transaction_details\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## data fetch from merged db after transaction_date_update\n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_ai_user_rw',\n",
    "    \t\t\tpassword='AIPwd@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "                SELECT table_name \n",
    "                FROM information_schema.tables \n",
    "            \"\"\")\n",
    "    \n",
    "    tables = [row[0] for row in cur.fetchall()]\n",
    "    po_data= pd.read_sql_query(f\"SELECT * FROM staging_db.{'po_header_lineitem_merged_with_risks'}\", conn)\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    #p2p_header_po_data.head()\n",
    "    #print(po_data.shape)\n",
    "    #po_data.head()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    #\t\t\tdatabase='baldota-dev-db',\n",
    "    #\t\t\tuser='fortifai_ng_user_ro',\n",
    "    #\t\t\tpassword='user@123!',\n",
    "    #\t\t\tport='5432',\n",
    "    #            sslmode=\"require\"\n",
    "    #\t\t)\n",
    "    \n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_ai_user_rw',\n",
    "    \t\t\tpassword='AIPwd@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "                SELECT table_name \n",
    "                FROM information_schema.tables \n",
    "            \"\"\")\n",
    "    tables = [row[0] for row in cur.fetchall()]\n",
    "    \n",
    "    ## table == name of table from transform_db or semantic db\n",
    "    #data= pd.read_sql_query(f\"SELECT * FROM transform_db.{table}\", conn)\n",
    "    #data= pd.read_sql_query(f\"SELECT * FROM semantic_db.{table}\", conn)\n",
    "    \n",
    "    #p2p_header_po_data= pd.read_sql_query(f\"SELECT * FROM transform_db.{'transaction_details'}\", conn)\n",
    "    #df['stage']='PO'\n",
    "    #df['region']='India'\n",
    "    po_data['stage']='PO'\n",
    "    po_data['region']='India'\n",
    "    new_df=po_data[[ 'purch_doc_no_src_po', 'purch_doc_item_no_src_po','short_text_src_po','vendor_or_creditor_acct_no_hpd_po','net_val_po_curr_src_po',\n",
    "    'currency_hpd_po','stage','region','purch_doc_date_hpd_po','vendor_name_1']]\n",
    "    rename_map = {\n",
    "        'purch_doc_no_src_po': 'doc_number',\n",
    "        'purch_doc_item_no_src_po': 'line_item_number',\n",
    "        'short_text_src_po': 'item_description',\n",
    "        'vendor_or_creditor_acct_no_hpd_po': 'vendor_code',\n",
    "        'net_val_po_curr_src_po': 'amount',\n",
    "        'currency_hpd_po': 'currency',\n",
    "        'stage': 'document_stage',\n",
    "        'region': 'region',\n",
    "        'purch_doc_date_hpd_po': 'transaction_date','vendor_name_1':'vendor_name',\n",
    "    }\n",
    "    \n",
    "    new_df_1 = new_df.rename(columns=rename_map)\n",
    "    new_df_1=new_df_1[new_df_1['doc_number'].notna()].copy()\n",
    "    new_df_1['module_name']=None\n",
    "    new_df_1['document_type']=None\t\n",
    "    \n",
    "    \n",
    "    \n",
    "    schema = \"transform_db\"\n",
    "    table  = \"transaction_details\"\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT column_name\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_schema=%s AND table_name=%s\n",
    "            ORDER BY ordinal_position\n",
    "        \"\"\", (schema, table))\n",
    "        db_cols = [r[0] for r in cur.fetchall()]\n",
    "    \n",
    "    df_to_insert = new_df_1[db_cols].replace({np.nan: None})  # align cols & fix NaN\n",
    "    \n",
    "    buf = io.StringIO()\n",
    "    df_to_insert.to_csv(buf, index=False, header=False)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    copy_sql = f\"COPY {schema}.{table} ({', '.join(db_cols)}) FROM STDIN WITH CSV\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.copy_expert(copy_sql, buf)\n",
    "    \n",
    "    p2p_header_po_data= pd.read_sql_query(f\"SELECT * FROM transform_db.{'transaction_details'}\", conn)\n",
    "    p2p_header_po_data.head()\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"✅ Inserted {len(df_to_insert)} rows into {schema}.{table}\")\n",
    "    \n",
    "    #p2p_header_po_data= pd.read_sql_query(f\"SELECT * FROM transform_db.{'transaction_details'}\", conn)\n",
    "    #p2p_header_po_data\n",
    "    \n",
    "    \n",
    "    ##transform_db.transaction_risk_analysis truncate\n",
    "    #conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    #\t\t\tdatabase='baldota-dev-db',\n",
    "    #\t\t\tuser='fortifai_ng_user_ro',\n",
    "    #\t\t\tpassword='user@123!',\n",
    "    #\t\t\tport='5432',\n",
    "    #            sslmode=\"require\"\n",
    "    #\t\t)\n",
    "    \n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_ai_user_rw',\n",
    "    \t\t\tpassword='AIPwd@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"DELETE FROM transform_db.transaction_risk_analysis;\")\n",
    "        conn.commit()\n",
    "    \n",
    "    print(\"✅ Cleared all rows from transform_db.transaction_risk_analysis\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    #\t\t\tdatabase='baldota-dev-db',\n",
    "    #\t\t\tuser='fortifai_ng_user_ro',\n",
    "    #\t\t\tpassword='user@123!',\n",
    "    #\t\t\tport='5432',\n",
    "    #            sslmode=\"require\"\n",
    "    #\t\t)\n",
    "    \n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_ai_user_rw',\n",
    "    \t\t\tpassword='AIPwd@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "                SELECT table_name \n",
    "                FROM information_schema.tables \n",
    "            \"\"\")\n",
    "    tables = [row[0] for row in cur.fetchall()]\n",
    "    \n",
    "    ## table == name of table from transform_db or semantic db\n",
    "    #data= pd.read_sql_query(f\"SELECT * FROM transform_db.{table}\", conn)\n",
    "    #data= pd.read_sql_query(f\"SELECT * FROM semantic_db.{table}\", conn)\n",
    "    new_df=po_data[['purch_doc_date_hpd_po', 'purch_doc_no_src_po', 'purch_doc_item_no_src_po','sub_risk_1','main_risk_scenario',\n",
    "               'risk_score','impact_1','risk_level','risk_summary_object']]\n",
    "    \n",
    "    rename_map = {\n",
    "        'purch_doc_date_hpd_po':'transaction_date', \n",
    "        'purch_doc_no_src_po':'doc_number',\n",
    "           'purch_doc_item_no_src_po':'line_item_number',\n",
    "        'sub_risk_1':'risk_definition',\n",
    "        'main_risk_scenario':'risk_category',\n",
    "           'risk_score':'risk_score', \n",
    "        'impact_1':'risk_impact_type', \n",
    "        'risk_level':'risk_severity_level',\n",
    "        'risk_summary_object':'risk_description'\n",
    "    }\n",
    "    \n",
    "    new_df_1 = new_df.rename(columns=rename_map)\n",
    "    new_df_1=new_df_1[new_df_1['doc_number'].notna()].copy()\n",
    "    new_df_1['identified_by']='AI Engine'\n",
    "    #new_df_1['risk_description']='Coming Soon'\n",
    "    new_df_1['risk_date']=date.today().strftime(\"%Y-%m-%d\")\n",
    "    new_df_1['mitigation_status']=None\n",
    "    new_df_1['comments']=None\n",
    "    new_df_1['risk_module']='Procure-to-Pay (P2P)'\n",
    "    \n",
    "    \n",
    "    schema = \"transform_db\"\n",
    "    table  = \"transaction_risk_analysis\"\n",
    "    \n",
    "    # 1️⃣ Get DB column names\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT column_name\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_schema=%s AND table_name=%s\n",
    "            ORDER BY ordinal_position\n",
    "        \"\"\", (schema, table))\n",
    "        db_cols = [r[0] for r in cur.fetchall()]\n",
    "    \n",
    "    # 2️⃣ Remove 'risk_id' from insertion columns\n",
    "    insert_cols = [c for c in db_cols if c.lower() != \"risk_id\"]\n",
    "    \n",
    "    # 3️⃣ Align DataFrame columns and replace NaNs\n",
    "    df_to_insert = new_df_1[insert_cols].replace({np.nan: None})\n",
    "    \n",
    "    # 4️⃣ Create CSV buffer\n",
    "    buf = io.StringIO()\n",
    "    df_to_insert.to_csv(buf, index=False, header=False)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # 5️⃣ COPY into Postgres (excluding risk_id)\n",
    "    copy_sql = f\"COPY {schema}.{table} ({', '.join(insert_cols)}) FROM STDIN WITH CSV\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.copy_expert(copy_sql, buf)\n",
    "    p2p_header_po_data= pd.read_sql_query(f\"SELECT * FROM transform_db.{'transaction_risk_analysis'}\", conn)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"✅ Inserted {len(df_to_insert)} rows into {schema}.{table} (risk_id auto-generated)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
