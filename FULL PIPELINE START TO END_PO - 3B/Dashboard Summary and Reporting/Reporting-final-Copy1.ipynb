{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398c8704-ebb8-44ba-8c6c-7567a95dd644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15007, 94)\n",
      "Filtered 0 records from last 6 months out of 15007 total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayn\\AppData\\Local\\Temp\\ipykernel_28148\\1545674568.py:84: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  fp_data= pd.read_sql_query(f\"SELECT * FROM admin_db.{'false_positive_criteria'}\", conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 8)\n",
      "Filtered: 0 rows kept (out of 0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No usable date column found. Set DATE_COL or add to DATE_CANDIDATES.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 880\u001b[39m\n\u001b[32m    871\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mScheduler stopped.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    873\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m    874\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m    875\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m \u001b[38;5;66;03m# run_reports_job(cleaned, mode=\"weekly\")\u001b[39;00m\n\u001b[32m    879\u001b[39m \u001b[38;5;66;03m# run_reports_job(cleaned, mode=\"monthly\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m880\u001b[39m \u001b[43mrun_reports_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackfill6m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 789\u001b[39m, in \u001b[36mrun_reports_job\u001b[39m\u001b[34m(cleaned_df, mode, upload_to_blob)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_reports_job\u001b[39m(cleaned_df: pd.DataFrame, mode: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mdaily\u001b[39m\u001b[33m\"\u001b[39m, *, upload_to_blob: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    781\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    782\u001b[39m \u001b[33;03m    Generate reports for the given cleaned DataFrame.\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[33;03m    mode: 'daily' | 'weekly' | 'monthly' | 'backfill6m'\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    787\u001b[39m \u001b[33;03m        - None  -> auto (upload if creds present, else local only)  <-- DEFAULT\u001b[39;00m\n\u001b[32m    788\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     df_full = \u001b[43m_prep_df_for_reports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df_full.empty:\n\u001b[32m    791\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo valid dates after parsing — nothing to report.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 774\u001b[39m, in \u001b[36m_prep_df_for_reports\u001b[39m\u001b[34m(cleaned_df)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prep_df_for_reports\u001b[39m(cleaned_df: pd.DataFrame) -> pd.DataFrame:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     dcol = \u001b[43mchoose_date_col\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m     df = cleaned_df.copy()\n\u001b[32m    776\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33m__report_date\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(df[dcol], errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 261\u001b[39m, in \u001b[36mchoose_date_col\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m    259\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    260\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo usable date column found. Set DATE_COL or add to DATE_CANDIDATES.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: No usable date column found. Set DATE_COL or add to DATE_CANDIDATES."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import joblib\n",
    "import logging\n",
    "from typing import List\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import logging\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error\n",
    ")\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import duckdb\n",
    "\n",
    "import re\n",
    "from datetime import date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "# Set options to show full DataFrame output\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "#\t\t\tdatabase='baldota-dev-db',\n",
    "#\t\t\tuser='fortifai_ng_ai_user_rw',\n",
    "#\t\t\tpassword='AIPwd@123!',\n",
    "#\t\t\tport='5432',\n",
    "#            sslmode=\"require\"\n",
    "#\t\t)\n",
    "#cur = conn.cursor()\n",
    "#cur.execute(\"\"\"\n",
    "#            SELECT table_name \n",
    "#            FROM information_schema.tables \n",
    "#        \"\"\")\n",
    "\n",
    "#tables = [row[0] for row in cur.fetchall()]\n",
    "#po_data= pd.read_sql_query(f\"SELECT * FROM staging_db.{'po_header_lineitem_merged_with_risks'}\", conn)\n",
    "#cur.close()\n",
    "#conn.close()\n",
    "po_data=pd.read_excel(r\"C:\\Users\\ajayn\\Downloads\\SARA-23-09\\New 7 Rules\\pipeline\\data_for_db_update_without_risk_driver_17_10.xlsx\")\n",
    "#p2p_header_po_data.head()\n",
    "print(po_data.shape)\n",
    "#po_data.head()\n",
    "po_data['net_val_po_curr_src_po_with_exchange_rate']=po_data['net_val_po_curr_src_po']*po_data['exchange_rate_hpd_po']\n",
    "\n",
    "# Example: assuming your date column is named 'po_date'\n",
    "#po_data['po_date'] = pd.to_datetime(po_data['po_date'], errors='coerce')\n",
    "\n",
    "# Calculate the cutoff date (today - 6 months)\n",
    "cutoff_date = pd.Timestamp.today() - pd.DateOffset(months=6)\n",
    "\n",
    "# Filter rows where po_date is within last 6 months\n",
    "po_last_6m = po_data[po_data['purch_doc_date_hpd_po'] >= cutoff_date].copy()\n",
    "\n",
    "print(f\"Filtered {len(po_last_6m)} records from last 6 months out of {len(po_data)} total.\")\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "\t\t\tdatabase='baldota-dev-db',\n",
    "\t\t\tuser='fortifai_ng_ai_user_rw',\n",
    "\t\t\tpassword='AIPwd@123!',\n",
    "\t\t\tport='5432',\n",
    "            sslmode=\"require\"\n",
    "\t\t)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "            SELECT table_name \n",
    "            FROM information_schema.tables \n",
    "        \"\"\")\n",
    "\n",
    "tables = [row[0] for row in cur.fetchall()]\n",
    "fp_data= pd.read_sql_query(f\"SELECT * FROM admin_db.{'false_positive_criteria'}\", conn)\n",
    "cur.close()\n",
    "conn.close()\n",
    "#p2p_header_po_data.head()\n",
    "print(fp_data.shape)\n",
    "\n",
    "\n",
    "# register your two DataFrames as in-memory tables\n",
    "duckdb.register('po', po_last_6m)\n",
    "duckdb.register('fp', fp_data)\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM po AS t\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT 1\n",
    "    FROM fp\n",
    "    WHERE (\n",
    "        -- Supplier-based exclusion\n",
    "        UPPER(CAST(fp.entity_type AS VARCHAR)) = 'SUPPLIER'\n",
    "        AND UPPER(CAST(fp.entity_id AS VARCHAR)) = UPPER(CAST(t.vendor_or_creditor_acct_no_hpd_po AS VARCHAR))\n",
    "        AND CASE\n",
    "            WHEN TRIM(fp.operator) = '<=' THEN t.net_val_po_curr_src_po_with_exchange_rate <= CAST(fp.threshold_value AS DOUBLE)\n",
    "            WHEN TRIM(fp.operator) = '<'  THEN t.net_val_po_curr_src_po_with_exchange_rate  < CAST(fp.threshold_value AS DOUBLE)\n",
    "            WHEN TRIM(fp.operator) = '>=' THEN t.net_val_po_curr_src_po_with_exchange_rate >= CAST(fp.threshold_value AS DOUBLE)\n",
    "            WHEN TRIM(fp.operator) = '>'  THEN t.net_val_po_curr_src_po_with_exchange_rate  > CAST(fp.threshold_value AS DOUBLE)\n",
    "            WHEN TRIM(fp.operator) = '='  THEN t.net_val_po_curr_src_po_with_exchange_rate  = CAST(fp.threshold_value AS DOUBLE)\n",
    "            ELSE FALSE\n",
    "        END\n",
    "    )\n",
    "    OR (\n",
    "        -- Material-based exclusion\n",
    "        UPPER(CAST(fp.entity_type AS VARCHAR)) = 'MATERIAL'\n",
    "        AND UPPER(CAST(fp.entity_id AS VARCHAR)) = UPPER(CAST(t.material_no_src_po AS VARCHAR))\n",
    "        AND CASE\n",
    "            WHEN TRIM(fp.operator) = '<=' THEN t.net_val_po_curr_src_po_with_exchange_rate <= CAST(fp.threshold_value AS DOUBLE)\n",
    "            WHEN TRIM(fp.operator) = '<'  THEN t.net_val_po_curr_src_po_with_exchange_rate  < CAST(fp.threshold_value AS DOUBLE)\n",
    "            WHEN TRIM(fp.operator) = '>=' THEN t.net_val_po_curr_src_po_with_exchange_rate >= CAST(fp.threshold_value AS DOUBLE)\n",
    "            WHEN TRIM(fp.operator) = '>'  THEN t.net_val_po_curr_src_po_with_exchange_rate  > CAST(fp.threshold_value AS DOUBLE)\n",
    "            WHEN TRIM(fp.operator) = '='  THEN t.net_val_po_curr_src_po_with_exchange_rate  = CAST(fp.threshold_value AS DOUBLE)\n",
    "            ELSE FALSE\n",
    "        END\n",
    "    )\n",
    "    OR (\n",
    "        -- Global transaction exclusion\n",
    "        UPPER(CAST(fp.entity_type AS VARCHAR)) = 'TRANSACTION'\n",
    "        AND CASE\n",
    "            WHEN TRIM(fp.operator) = '<=' THEN t.net_val_po_curr_src_po_with_exchange_rate <= CAST(fp.threshold_value AS DOUBLE)\n",
    "            WHEN TRIM(fp.operator) = '<'  THEN t.net_val_po_curr_src_po_with_exchange_rate  < CAST(fp.threshold_value AS DOUBLE)\n",
    "            WHEN TRIM(fp.operator) = '>=' THEN t.net_val_po_curr_src_po_with_exchange_rate >= CAST(fp.threshold_value AS DOUBLE)\n",
    "            WHEN TRIM(fp.operator) = '>'  THEN t.net_val_po_curr_src_po_with_exchange_rate  > CAST(fp.threshold_value AS DOUBLE)\n",
    "            WHEN TRIM(fp.operator) = '='  THEN t.net_val_po_curr_src_po_with_exchange_rate  = CAST(fp.threshold_value AS DOUBLE)\n",
    "            ELSE FALSE\n",
    "        END\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cleaned = duckdb.sql(query).to_df()\n",
    "print(f\"Filtered: {len(cleaned)} rows kept (out of {len(po_last_6m)})\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FortifAI — Reports + Scheduler (Local-first; Blob optional)\n",
    "# - You pass a ready DataFrame: `cleaned_df`\n",
    "# - Generates Daily/Weekly/Monthly reports\n",
    "# - Saves locally; uploads to Azure Blob only if creds provided\n",
    "# - Public API:\n",
    "#     run_reports_job(cleaned_df, mode=\"daily\"|\"weekly\"|\"monthly\"|\"backfill6m\",\n",
    "#                     upload_to_blob=None)\n",
    "#     start_report_scheduler(get_df_callable)\n",
    "# ============================================================\n",
    "\n",
    "# ------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------\n",
    "# ============================================================\n",
    "# FortifAI — Batch Reports (Daily / Weekly / Monthly) + Backfill\n",
    "# Exact schemas + ALL Sub-Risk (1..3) and Impact Area (1..5)\n",
    "# ============================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "from datetime import date, timedelta\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "BASE_OUT     = \"generate_reports\"\n",
    "TZ_NAME      = \"Asia/Kolkata\"\n",
    "RUN_DATE     = pd.Timestamp.now(tz=TZ_NAME).date()\n",
    "DATE_COL     = None  # set if you know the exact date column\n",
    "\n",
    "# Toggle specific report families\n",
    "RUN_TOP10_VENDORS_AND_TXNS = True\n",
    "RUN_HIGHRISK_TXNS          = True\n",
    "RUN_SPLIT_PO               = True\n",
    "RUN_PRICE_VARIANCE         = True\n",
    "\n",
    "# ------------------------------\n",
    "# Excel engine & formatting utils\n",
    "# ------------------------------\n",
    "def pick_engine():\n",
    "    try:\n",
    "        import xlsxwriter  # noqa: F401\n",
    "        return \"xlsxwriter\"\n",
    "    except Exception:\n",
    "        return \"openpyxl\"\n",
    "\n",
    "ENGINE = pick_engine()\n",
    "\n",
    "def writer_with_formats(path):\n",
    "    w = pd.ExcelWriter(path, engine=ENGINE)\n",
    "    if ENGINE == \"xlsxwriter\":\n",
    "        wb = w.book\n",
    "        fmt = {\n",
    "            \"money\": wb.add_format({\"num_format\": \"#,##0.00\"}),\n",
    "            \"text\":  wb.add_format({\"num_format\": \"@\"}),\n",
    "        }\n",
    "        return w, fmt\n",
    "    return w, None\n",
    "\n",
    "def autofit_and_formats(writer, sheet_name, df, fmt):\n",
    "    money_like = {\n",
    "        \"value\", \"impact value\", \"impact_value\", \"total_value\", \"total_impact\", \"Value\",\n",
    "        \"price per unit (doc curr)\", \"net po value (doc curr)\",\n",
    "        \"gross po value (doc curr)\", \"total value on release\"\n",
    "    }\n",
    "    if ENGINE == \"xlsxwriter\":\n",
    "        ws = writer.sheets[sheet_name]\n",
    "        ws.freeze_panes(1, 0)\n",
    "        ws.autofilter(0, 0, len(df), max(len(df.columns)-1, 0))\n",
    "        for i, c in enumerate(df.columns):\n",
    "            cl = c.lower()\n",
    "            if c in {\"document_id\", \"line_item\", \"cost_center\"}:\n",
    "                ws.set_column(i, i, 20, fmt[\"text\"])\n",
    "            elif cl in money_like:\n",
    "                ws.set_column(i, i, 18, fmt[\"money\"])\n",
    "            elif cl in {\"vendor\", \"vendor name\", \"vendor_name\"}:\n",
    "                ws.set_column(i, i, 34)\n",
    "            else:\n",
    "                ws.set_column(i, i, 18)\n",
    "    else:\n",
    "        from openpyxl.utils import get_column_letter\n",
    "        ws = writer.sheets[sheet_name]\n",
    "        ws.freeze_panes = \"A2\"\n",
    "        ws.auto_filter.ref = ws.dimensions\n",
    "        for i, _ in enumerate(df.columns, start=1):\n",
    "            ws.column_dimensions[get_column_letter(i)].width = 18\n",
    "\n",
    "# ------------------------------\n",
    "# Date column detection\n",
    "# ------------------------------\n",
    "DATE_CANDIDATES = [\n",
    "    'purch_doc_date_hpd_po',  # present in your schema\n",
    "    'po_date',\n",
    "    'document_date',\n",
    "]\n",
    "\n",
    "def choose_date_col(df):\n",
    "    if DATE_COL and DATE_COL in df.columns:\n",
    "        return DATE_COL\n",
    "    for c in DATE_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            try:\n",
    "                as_dt = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "                if as_dt.notna().any():\n",
    "                    return c\n",
    "            except Exception:\n",
    "                pass\n",
    "    raise ValueError(\"No usable date column found. Set DATE_COL or add to DATE_CANDIDATES.\")\n",
    "\n",
    "# ------------------------------\n",
    "# Period helpers\n",
    "# ------------------------------\n",
    "def clamp_to_six_months(df, dcol):\n",
    "    six_months_ago = RUN_DATE - relativedelta(months=6)\n",
    "    data_min_ts = pd.to_datetime(df[dcol], errors=\"coerce\")\n",
    "    data_min = data_min_ts.min().date() if data_min_ts.notna().any() else RUN_DATE\n",
    "    start = max(six_months_ago, data_min)\n",
    "    return start, RUN_DATE\n",
    "\n",
    "def iter_daily(start: date, end: date):\n",
    "    d = start\n",
    "    while d <= end:\n",
    "        yield d, d\n",
    "        d += timedelta(days=1)\n",
    "\n",
    "def start_of_week_monday(d: date):\n",
    "    return d - timedelta(days=d.weekday())  # Monday=0\n",
    "\n",
    "def iter_weekly(start: date, end: date):\n",
    "    w_start = start_of_week_monday(start)\n",
    "    while w_start <= end:\n",
    "        w_end = min(w_start + timedelta(days=6), end)\n",
    "        s = max(w_start, start)\n",
    "        yield s, w_end\n",
    "        w_start += timedelta(days=7)\n",
    "\n",
    "def iter_monthly(start: date, end: date):\n",
    "    m_start = date(start.year, start.month, 1)\n",
    "    while m_start <= end:\n",
    "        next_month = (m_start + relativedelta(months=1))\n",
    "        m_end = min(next_month - timedelta(days=1), end)\n",
    "        s = max(m_start, start)\n",
    "        yield s, m_end\n",
    "        m_start = next_month\n",
    "\n",
    "# ============================================================\n",
    "# P2P detail mapping (tailored to your columns) — now with ALL sub-risks & impacts\n",
    "# ============================================================\n",
    "def _first_present(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# Output label -> source priority list (first present is used)\n",
    "P2P_MAP = {\n",
    "    \"PO Number\":                     [\"purch_doc_no_src_po\", \"base_id_src_po\"],\n",
    "    \"PO Item Number\":                [\"purch_doc_item_no_src_po\"],\n",
    "    \"PO Creation Date\":              [\"purch_doc_date_hpd_po\"],\n",
    "    \"PO Category\":                   [\"purch_doc_category_hpd_po\"],\n",
    "    \"Vendor Account Number\":         [\"vendor_or_creditor_acct_no_hpd_po\", \"vendor_or_creditor_acct_no\"],\n",
    "    \"Vendor Name\":                   [\"vendor_name_1\"],\n",
    "\n",
    "    \"Material Number\":               [\"material_no_src_po\"],\n",
    "    \"Short Text / Description\":      [\"short_text_src_po\"],\n",
    "    \"Material Type\":                 [\"material_type_src_po\"],\n",
    "    \"Material Group\":                [\"matl_group_src_po\"],\n",
    "\n",
    "    \"Order UoM\":                     [\"order_uom_src_po\"],\n",
    "    \"Ordered Quantity\":              [\"quantity_src_po\"],\n",
    "    \"Plant Code\":                    [\"plant_src_po\"],\n",
    "    \"Company Code\":                  [\"company_code_src_po\"],\n",
    "\n",
    "    \"Price per Unit (Doc Curr)\":     [\"net_price_doc_curr_src_po\"],\n",
    "    \"Net PO Value (Doc Curr)\":       [\"net_val_po_curr_src_po\"],\n",
    "\n",
    "    \"PR Number\":                     [\"pr_no_src_po\"],\n",
    "    \"PR Item Number\":                [\"pr_item_no_src_po\"],\n",
    "    \"PO Requester Name\":             [\"requester_name_src_po\"],\n",
    "\n",
    "    \"PO Item Deletion Flag\":         [\"po_item_del_flag_src_po\"],\n",
    "    \"GR Indicator\":                  [\"gr_indicator_src_po\"],\n",
    "    \"Gross PO Value (Doc Curr)\":     [\"gross_val_po_curr_src_po\"],\n",
    "    \"Invoice Receipt Indicator\":     [\"inv_receipt_indicator_src_po\", \"gr_invoice_verif_flag_src_po\"],\n",
    "\n",
    "    \"Company Code (Header)\":         [\"company_code_hpd_po\"],\n",
    "    \"Currency\":                      [\"currency_hpd_po\"],\n",
    "    \"Exchange Rate\":                 [\"exchange_rate_hpd_po\"],\n",
    "\n",
    "    \"Created By\":                    [\"object_created_by_hpd_po\", \"object_created_by\"],\n",
    "    \"Principal Purchase Agreement No.\": [\"principal_purch_agrmt_no_hpd_po\"],\n",
    "    \"Purchase Group\":                [\"purch_group_hpd_po\"],\n",
    "    \"Purchasing Organization\":       [\"purch_org_hpd_po\"],\n",
    "    \"Payment Terms\":                 [\"pymnt_terms_hpd_po\"],\n",
    "    \"PO Processing Status\":          [\"processing_status_hpd_po\"],\n",
    "    \"Release Indicator\":             [\"release_indicator_hpd_po\"],\n",
    "    \"Release Status\":                [\"release_status_hpd_po\"],\n",
    "    \"Responsible Vendor / Salesperson\": [\"resp_vendor_salesperson_hpd_po\"],\n",
    "\n",
    "    \"Total Value on Release\":        [\"on_release_total_value_hpd_po\"],\n",
    "    \"PO Type\":                       [\"purch_doc_type_hpd_po\"],\n",
    "\n",
    "    \"Risk Score\":                    [\"risk_score\", \"gbt_fraud_score\", \"ae_fraud_score\", \"iso_fraud_score\"],\n",
    "    \"Risk Level\":                    [\"risk_level\"],\n",
    "    \"Risk Scenario\":                 [\"main_risk_scenario\"],\n",
    "\n",
    "    # ALL Sub-Risks\n",
    "    \"Sub-Risk 1\":                    [\"sub_risk_1\"],\n",
    "    \"Sub-Risk 2\":                    [\"sub_risk_2\"],\n",
    "    \"Sub-Risk 3\":                    [\"sub_risk_3\"],\n",
    "\n",
    "    # ALL Impact Areas\n",
    "    \"Impact Area 1\":                 [\"impact_1\"],\n",
    "    \"Impact Area 2\":                 [\"impact_2\"],\n",
    "    \"Impact Area 3\":                 [\"impact_3\"],\n",
    "    \"Impact Area 4\":                 [\"impact_4\"],\n",
    "    \"Impact Area 5\":                 [\"impact_5\"],\n",
    "}\n",
    "P2P_ORDER = list(P2P_MAP.keys())\n",
    "\n",
    "def make_p2p_detail(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_in.copy()\n",
    "    rename_map = {}\n",
    "    for out_col, candidates in P2P_MAP.items():\n",
    "        src = _first_present(df, candidates)\n",
    "        if src is not None:\n",
    "            rename_map[src] = out_col\n",
    "    if not rename_map:\n",
    "        return pd.DataFrame(columns=P2P_ORDER)\n",
    "\n",
    "    out = df.rename(columns=rename_map)\n",
    "\n",
    "    # Coerce numerics for money/qty/score\n",
    "    for col in [\"Price per Unit (Doc Curr)\", \"Net PO Value (Doc Curr)\",\n",
    "                \"Gross PO Value (Doc Curr)\", \"Total Value on Release\",\n",
    "                \"Ordered Quantity\", \"Risk Score\"]:\n",
    "        if col in out.columns:\n",
    "            out[col] = pd.to_numeric(out[col], errors=\"coerce\")\n",
    "\n",
    "    # Stringify key identifiers\n",
    "    for col in [\"PO Number\", \"PO Item Number\", \"Vendor Account Number\", \"PR Number\", \"PR Item Number\", \"Plant Code\"]:\n",
    "        if col in out.columns:\n",
    "            out[col] = out[col].astype(\"string\")\n",
    "\n",
    "    have = [c for c in P2P_ORDER if c in out.columns]\n",
    "    return out.loc[:, have]\n",
    "\n",
    "# ============================================================\n",
    "# REPORT 1: Top 10 Vendors & Risky Transactions (exact schema)\n",
    "#   - Sheet 1: Top10_Vendors with EXACT columns\n",
    "#   - Sheet 2: Risky_Txns_Top10 with FULL P2P + all sub-risks/impacts\n",
    "# ============================================================\n",
    "def report_top10_vendors_and_txns(df_in, out_path):\n",
    "    C = {\n",
    "        \"vendor_id\":   \"vendor_or_creditor_acct_no_hpd_po\",\n",
    "        \"vendor_name\": \"vendor_name_1\",\n",
    "        \"document_id\": \"purch_doc_no_src_po\",\n",
    "        \"risk_level\":  \"risk_level\",\n",
    "        \"plant\":       \"plant_src_po\",\n",
    "        \"total_value\": \"on_release_total_value_hpd_po\",\n",
    "        \"impact_value\":\"net_val_po_curr_src_po\",\n",
    "        \"main_scenario\":\"main_risk_scenario\",\n",
    "        \"process\":     \"process\",\n",
    "        \"vendor_category\": \"vendor_category\",\n",
    "        \"del_flag\":    \"po_item_del_flag_src_po\",  # treat 'L' as deletion\n",
    "    }\n",
    "    FILTER_TO_P2P = True\n",
    "\n",
    "    df = df_in.copy()\n",
    "    present = {k: v for k, v in C.items() if v in df.columns}\n",
    "    df = df.rename(columns={present[k]: k for k in present})\n",
    "\n",
    "    for opt in [\"vendor_category\",\"impact_value\",\"process\",\"main_scenario\",\"del_flag\",\"plant\",\"document_id\",\"total_value\",\"vendor_name\"]:\n",
    "        if opt not in df.columns: df[opt] = pd.NA\n",
    "    df = df[~df[\"del_flag\"].fillna(\"\").astype(str).str.upper().eq(\"L\")]\n",
    "\n",
    "    if FILTER_TO_P2P:\n",
    "        if \"process\" in df and df[\"process\"].notna().any():\n",
    "            df = df[df[\"process\"].fillna(\"\").str.upper().eq(\"P2P\")]\n",
    "        elif \"main_scenario\" in df and df[\"main_scenario\"].notna().any():\n",
    "            df = df[df[\"main_scenario\"].fillna(\"\").str.lower().eq(\"procurement risk\")]\n",
    "\n",
    "    risk_series = df[\"risk_level\"].fillna(\"\").astype(str).str.strip()\n",
    "    df_risky = df[(~risk_series.str.lower().eq(\"no risk\")) & (risk_series != \"\")]\n",
    "    if df_risky.empty: return False\n",
    "\n",
    "    df_risky[\"impact_value\"] = pd.to_numeric(df_risky[\"impact_value\"], errors=\"coerce\").fillna(0.0)\n",
    "    df_risky[\"total_value\"]  = pd.to_numeric(df_risky[\"total_value\"],  errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Vendor ranking: rows desc, then total impact desc\n",
    "    grp = (\n",
    "        df_risky.groupby([\"vendor_id\",\"vendor_name\",\"vendor_category\"], dropna=False)\n",
    "                .agg(rows=(\"document_id\",\"count\"),\n",
    "                     total_impact=(\"impact_value\",\"sum\"))\n",
    "                .reset_index()\n",
    "                .sort_values([\"rows\",\"total_impact\"], ascending=[False, False])\n",
    "                .head(10)\n",
    "    )\n",
    "    if grp.empty: return False\n",
    "\n",
    "    top_vendor_ids = set(grp[\"vendor_id\"].astype(str))\n",
    "\n",
    "    # Sheet 1: EXACT columns\n",
    "    idx = (\n",
    "        df_risky[df_risky[\"vendor_id\"].astype(str).isin(top_vendor_ids)]\n",
    "        .sort_values([\"vendor_id\",\"impact_value\"], ascending=[True, False])\n",
    "        .groupby(\"vendor_id\", as_index=False)\n",
    "        .nth(0)\n",
    "    )\n",
    "    top10_table = (\n",
    "        idx.rename(columns={\n",
    "            \"vendor_id\": \"Vendor\",\n",
    "            \"vendor_category\": \"Vendor Category\",\n",
    "            \"document_id\": \"Document ID\",\n",
    "            \"risk_level\": \"Risk Level\",\n",
    "            \"plant\": \"Plant\",\n",
    "            \"total_value\": \"Total Value\",\n",
    "        })[[\"Vendor\", \"Vendor Category\", \"Document ID\", \"Risk Level\", \"Plant\", \"Total Value\"]]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Sheet 2: FULL P2P + ALL sub-risks/impacts\n",
    "    detail_base = df_risky[df_risky[\"vendor_id\"].astype(str).isin(top_vendor_ids)].copy()\n",
    "    detail = make_p2p_detail(detail_base)\n",
    "    sort_cols = [c for c in [\"Vendor Account Number\",\"Vendor Name\",\"Net PO Value (Doc Curr)\"] if c in detail.columns]\n",
    "    if sort_cols:\n",
    "        detail = detail.sort_values(sort_cols, ascending=[True, True, False]).reset_index(drop=True)\n",
    "\n",
    "    writer, fmt = writer_with_formats(out_path)\n",
    "    top10_table.to_excel(writer, \"Top10_Vendors\", index=False)\n",
    "    autofit_and_formats(writer, \"Top10_Vendors\", top10_table, fmt)\n",
    "    detail.to_excel(writer, \"Risky_Txns_Top10\", index=False)\n",
    "    autofit_and_formats(writer, \"Risky_Txns_Top10\", detail, fmt)\n",
    "    writer.close()\n",
    "    return True\n",
    "\n",
    "# ============================================================\n",
    "# REPORT 2: Top 10 High-Risk Transactions (Top10 + All) — FULL P2P + ALL sub-risks/impacts\n",
    "# ============================================================\n",
    "def report_highrisk_txns(df_in, out_path):\n",
    "    RISKY_LEVELS = {\"High Risk\", \"Very High Risk\", \"Needs Validation\"}\n",
    "    FILTER_TO_P2P = True\n",
    "\n",
    "    df = df_in.copy()\n",
    "    for opt in [\"process\",\"main_scenario\",\"del_flag\"]:\n",
    "        if opt not in df.columns: df[opt] = pd.NA\n",
    "    df = df[~df[\"del_flag\"].fillna(\"\").astype(str).str.upper().eq(\"L\")]\n",
    "\n",
    "    if FILTER_TO_P2P:\n",
    "        if \"process\" in df and df[\"process\"].notna().any():\n",
    "            df = df[df[\"process\"].fillna(\"\").str.upper().eq(\"P2P\")]\n",
    "        elif \"main_scenario\" in df and df[\"main_scenario\"].notna().any():\n",
    "            df = df[df[\"main_scenario\"].fillna(\"\").str.lower().eq(\"procurement risk\")]\n",
    "\n",
    "    risk_series = df.get(\"risk_level\", pd.Series(dtype=str)).fillna(\"\").astype(str).str.strip()\n",
    "    df_hr = df[risk_series.str.lower().isin({x.lower() for x in RISKY_LEVELS})]\n",
    "    if df_hr.empty: return False\n",
    "\n",
    "    detail_all = make_p2p_detail(df_hr)\n",
    "    if detail_all.empty: return False\n",
    "\n",
    "    sort_cols = [c for c in [\"Net PO Value (Doc Curr)\",\"Gross PO Value (Doc Curr)\",\"Total Value on Release\"] if c in detail_all.columns]\n",
    "    detail_top10 = (detail_all.sort_values(sort_cols, ascending=[False]*len(sort_cols))\n",
    "                               .head(10).reset_index(drop=True)) if sort_cols else detail_all.head(10)\n",
    "\n",
    "    writer, fmt = writer_with_formats(out_path)\n",
    "    detail_top10.to_excel(writer, \"Top10_HighRisk_Txns\", index=False)\n",
    "    autofit_and_formats(writer, \"Top10_HighRisk_Txns\", detail_top10, fmt)\n",
    "    detail_all.to_excel(writer, \"All_HighRisk_Txns\", index=False)\n",
    "    autofit_and_formats(writer, \"All_HighRisk_Txns\", detail_all, fmt)\n",
    "    writer.close()\n",
    "    return True\n",
    "\n",
    "# ============================================================\n",
    "# REPORT 3: Split PO — FULL P2P + ALL sub-risks/impacts\n",
    "# ============================================================\n",
    "def report_split_po(df_in, out_path):\n",
    "    FILTER_TO_P2P = True\n",
    "    split_pattern = re.compile(r\"\\bsplit[\\s_-]*po\\b\", flags=re.IGNORECASE)\n",
    "\n",
    "    df = df_in.copy()\n",
    "    for opt in [\"process\",\"main_scenario\",\"del_flag\",\"sub_risk_1\",\"sub_risk_2\",\"sub_risk_3\"]:\n",
    "        if opt not in df.columns: df[opt] = pd.NA\n",
    "    df = df[~df[\"del_flag\"].fillna(\"\").astype(str).str.upper().eq(\"L\")]\n",
    "\n",
    "    if FILTER_TO_P2P:\n",
    "        if \"process\" in df and df[\"process\"].notna().any():\n",
    "            df = df[df[\"process\"].fillna(\"\").str.upper().eq(\"P2P\")]\n",
    "        elif \"main_scenario\" in df and df[\"main_scenario\"].notna().any():\n",
    "            df = df[df[\"main_scenario\"].fillna(\"\").str.lower().eq(\"procurement risk\")]\n",
    "\n",
    "    def _is_split(val):\n",
    "        if pd.isna(val): return False\n",
    "        if isinstance(val, (list, tuple, set)):\n",
    "            return any(bool(split_pattern.search(str(x))) for x in val)\n",
    "        return bool(split_pattern.search(str(val)))\n",
    "\n",
    "    risk_cols = [c for c in [\"sub_risk_1\",\"sub_risk_2\",\"sub_risk_3\"] if c in df.columns]\n",
    "    if not risk_cols: return False\n",
    "\n",
    "    mask = False\n",
    "    for c in risk_cols:\n",
    "        mask = mask | df[c].apply(_is_split)\n",
    "    df_split = df[mask]\n",
    "    if df_split.empty: return False\n",
    "\n",
    "    detail = make_p2p_detail(df_split)\n",
    "    if detail.empty: return False\n",
    "    sort_cols = [c for c in [\"Net PO Value (Doc Curr)\",\"Gross PO Value (Doc Curr)\",\"Total Value on Release\"] if c in detail.columns]\n",
    "    if sort_cols:\n",
    "        detail = detail.sort_values(sort_cols, ascending=[False]*len(sort_cols))\n",
    "\n",
    "    writer, fmt = writer_with_formats(out_path)\n",
    "    detail.to_excel(writer, \"SplitPO_Txns\", index=False)\n",
    "    autofit_and_formats(writer, \"SplitPO_Txns\", detail, fmt)\n",
    "    writer.close()\n",
    "    return True\n",
    "\n",
    "# ============================================================\n",
    "# REPORT 4: Price Variance — FULL P2P + ALL sub-risks/impacts\n",
    "# ============================================================\n",
    "def report_price_variance(df_in, out_path):\n",
    "    FILTER_TO_P2P = True\n",
    "    pv_pattern = re.compile(r\"\\bprice[\\s_-]*variance\\b\", flags=re.IGNORECASE)\n",
    "\n",
    "    df = df_in.copy()\n",
    "    for opt in [\"process\",\"main_scenario\",\"del_flag\",\"sub_risk_1\",\"sub_risk_2\",\"sub_risk_3\"]:\n",
    "        if opt not in df.columns: df[opt] = pd.NA\n",
    "    df = df[~df[\"del_flag\"].fillna(\"\").astype(str).str.upper().eq(\"L\")]\n",
    "\n",
    "    if FILTER_TO_P2P:\n",
    "        if \"process\" in df and df[\"process\"].notna().any():\n",
    "            df = df[df[\"process\"].fillna(\"\").str.upper().eq(\"P2P\")]\n",
    "        elif \"main_scenario\" in df and df[\"main_scenario\"].notna().any():\n",
    "            df = df[df[\"main_scenario\"].fillna(\"\").str.lower().eq(\"procurement risk\")]\n",
    "\n",
    "    def _is_pv(val):\n",
    "        if pd.isna(val): return False\n",
    "        if isinstance(val, (list, tuple, set)):\n",
    "            return any(bool(pv_pattern.search(str(x))) for x in val)\n",
    "        return bool(pv_pattern.search(str(val)))\n",
    "\n",
    "    risk_cols = [c for c in [\"sub_risk_1\",\"sub_risk_2\",\"sub_risk_3\"] if c in df.columns]\n",
    "    if not risk_cols: return False\n",
    "\n",
    "    mask = False\n",
    "    for c in risk_cols:\n",
    "        mask = mask | df[c].apply(_is_pv)\n",
    "    df_pv = df[mask]\n",
    "    if df_pv.empty: return False\n",
    "\n",
    "    detail = make_p2p_detail(df_pv)\n",
    "    if detail.empty: return False\n",
    "    sort_cols = [c for c in [\"Net PO Value (Doc Curr)\",\"Gross PO Value (Doc Curr)\",\"Total Value on Release\"] if c in detail.columns]\n",
    "    if sort_cols:\n",
    "        detail = detail.sort_values(sort_cols, ascending=[False]*len(sort_cols))\n",
    "\n",
    "    writer, fmt = writer_with_formats(out_path)\n",
    "    detail.to_excel(writer, \"PriceVariance_Txns\", index=False)\n",
    "    autofit_and_formats(writer, \"PriceVariance_Txns\", detail, fmt)\n",
    "    writer.close()\n",
    "    return True\n",
    "\n",
    "# ============================================================\n",
    "# RUNNER (folderized: Daily/Weekly/Monthly subfolders)\n",
    "# ============================================================\n",
    "def ensure_base_dirs():\n",
    "    for sub in [\"Daily\", \"Weekly\", \"Monthly\"]:\n",
    "        os.makedirs(os.path.join(BASE_OUT, sub), exist_ok=True)\n",
    "\n",
    "def weekly_folder_name(start_d: date, end_d: date):\n",
    "    iso_year, iso_week, _ = start_d.isocalendar()\n",
    "    return f\"{iso_year}-W{iso_week:02d}_{start_d:%Y-%m-%d}_to_{end_d:%Y-%m-%d}\"\n",
    "\n",
    "def monthly_folder_name(start_d: date, end_d: date):\n",
    "    return f\"{start_d:%Y-%m}\"\n",
    "\n",
    "def run_for_period(df_full, start_d: date, end_d: date, granularity: str):\n",
    "    mask = (df_full[\"__report_date\"].dt.date >= start_d) & (df_full[\"__report_date\"].dt.date <= end_d)\n",
    "    df_slice = df_full.loc[mask].copy()\n",
    "    if df_slice.empty: return\n",
    "\n",
    "    if granularity == \"Daily\":\n",
    "        subfolder = f\"{start_d:%Y-%m-%d}\"\n",
    "    elif granularity == \"Weekly\":\n",
    "        subfolder = weekly_folder_name(start_d, end_d)\n",
    "    elif granularity == \"Monthly\":\n",
    "        subfolder = monthly_folder_name(start_d, end_d)\n",
    "    else:\n",
    "        subfolder = f\"{start_d:%Y-%m-%d}_to_{end_d:%Y-%m-%d}\"\n",
    "\n",
    "    folder = os.path.join(BASE_OUT, granularity, subfolder)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    if RUN_TOP10_VENDORS_AND_TXNS:\n",
    "        path = os.path.join(folder, \"Top 10 high-risk vendors by risky transactions.xlsx\")\n",
    "        ok = report_top10_vendors_and_txns(df_slice, path)\n",
    "        if ok: print(f\"[{granularity}] {subfolder} → Top10 Vendors ✔\")\n",
    "\n",
    "    if RUN_HIGHRISK_TXNS:\n",
    "        path = os.path.join(folder, \"Top 10 high-risk transactions.xlsx\")\n",
    "        ok = report_highrisk_txns(df_slice, path)\n",
    "        if ok: print(f\"[{granularity}] {subfolder} → High-Risk Txns ✔\")\n",
    "\n",
    "    if RUN_SPLIT_PO:\n",
    "        path = os.path.join(folder, \"Split PO Report.xlsx\")\n",
    "        ok = report_split_po(df_slice, path)\n",
    "        if ok: print(f\"[{granularity}] {subfolder} → Split PO ✔\")\n",
    "\n",
    "    if RUN_PRICE_VARIANCE:\n",
    "        path = os.path.join(folder, \"Price Variance Risk Report.xlsx\")\n",
    "        ok = report_price_variance(df_slice, path)\n",
    "        if ok: print(f\"[{granularity}] {subfolder} → Price Variance ✔\")\n",
    "\n",
    "# ============================================================\n",
    "# OPTIONAL BLOB UPLOAD (auto-disabled if creds missing)\n",
    "# ============================================================\n",
    "try:\n",
    "    from azure.storage.blob import BlobServiceClient, ContentSettings\n",
    "except Exception:\n",
    "    BlobServiceClient = None\n",
    "    ContentSettings = None\n",
    "\n",
    "AZURE_STORAGE_CONNECTION_STRING = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\", \"\")\n",
    "AZURE_CONTAINER_NAME            = os.getenv(\"AZURE_CONTAINER_NAME\", \"fortifai-reports\")\n",
    "AZURE_BLOB_BASE_PREFIX          = os.getenv(\"AZURE_BLOB_BASE_PREFIX\", \"reports\")\n",
    "UPLOAD_TO_BLOB_DEFAULT = bool(AZURE_STORAGE_CONNECTION_STRING and BlobServiceClient)\n",
    "\n",
    "_blob_client_cache = None\n",
    "\n",
    "def _guess_content_type(path: str):\n",
    "    if ContentSettings is None:\n",
    "        return None\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".xlsx\":\n",
    "        return ContentSettings(content_type=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "    if ext == \".csv\":\n",
    "        return ContentSettings(content_type=\"text/csv\")\n",
    "    return ContentSettings(content_type=\"application/octet-stream\")\n",
    "\n",
    "def _get_blob_service_or_none():\n",
    "    if not UPLOAD_TO_BLOB_DEFAULT:\n",
    "        return None\n",
    "    global _blob_client_cache\n",
    "    if _blob_client_cache is None:\n",
    "        _blob_client_cache = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)\n",
    "    return _blob_client_cache\n",
    "\n",
    "def _ensure_container(name: str):\n",
    "    svc = _get_blob_service_or_none()\n",
    "    if svc is None: return None\n",
    "    try:\n",
    "        svc.create_container(name)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return svc.get_container_client(name)\n",
    "\n",
    "def upload_folder_to_blob(local_folder: str, blob_prefix: str):\n",
    "    svc = _get_blob_service_or_none()\n",
    "    if svc is None:\n",
    "        print(f\"[Blob] Skipped upload (no credentials). Saved locally: {local_folder}\")\n",
    "        return\n",
    "    cont = _ensure_container(AZURE_CONTAINER_NAME)\n",
    "    for fname in os.listdir(local_folder):\n",
    "        fpath = os.path.join(local_folder, fname)\n",
    "        if os.path.isfile(fpath):\n",
    "            blob_path = f\"{AZURE_BLOB_BASE_PREFIX}/{blob_prefix}/{fname}\".replace(\"\\\\\", \"/\")\n",
    "            with open(fpath, \"rb\") as fh:\n",
    "                cont.upload_blob(\n",
    "                    name=blob_path, data=fh, overwrite=True,\n",
    "                    content_settings=_guess_content_type(fpath)\n",
    "                )\n",
    "    print(f\"[Blob] Uploaded: {AZURE_BLOB_BASE_PREFIX}/{blob_prefix}/\")\n",
    "\n",
    "# ============================================================\n",
    "# WRAPPED PERIOD RUNNER (local save + optional upload)\n",
    "# ============================================================\n",
    "def _run_for_period_with_optional_upload(df_full: pd.DataFrame, start_d: date, end_d: date, granularity: str, do_upload: bool):\n",
    "    mask = (df_full[\"__report_date\"].dt.date >= start_d) & (df_full[\"__report_date\"].dt.date <= end_d)\n",
    "    df_slice = df_full.loc[mask].copy()\n",
    "    if df_slice.empty: return\n",
    "\n",
    "    if granularity == \"Daily\":\n",
    "        subfolder = f\"{start_d:%Y-%m-%d}\"\n",
    "    elif granularity == \"Weekly\":\n",
    "        subfolder = weekly_folder_name(start_d, end_d)\n",
    "    elif granularity == \"Monthly\":\n",
    "        subfolder = monthly_folder_name(start_d, end_d)\n",
    "    else:\n",
    "        subfolder = f\"{start_d:%Y-%m-%d}_to_{end_d:%Y-%m-%d}\"\n",
    "\n",
    "    folder = os.path.join(BASE_OUT, granularity, subfolder)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    any_written = False\n",
    "\n",
    "    if RUN_TOP10_VENDORS_AND_TXNS:\n",
    "        p = os.path.join(folder, \"Top 10 high-risk vendors by risky transactions.xlsx\")\n",
    "        if report_top10_vendors_and_txns(df_slice, p): any_written = True\n",
    "\n",
    "    if RUN_HIGHRISK_TXNS:\n",
    "        p = os.path.join(folder, \"Top 10 high-risk transactions.xlsx\")\n",
    "        if report_highrisk_txns(df_slice, p): any_written = True\n",
    "\n",
    "    if RUN_SPLIT_PO:\n",
    "        p = os.path.join(folder, \"Split PO Report.xlsx\")\n",
    "        if report_split_po(df_slice, p): any_written = True\n",
    "\n",
    "    if RUN_PRICE_VARIANCE:\n",
    "        p = os.path.join(folder, \"Price Variance Risk Report.xlsx\")\n",
    "        if report_price_variance(df_slice, p): any_written = True\n",
    "\n",
    "    if any_written and do_upload:\n",
    "        blob_prefix = f\"{granularity}/{subfolder}\"\n",
    "        upload_folder_to_blob(folder, blob_prefix)\n",
    "\n",
    "# ============================================================\n",
    "# PUBLIC API\n",
    "# ============================================================\n",
    "def _prep_df_for_reports(cleaned_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dcol = choose_date_col(cleaned_df)\n",
    "    df = cleaned_df.copy()\n",
    "    df[\"__report_date\"] = pd.to_datetime(df[dcol], errors=\"coerce\")\n",
    "    df = df[df[\"__report_date\"].notna()].copy()\n",
    "    return df\n",
    "\n",
    "def run_reports_job(cleaned_df: pd.DataFrame, mode: str = \"daily\", *, upload_to_blob: bool | None = None):\n",
    "    \"\"\"\n",
    "    Generate reports for the given cleaned DataFrame.\n",
    "    mode: 'daily' | 'weekly' | 'monthly' | 'backfill6m'\n",
    "    upload_to_blob:\n",
    "        - True  -> try to upload (requires azure creds + package)\n",
    "        - False -> local only\n",
    "        - None  -> auto (upload if creds present, else local only)  <-- DEFAULT\n",
    "    \"\"\"\n",
    "    df_full = _prep_df_for_reports(cleaned_df)\n",
    "    if df_full.empty:\n",
    "        print(\"No valid dates after parsing — nothing to report.\")\n",
    "        return\n",
    "\n",
    "    if upload_to_blob is None:\n",
    "        do_upload = UPLOAD_TO_BLOB_DEFAULT\n",
    "    else:\n",
    "        do_upload = bool(upload_to_blob and UPLOAD_TO_BLOB_DEFAULT)\n",
    "\n",
    "    ensure_base_dirs()\n",
    "\n",
    "    if mode == \"daily\":\n",
    "        today = pd.Timestamp.now(tz=TZ_NAME).date()\n",
    "        _run_for_period_with_optional_upload(df_full, today, today, \"Daily\", do_upload)\n",
    "\n",
    "    elif mode == \"weekly\":\n",
    "        today = pd.Timestamp.now(tz=TZ_NAME).date()\n",
    "        start = start_of_week_monday(today)\n",
    "        _run_for_period_with_optional_upload(df_full, start, today, \"Weekly\", do_upload)\n",
    "\n",
    "    elif mode == \"monthly\":\n",
    "        today = pd.Timestamp.now(tz=TZ_NAME).date()\n",
    "        start = date(today.year, today.month, 1)\n",
    "        _run_for_period_with_optional_upload(df_full, start, today, \"Monthly\", do_upload)\n",
    "\n",
    "    elif mode == \"backfill6m\":\n",
    "        start, end = clamp_to_six_months(df_full, \"__report_date\")\n",
    "        for s, e in iter_daily(start, end):   _run_for_period_with_optional_upload(df_full, s, e, \"Daily\",   do_upload)\n",
    "        for s, e in iter_weekly(start, end):  _run_for_period_with_optional_upload(df_full, s, e, \"Weekly\",  do_upload)\n",
    "        for s, e in iter_monthly(start, end): _run_for_period_with_optional_upload(df_full, s, e, \"Monthly\", do_upload)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"mode must be one of: daily, weekly, monthly, backfill6m\")\n",
    "\n",
    "# ============================================================\n",
    "# OPTIONAL: Long-running scheduler (IST)\n",
    "# ============================================================\n",
    "def start_report_scheduler(get_df_callable):\n",
    "    \"\"\"\n",
    "    Pass a zero-arg callable that returns the latest cleaned DataFrame each run.\n",
    "    Example: start_report_scheduler(lambda: cleaned_df)\n",
    "    \"\"\"\n",
    "    import pytz\n",
    "    from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "    from apscheduler.triggers.cron import CronTrigger\n",
    "\n",
    "    def _get_df():\n",
    "        df = get_df_callable()\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(\"get_df_callable() must return a pandas DataFrame\")\n",
    "        return df\n",
    "\n",
    "    def job_daily():\n",
    "        print(\"[Scheduler] Daily job started].\")\n",
    "        run_reports_job(_get_df(), mode=\"daily\")  # auto: local-only if no blob creds\n",
    "        print(\"[Scheduler] Daily job completed.\")\n",
    "\n",
    "    def job_weekly():\n",
    "        print(\"[Scheduler] Weekly job started.\")\n",
    "        run_reports_job(_get_df(), mode=\"weekly\")\n",
    "        print(\"[Scheduler] Weekly job completed.\")\n",
    "\n",
    "    def job_monthly():\n",
    "        print(\"[Scheduler] Monthly job started.\")\n",
    "        run_reports_job(_get_df(), mode=\"monthly\")\n",
    "        print(\"[Scheduler] Monthly job completed.\")\n",
    "\n",
    "    tz = pytz.timezone(TZ_NAME)\n",
    "    sched = BlockingScheduler(timezone=tz)\n",
    "\n",
    "    # Daily at 02:00 IST\n",
    "    sched.add_job(job_daily,  CronTrigger(day=\"*\", hour=2, minute=0))\n",
    "    # Weekly (Mondays) at 02:30 IST\n",
    "    sched.add_job(job_weekly, CronTrigger(day_of_week=\"mon\", hour=2, minute=30))\n",
    "    # Monthly (1st) at 03:00 IST\n",
    "    sched.add_job(job_monthly, CronTrigger(day=\"1\", hour=3, minute=0))\n",
    "\n",
    "    print(\"Report scheduler started (IST). Ctrl+C to stop.\")\n",
    "    try:\n",
    "        sched.start()\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        print(\"Scheduler stopped.\")\n",
    "\n",
    "# ============================================================\n",
    "# Example usage\n",
    "# ============================================================\n",
    "# cleaned = ...  # your DataFrame\n",
    "# run_reports_job(cleaned, mode=\"daily\")\n",
    "# run_reports_job(cleaned, mode=\"weekly\")\n",
    "# run_reports_job(cleaned, mode=\"monthly\")\n",
    "run_reports_job(cleaned, mode=\"backfill6m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf931df-3f11-4429-ab14-af66ee75f469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (py312env)",
   "language": "python",
   "name": "py312env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
