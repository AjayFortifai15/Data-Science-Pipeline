{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a059e2df-b099-4731-9617-f95c06a3ec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayn\\AppData\\Local\\Temp\\ipykernel_35140\\1632847627.py:117: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  summary_multi = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import psycopg2\n",
    "# ---- choose the severities you want ----\n",
    "severities = [\"no risk\", \"high risk\", \"needs validation\", \"very high risk\"]  # case-insensitive\n",
    "sev_array_sql = \",\".join([f\"'{s}'\" for s in severities])\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    lower(risk_severity)  AS risk_severity,\n",
    "    risk_category,\n",
    "    risk_definition,\n",
    "    -- counts\n",
    "    COALESCE(txn_count_24h,0)::numeric AS c_24h,\n",
    "    COALESCE(txn_count_3d,0)::numeric  AS c_3d,\n",
    "    COALESCE(txn_count_7d,0)::numeric  AS c_7d,\n",
    "    COALESCE(txn_count_1m,0)::numeric  AS c_1m,\n",
    "    COALESCE(txn_count_3m,0)::numeric  AS c_3m,\n",
    "    COALESCE(txn_count_6m,0)::numeric  AS c_6m,\n",
    "    -- amounts\n",
    "    COALESCE(txn_amt_24h,0)::numeric   AS a_24h,\n",
    "    COALESCE(txn_amt_3d,0)::numeric    AS a_3d,\n",
    "    COALESCE(txn_amt_7d,0)::numeric    AS a_7d,\n",
    "    COALESCE(txn_amt_1m,0)::numeric    AS a_1m,\n",
    "    COALESCE(txn_amt_3m,0)::numeric    AS a_3m,\n",
    "    COALESCE(txn_amt_6m,0)::numeric    AS a_6m\n",
    "  FROM semantic_db.vw_risk_summary_by_severity_category_and_def_6m\n",
    "  WHERE risk_definition IS NOT NULL\n",
    "    AND risk_definition !~* '^overall$'\n",
    "    AND lower(risk_severity) = ANY(ARRAY[{sev_array_sql}])\n",
    "),\n",
    "exploded AS (\n",
    "  SELECT risk_severity, risk_category, risk_definition, win, cnt, amt\n",
    "  FROM (\n",
    "    SELECT risk_severity, risk_category, risk_definition, '24h' AS win, c_24h AS cnt, a_24h AS amt FROM base\n",
    "    UNION ALL SELECT risk_severity, risk_category, risk_definition, '3d',  c_3d,  a_3d  FROM base\n",
    "    UNION ALL SELECT risk_severity, risk_category, risk_definition, '7d',  c_7d,  a_7d  FROM base\n",
    "    UNION ALL SELECT risk_severity, risk_category, risk_definition, '1m',  c_1m,  a_1m  FROM base\n",
    "    UNION ALL SELECT risk_severity, risk_category, risk_definition, '3m',  c_3m,  a_3m  FROM base\n",
    "    UNION ALL SELECT risk_severity, risk_category, risk_definition, '6m',  c_6m,  a_6m  FROM base\n",
    "  ) u\n",
    "),\n",
    "agg_def AS (\n",
    "  SELECT\n",
    "    risk_severity,\n",
    "    win,\n",
    "    risk_category,\n",
    "    risk_definition,\n",
    "    SUM(cnt) AS def_cnt,\n",
    "    SUM(amt) AS def_amt\n",
    "  FROM exploded\n",
    "  GROUP BY risk_severity, win, risk_category, risk_definition\n",
    ")\n",
    "SELECT\n",
    "  risk_severity,\n",
    "  win,\n",
    "  -- severity totals across all categories\n",
    "  SUM(def_cnt) AS severity_total_count,\n",
    "  SUM(def_amt) AS severity_total_amount,\n",
    "\n",
    "  -- Dynamic category (No Risk -> 'No Risk', else 'Procurement Risk')\n",
    "  SUM(def_cnt) FILTER (\n",
    "    WHERE risk_category ILIKE\n",
    "      CASE WHEN lower(risk_severity) = 'no risk' THEN 'no risk' ELSE 'procurement risk' END\n",
    "  ) AS category_count,\n",
    "  SUM(def_amt) FILTER (\n",
    "    WHERE risk_category ILIKE\n",
    "      CASE WHEN lower(risk_severity) = 'no risk' THEN 'no risk' ELSE 'procurement risk' END\n",
    "  ) AS category_amount,\n",
    "\n",
    "  -- Within chosen category: per risk_definition maps\n",
    "  jsonb_object_agg(risk_definition, def_cnt)\n",
    "    FILTER (WHERE risk_category ILIKE\n",
    "      CASE WHEN lower(risk_severity) = 'no risk' THEN 'no risk' ELSE 'procurement risk' END\n",
    "    ) AS category_by_definition_counts,\n",
    "\n",
    "  jsonb_object_agg(risk_definition, def_amt)\n",
    "    FILTER (WHERE risk_category ILIKE\n",
    "      CASE WHEN lower(risk_severity) = 'no risk' THEN 'no risk' ELSE 'procurement risk' END\n",
    "    ) AS category_by_definition_amounts\n",
    "\n",
    "FROM agg_def\n",
    "GROUP BY risk_severity, win\n",
    "ORDER BY\n",
    "  CASE win WHEN '24h' THEN 1 WHEN '3d' THEN 2 WHEN '7d' THEN 3\n",
    "           WHEN '1m'  THEN 4 WHEN '3m' THEN 5 WHEN '6m' THEN 6 END,\n",
    "  CASE risk_severity\n",
    "    WHEN 'no risk' THEN 1\n",
    "    WHEN 'high risk' THEN 2\n",
    "    WHEN 'needs validation' THEN 3\n",
    "    WHEN 'very high risk' THEN 4\n",
    "    ELSE 99\n",
    "  END;\n",
    "\"\"\"\n",
    "\n",
    "#conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "#\t\t\tdatabase='baldota-dev-db',\n",
    "#\t\t\tuser='fortifai_ng_user_ro',\n",
    "#\t\t\tpassword='user@123!',\n",
    "#\t\t\tport='5432',\n",
    "#            sslmode=\"require\"\n",
    "#\t\t)\n",
    "\n",
    "conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_ai_user_rw',\n",
    "    \t\t\tpassword='AIPwd@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "cur = conn.cursor()\n",
    "# make this transaction read-only\n",
    "conn.set_session(readonly=True)        # start session as read-only\n",
    "try:\n",
    "    summary_multi = pd.read_sql_query(query, conn)\n",
    "finally:\n",
    "    conn.commit()                      # or conn.rollback()\n",
    "    conn.set_session(readonly=False)   # now safe\n",
    "\n",
    "\n",
    "# Parse JSONB to dicts if strings\n",
    "for col in [\"category_by_definition_counts\", \"category_by_definition_amounts\"]:\n",
    "    summary_multi[col] = summary_multi[col].apply(lambda x: json.loads(x) if isinstance(x, str) and x else (x or {}))\n",
    "\n",
    "# ---- Helpers ----\n",
    "def fmt_int(x):\n",
    "    try:\n",
    "        v = float(x)\n",
    "        if not math.isfinite(v): return \"0\"\n",
    "        return f\"{int(round(v)):,}\"\n",
    "    except:\n",
    "        return \"0\"\n",
    "\n",
    "def fmt_amt(x):\n",
    "    try:\n",
    "        v = float(x)\n",
    "        if not math.isfinite(v): return \"0.00\"\n",
    "        return f\"{v:,.2f}\"\n",
    "    except:\n",
    "        return \"0.00\"\n",
    "\n",
    "def fmt_pct(n, d):\n",
    "    try:\n",
    "        n = float(n); d = float(d)\n",
    "        if d <= 0 or not math.isfinite(n) or not math.isfinite(d):\n",
    "            return \"0.00%\"\n",
    "        return f\"{(n/d)*100:,.2f}%\"\n",
    "    except:\n",
    "        return \"0.00%\"\n",
    "\n",
    "def fmt_outof(n, d, kind=\"int\"):\n",
    "    if kind == \"int\":\n",
    "        return f\"{fmt_int(n)} of {fmt_int(d)}\"\n",
    "    else:\n",
    "        return f\"{fmt_amt(n)} of {fmt_amt(d)}\"\n",
    "\n",
    "def nice_sev(s: str) -> str:\n",
    "    s = (s or \"\").lower()\n",
    "    if s == \"high risk\": return \"High Risk\"\n",
    "    if s == \"needs validation\": return \"Needs Validation\"\n",
    "    if s == \"very high risk\": return \"Very High Risk\"\n",
    "    if s == \"no risk\": return \"No Risk\"\n",
    "    return s.title()\n",
    "\n",
    "def category_for_severity(sev_label: str) -> str:\n",
    "    return \"No Risk\" if sev_label == \"No Risk\" else \"Procurement Risk\"\n",
    "\n",
    "summary_multi[\"sev_label\"] = summary_multi[\"risk_severity\"].apply(nice_sev)\n",
    "\n",
    "# Orders\n",
    "win_order = [\"24h\", \"3d\", \"7d\", \"1m\", \"3m\", \"6m\"]\n",
    "sev_order = [\"High Risk\", \"Needs Validation\", \"Very High Risk\", \"No Risk\"]\n",
    "preferred_defs = [\"No Risk\", \"Price Variance Risk\", \"Split PO\"]\n",
    "\n",
    "# Ensure numerics\n",
    "for col in [\"severity_total_count\", \"severity_total_amount\", \"category_count\", \"category_amount\"]:\n",
    "    summary_multi[col] = pd.to_numeric(summary_multi[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Index for quick lookup\n",
    "summary_multi.set_index([\"win\", \"sev_label\"], inplace=True)\n",
    "\n",
    "# Window totals (for % of window)\n",
    "win_totals = summary_multi.groupby(level=0)[[\"severity_total_count\", \"severity_total_amount\"]].sum()\n",
    "win_total_cnt = win_totals[\"severity_total_count\"].to_dict()\n",
    "win_total_amt = win_totals[\"severity_total_amount\"].to_dict()\n",
    "\n",
    "# Build the summary text for each (win, severity) with dynamic category + percentages + \"x of y\"\n",
    "def build_text(win, sev):\n",
    "    cat = category_for_severity(sev)  # 'Procurement Risk' or 'No Risk'\n",
    "\n",
    "    if (win, sev) not in summary_multi.index:\n",
    "        return (f\"[{win}] [{sev}] Total: 0 of 0 (0.00% of window, \"\n",
    "                f\"amount 0.00 of 0.00 â€” 0.00% of window); \"\n",
    "                f\"{cat}: 0 of 0 (0.00% of {sev}, amount 0.00 of 0.00 â€” 0.00% of {sev}). \"\n",
    "                f\"Within {cat} â€” counts â†’ â€”; amounts â†’ â€”\")\n",
    "\n",
    "    r = summary_multi.loc[(win, sev)]\n",
    "\n",
    "    total_cnt = r[\"severity_total_count\"]\n",
    "    total_amt = r[\"severity_total_amount\"]\n",
    "    cat_cnt   = r[\"category_count\"]\n",
    "    cat_amt   = r[\"category_amount\"]\n",
    "\n",
    "    # Window totals\n",
    "    w_cnt = win_total_cnt.get(win, 0)\n",
    "    w_amt = win_total_amt.get(win, 0)\n",
    "\n",
    "    # %s\n",
    "    pct_sev_of_win_cnt = fmt_pct(total_cnt, w_cnt)\n",
    "    pct_sev_of_win_amt = fmt_pct(total_amt, w_amt)\n",
    "    pct_cat_of_sev_cnt = fmt_pct(cat_cnt, total_cnt)\n",
    "    pct_cat_of_sev_amt = fmt_pct(cat_amt, total_amt)\n",
    "\n",
    "    # Definitions within the chosen category\n",
    "    by_def_c = r[\"category_by_definition_counts\"] or {}\n",
    "    by_def_a = r[\"category_by_definition_amounts\"] or {}\n",
    "\n",
    "    keys = [k for k in preferred_defs if k in by_def_c] + [k for k in by_def_c if k not in preferred_defs]\n",
    "\n",
    "    if keys and cat_cnt > 0:\n",
    "        parts_c = [f\"{k}: {fmt_outof(by_def_c.get(k, 0), cat_cnt, 'int')} ({fmt_pct(by_def_c.get(k, 0), cat_cnt)})\"\n",
    "                   for k in keys]\n",
    "    else:\n",
    "        parts_c = [\"â€”\"]\n",
    "\n",
    "    if keys and float(cat_amt) > 0:\n",
    "        parts_a = [f\"{k}: {fmt_outof(by_def_a.get(k, 0), cat_amt, 'amt')} ({fmt_pct(by_def_a.get(k, 0), cat_amt)})\"\n",
    "                   for k in keys]\n",
    "    else:\n",
    "        parts_a = [\"â€”\"]\n",
    "\n",
    "    return (\n",
    "        f\"[{win}] [{sev}] \"\n",
    "        f\"Total: {fmt_outof(total_cnt, w_cnt, 'int')} ({pct_sev_of_win_cnt} of window, \"\n",
    "        f\"amount {fmt_outof(total_amt, w_amt, 'amt')} â€” {pct_sev_of_win_amt} of window); \"\n",
    "        f\"{cat}: {fmt_outof(cat_cnt, total_cnt, 'int')} ({pct_cat_of_sev_cnt} of {sev}, \"\n",
    "        f\"amount {fmt_outof(cat_amt, total_amt, 'amt')} â€” {pct_cat_of_sev_amt} of {sev}). \"\n",
    "        f\"Within {cat} â€” counts â†’ \" + \", \".join(parts_c) +\n",
    "        \"; amounts â†’ \" + \", \".join(parts_a)\n",
    "    )\n",
    "\n",
    "# Assemble the grid: rows=time windows, cols=severities, cells=summary text\n",
    "data = {sev: [build_text(win, sev) for win in win_order] for sev in sev_order}\n",
    "summary_text_grid = pd.DataFrame(data, index=win_order)\n",
    "\n",
    "# Optional: a single-column dataframe with merged summaries per window\n",
    "def merge_window(df, window, severities):\n",
    "    if window not in df.index:\n",
    "        return \"\"\n",
    "    cols = [c for c in severities if c in df.columns]\n",
    "    parts = []\n",
    "    for c in cols:\n",
    "        val = df.at[window, c] if c in df.columns else \"\"\n",
    "        if pd.notna(val) and str(val).strip():\n",
    "            parts.append(str(val).strip())\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "all_merged = {w: merge_window(summary_text_grid, w, sev_order) for w in win_order}\n",
    "all_summaries_df = pd.DataFrame({\"All Summary\": [all_merged[w] for w in win_order]}, index=win_order)\n",
    "\n",
    "# Outputs:\n",
    "# - summary_text_grid : main table with \"x of y\" + percentages\n",
    "# - all_summaries_df  : one-column merged summaries per time window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e753d6b2-fd04-4fee-ab39-073a19965fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24h</th>\n",
       "      <td>[24h] [High Risk] Total: 0 of 0 (0.00% of wind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d</th>\n",
       "      <td>[3d] [High Risk] Total: 0 of 0 (0.00% of windo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7d</th>\n",
       "      <td>[7d] [High Risk] Total: 0 of 0 (0.00% of windo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1m</th>\n",
       "      <td>[1m] [High Risk] Total: 0 of 0 (0.00% of windo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3m</th>\n",
       "      <td>[3m] [High Risk] Total: 116 of 1,560 (7.44% of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6m</th>\n",
       "      <td>[6m] [High Risk] Total: 260 of 4,366 (5.96% of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           All Summary\n",
       "24h  [24h] [High Risk] Total: 0 of 0 (0.00% of wind...\n",
       "3d   [3d] [High Risk] Total: 0 of 0 (0.00% of windo...\n",
       "7d   [7d] [High Risk] Total: 0 of 0 (0.00% of windo...\n",
       "1m   [1m] [High Risk] Total: 0 of 0 (0.00% of windo...\n",
       "3m   [3m] [High Risk] Total: 116 of 1,560 (7.44% of...\n",
       "6m   [6m] [High Risk] Total: 260 of 4,366 (5.96% of..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_summaries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acc3526b-2110-4f33-ac75-abaa491e50dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6m] [High Risk] Total: 260 of 4,366 (5.96% of window, amount 565,010,809.75 of 7,098,567,256.29 â€” 7.96% of window); Procurement Risk: 260 of 260 (100.00% of High Risk, amount 565,010,809.75 of 565,010,809.75 â€” 100.00% of High Risk). Within Procurement Risk â€” counts â†’ Price Variance Risk: 211 of 260 (81.15%), Split PO: 49 of 260 (18.85%); amounts â†’ Price Variance Risk: 148,315,472.36 of 565,010,809.75 (26.25%), Split PO: 416,695,337.39 of 565,010,809.75 (73.75%)\n",
      "\n",
      "[6m] [Needs Validation] Total: 169 of 4,366 (3.87% of window, amount 1,189,987,193.40 of 7,098,567,256.29 â€” 16.76% of window); Procurement Risk: 169 of 169 (100.00% of Needs Validation, amount 1,189,987,193.40 of 1,189,987,193.40 â€” 100.00% of Needs Validation). Within Procurement Risk â€” counts â†’ Price Variance Risk: 169 of 169 (100.00%); amounts â†’ Price Variance Risk: 1,189,987,193.40 of 1,189,987,193.40 (100.00%)\n",
      "\n",
      "[6m] [Very High Risk] Total: 80 of 4,366 (1.83% of window, amount 913,484,732.12 of 7,098,567,256.29 â€” 12.87% of window); Procurement Risk: 80 of 80 (100.00% of Very High Risk, amount 913,484,732.12 of 913,484,732.12 â€” 100.00% of Very High Risk). Within Procurement Risk â€” counts â†’ Price Variance Risk: 75 of 80 (93.75%), Split PO: 5 of 80 (6.25%); amounts â†’ Price Variance Risk: 790,708,732.12 of 913,484,732.12 (86.56%), Split PO: 122,776,000.00 of 913,484,732.12 (13.44%)\n",
      "\n",
      "[6m] [No Risk] Total: 3,857 of 4,366 (88.34% of window, amount 4,430,084,521.03 of 7,098,567,256.29 â€” 62.41% of window); No Risk: 3,857 of 3,857 (100.00% of No Risk, amount 4,430,084,521.03 of 4,430,084,521.03 â€” 100.00% of No Risk). Within No Risk â€” counts â†’ No Risk: 3,857 of 3,857 (100.00%); amounts â†’ No Risk: 4,430,084,521.03 of 4,430,084,521.03 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "high_24h_summary = all_summaries_df.loc[\"6m\", \"All Summary\"]\n",
    "print(high_24h_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585eb38-e824-4d70-868b-d7be712b2b7f",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "ai_summary = (\n",
    "    \"\"\"In the last 6 months, FortifAIâ€™s AI engine SARAâ„¢ analyzed 15,007 transactions (â‰ˆ8.98B total value). Risk mix: High Risk 10.12% (1,518), Very High Risk 4.98% (748), Needs Validation 6.88% (1,033), and No Risk 78.02% (11,708). Procurement-related activity accounts for 3,299 / 15,007 transactions (21.98%) and â‰ˆ7.75B / 8.98B in value (86.32%).\n",
    "\n",
    "Within Procurement by count: Price Variance Risk 3,058 / 3,299 (92.70%), Split PO 83 / 3,299 (2.52%), and No Risk 158 / 3,299 (4.79%).\n",
    "\n",
    "Within Procurement by value: Price Variance Risk â‰ˆ6.38B / 7.75B (82.37%), Split PO â‰ˆ0.81B / 7.75B (10.51%), and No Risk â‰ˆ0.55B / 7.75B (7.12%).\n",
    "\n",
    "These insights highlight where reviews should focus and support early risk mitigation across procurement operations.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "df = pd.DataFrame([{\n",
    "    # meta\n",
    "    \"ai_summary\": ai_summary,\n",
    "    \"time_range_filter\": \"Last 6 Months\",\n",
    "}])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff613950-941d-44f9-a29a-b28e6565b541",
   "metadata": {},
   "source": [
    "ai_summary = \"\"\"In the last 6 months, FortifAIâ€™s AI engine SARAâ„¢ analyzed 15,007 transactions (â‰ˆ â‚¹8.98B total value). Risk mix by count: High Risk 9.99% (1,500), Very High Risk 4.96% (745), Needs Validation 1.76% (264), and No Risk 83.28% (12,498).\n",
    "\n",
    "Procurement-related activity accounts for 2,509 / 15,007 transactions (16.72%) and â‰ˆ â‚¹4.53B / â‚¹8.98B in value (50.51%).\n",
    "\n",
    "Within Procurement by count: Price Variance Risk 2,429 / 2,509 (96.81%) and Split PO 80 / 2,509 (3.19%).\n",
    "Within Procurement by value: Price Variance Risk â‰ˆ â‚¹3.72B / â‚¹4.53B (82.04%) and Split PO â‰ˆ â‚¹0.81B / â‚¹4.53B (17.96%).\n",
    "\n",
    "These insights point to Price Variance as the dominant driver by both count and value, suggesting review efforts should prioritize price-variance cases first, with targeted checks on Split PO activity.\n",
    "\"\"\"\n",
    "#conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "#\t\t\tdatabase='baldota-dev-db',\n",
    "#\t\t\tuser='fortifai_ng_user_ro',\n",
    "#\t\t\tpassword='user@123!',\n",
    "#\t\t\tport='5432',\n",
    "#            sslmode=\"require\"\n",
    "#\t\t)\n",
    "\n",
    "conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_ai_user_rw',\n",
    "    \t\t\tpassword='AIPwd@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "\n",
    "cur = conn.cursor()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO transform_db.ai_summary_history (ai_summary, time_range_filter)\n",
    "        VALUES (%s, %s);\n",
    "    \"\"\", (ai_summary, \"Last 6 Months\"))  # or \"Last 6 Months\"\n",
    "    #new_id = cur.fetchone()[0]\n",
    "conn.commit()\n",
    "print(\"Inserted row\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e4661-5929-4a12-a488-9c0f0bcc9427",
   "metadata": {},
   "source": [
    "ai_summary = \"\"\"In the last 6 months, FortifAIâ€™s AI engine SARAâ„¢ analyzed 15,007 transactions (â‰ˆ â‚¹8.98B total value). Risk mix by count: High Risk 9.99% (1,500), Very High Risk 4.96% (745), Needs Validation 1.76% (264), and No Risk 83.28% (12,498).\n",
    "\n",
    "Procurement-related activity accounts for 2,509 / 15,007 transactions (16.72%) and â‰ˆ â‚¹4.53B / â‚¹8.98B in value (50.51%).\n",
    "\n",
    "Within Procurement by count: Price Variance Risk 2,429 / 2,509 (96.81%) and Split PO 80 / 2,509 (3.19%).\n",
    "Within Procurement by value: Price Variance Risk â‰ˆ â‚¹3.72B / â‚¹4.53B (82.04%) and Split PO â‰ˆ â‚¹0.81B / â‚¹4.53B (17.96%).\n",
    "\n",
    "These insights point to Price Variance as the dominant driver by both count and value, suggesting review efforts should prioritize price-variance cases first, with targeted checks on Split PO activity.\n",
    "\"\"\"\n",
    "#conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "#\t\t\tdatabase='baldota-dev-db',\n",
    "#\t\t\tuser='fortifai_ng_user_ro',\n",
    "#\t\t\tpassword='user@123!',\n",
    "#\t\t\tport='5432',\n",
    "#            sslmode=\"require\"\n",
    "#\t\t)\n",
    "\n",
    "conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_ai_user_rw',\n",
    "    \t\t\tpassword='AIPwd@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "cur = conn.cursor()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO transform_db.ai_summary_history (ai_summary, time_range_filter)\n",
    "        VALUES (%s, %s);\n",
    "    \"\"\", (ai_summary, \"Last 6 Months\"))  # or \"Last 6 Months\"\n",
    "    #new_id = cur.fetchone()[0]\n",
    "conn.commit()\n",
    "print(\"Inserted row\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e4f07b-78a5-481c-b98b-eccfff094d00",
   "metadata": {},
   "source": [
    "#conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "#\t\t\tdatabase='baldota-dev-db',\n",
    "#\t\t\tuser='fortifai_ng_user_ro',\n",
    "#\t\t\tpassword='user@123!',\n",
    "#\t\t\tport='5432',\n",
    "#            sslmode=\"require\"\n",
    "#\t\t)\n",
    "\n",
    "conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_ai_user_rw',\n",
    "    \t\t\tpassword='AIPwd@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "cur = conn.cursor()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        DELETE FROM transform_db.ai_summary_history\n",
    "WHERE ctid IN (\n",
    "  SELECT ctid\n",
    "  FROM transform_db.ai_summary_history\n",
    "  ORDER BY ctid DESC\n",
    "  LIMIT 1\n",
    ");\n",
    "    \"\"\")  # or \"Last 6 Months\"\n",
    "    #new_id = cur.fetchone()[0]\n",
    "conn.commit()\n",
    "print(\"Deleted row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff0241-2429-462d-9ef0-0e0344b88738",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "FortifAI Risk Analysis Summarizer - FastAPI Application\n",
    "Production-level API for generating AI-powered risk summaries\n",
    "\"\"\"\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Query, status\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, Any\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import psycopg2\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"FortifAI Risk Analysis API\",\n",
    "    description=\"AI-powered risk analysis and summarization API using Gemini AI\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=\"/redoc\"\n",
    ")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Pydantic Models\n",
    "class HealthResponse(BaseModel):\n",
    "    status: str\n",
    "    timestamp: str\n",
    "    version: str\n",
    "\n",
    "class SummaryRequest(BaseModel):\n",
    "    timeperiod: str = Field(default=\"6m\", description=\"Time period (24h, 3d, 7d, 1m, 3m, 6m)\")\n",
    "    api_key: Optional[str] = Field(None, description=\"Optional Gemini API key override\")\n",
    "\n",
    "class SummaryResponse(BaseModel):\n",
    "    success: bool\n",
    "    timeperiod: str\n",
    "    summary: Optional[str]\n",
    "    raw_data: Optional[str]\n",
    "    error: Optional[str] = None\n",
    "    timestamp: str\n",
    "\n",
    "class ErrorResponse(BaseModel):\n",
    "    success: bool = False\n",
    "    error: str\n",
    "    timestamp: str\n",
    "\n",
    "# Configuration Classes\n",
    "@dataclass\n",
    "class DatabaseConfig:\n",
    "    \"\"\"Database configuration settings\"\"\"\n",
    "    host: str\n",
    "    database: str\n",
    "    user: str\n",
    "    password: str\n",
    "    port: str = '5432'\n",
    "    sslmode: str = \"require\"\n",
    "\n",
    "@dataclass\n",
    "class GeminiConfig:\n",
    "    \"\"\"Gemini AI configuration settings\"\"\"\n",
    "    api_key: str\n",
    "    model_name: str = 'gemini-2.0-flash-exp'\n",
    "    max_retries: int = 3\n",
    "\n",
    "class ConfigManager:\n",
    "    \"\"\"Manages configuration from environment variables\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_db_config() -> DatabaseConfig:\n",
    "        \"\"\"Get database configuration from environment variables\"\"\"\n",
    "        return DatabaseConfig(\n",
    "            host=os.getenv('DB_HOST', 'fortifai-ng-dev-db.postgres.database.azure.com'),\n",
    "            database=os.getenv('DB_NAME', 'baldota-dev-db'),\n",
    "            user=os.getenv('DB_USER', 'fortifai_ng_ai_user_rw'),\n",
    "            password=os.getenv('DB_PASSWORD', 'AIPwd@123!'),\n",
    "            port=os.getenv('DB_PORT', '5432'),\n",
    "            sslmode=os.getenv('DB_SSL_MODE', 'require')\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gemini_config() -> GeminiConfig:\n",
    "        \"\"\"Get Gemini configuration from environment variables\"\"\"\n",
    "        api_key = os.getenv('GOOGLE_AI_API_KEY', 'AIzaSyCg7ocCHLoZzL5EfLJx30cfoxxScMHZ-EA')\n",
    "        if not api_key:\n",
    "            raise ValueError(\"GOOGLE_AI_API_KEY environment variable is required\")\n",
    "        \n",
    "        return GeminiConfig(\n",
    "            api_key=api_key,\n",
    "            model_name=os.getenv('GEMINI_MODEL', 'gemini-2.0-flash-exp'),\n",
    "            max_retries=int(os.getenv('GEMINI_MAX_RETRIES', '3'))\n",
    "        )\n",
    "\n",
    "class DatabaseManager:\n",
    "    \"\"\"Handles database connections and operations\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DatabaseConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    @contextmanager\n",
    "    def get_connection(self):\n",
    "        \"\"\"Get database connection with proper error handling and cleanup\"\"\"\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = psycopg2.connect(\n",
    "                host=self.config.host,\n",
    "                database=self.config.database,\n",
    "                user=self.config.user,\n",
    "                password=self.config.password,\n",
    "                port=self.config.port,\n",
    "                sslmode=self.config.sslmode\n",
    "            )\n",
    "            conn.set_session(readonly=True)\n",
    "            logger.info(\"Database connection established\")\n",
    "            yield conn\n",
    "        except psycopg2.Error as e:\n",
    "            logger.error(f\"Database connection error: {e}\")\n",
    "            if conn:\n",
    "                conn.rollback()\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected database error: {e}\")\n",
    "            if conn:\n",
    "                conn.rollback()\n",
    "            raise\n",
    "        finally:\n",
    "            if conn:\n",
    "                try:\n",
    "                    conn.commit()\n",
    "                    conn.set_session(readonly=False)\n",
    "                    conn.close()\n",
    "                    logger.info(\"Database connection closed\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error closing database connection: {e}\")\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Processes and formats risk analysis data\"\"\"\n",
    "    \n",
    "    SEVERITIES = [\"no risk\", \"high risk\", \"needs validation\", \"very high risk\"]\n",
    "    WIN_ORDER = [\"24h\", \"3d\", \"7d\", \"1m\", \"3m\", \"6m\"]\n",
    "    SEV_ORDER = [\"High Risk\", \"Needs Validation\", \"Very High Risk\", \"No Risk\"]\n",
    "    PREFERRED_DEFS = [\"No Risk\", \"Price Variance Risk\", \"Split PO\"]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_risk_query(severities: list) -> str:\n",
    "        \"\"\"Generate the SQL query for risk analysis\"\"\"\n",
    "        sev_array_sql = \",\".join([f\"'{s}'\" for s in severities])\n",
    "        \n",
    "        return f\"\"\"\n",
    "        WITH base AS (\n",
    "          SELECT\n",
    "            lower(risk_severity)  AS risk_severity,\n",
    "            risk_category,\n",
    "            risk_definition,\n",
    "            COALESCE(txn_count_24h,0)::numeric AS c_24h,\n",
    "            COALESCE(txn_count_3d,0)::numeric  AS c_3d,\n",
    "            COALESCE(txn_count_7d,0)::numeric  AS c_7d,\n",
    "            COALESCE(txn_count_1m,0)::numeric  AS c_1m,\n",
    "            COALESCE(txn_count_3m,0)::numeric  AS c_3m,\n",
    "            COALESCE(txn_count_6m,0)::numeric  AS c_6m,\n",
    "            COALESCE(txn_amt_24h,0)::numeric   AS a_24h,\n",
    "            COALESCE(txn_amt_3d,0)::numeric    AS a_3d,\n",
    "            COALESCE(txn_amt_7d,0)::numeric    AS a_7d,\n",
    "            COALESCE(txn_amt_1m,0)::numeric    AS a_1m,\n",
    "            COALESCE(txn_amt_3m,0)::numeric    AS a_3m,\n",
    "            COALESCE(txn_amt_6m,0)::numeric    AS a_6m\n",
    "          FROM semantic_db.vw_risk_summary_by_severity_category_and_def_6m\n",
    "          WHERE risk_definition IS NOT NULL\n",
    "            AND risk_definition !~* '^overall$'\n",
    "            AND lower(risk_severity) = ANY(ARRAY[{sev_array_sql}])\n",
    "        ),\n",
    "        exploded AS (\n",
    "          SELECT risk_severity, risk_category, risk_definition, win, cnt, amt\n",
    "          FROM (\n",
    "            SELECT risk_severity, risk_category, risk_definition, '24h' AS win, c_24h AS cnt, a_24h AS amt FROM base\n",
    "            UNION ALL SELECT risk_severity, risk_category, risk_definition, '3d',  c_3d,  a_3d  FROM base\n",
    "            UNION ALL SELECT risk_severity, risk_category, risk_definition, '7d',  c_7d,  a_7d  FROM base\n",
    "            UNION ALL SELECT risk_severity, risk_category, risk_definition, '1m',  c_1m,  a_1m  FROM base\n",
    "            UNION ALL SELECT risk_severity, risk_category, risk_definition, '3m',  c_3m,  a_3m  FROM base\n",
    "            UNION ALL SELECT risk_severity, risk_category, risk_definition, '6m',  c_6m,  a_6m  FROM base\n",
    "          ) u\n",
    "        ),\n",
    "        agg_def AS (\n",
    "          SELECT\n",
    "            risk_severity,\n",
    "            win,\n",
    "            risk_category,\n",
    "            risk_definition,\n",
    "            SUM(cnt) AS def_cnt,\n",
    "            SUM(amt) AS def_amt\n",
    "          FROM exploded\n",
    "          GROUP BY risk_severity, win, risk_category, risk_definition\n",
    "        )\n",
    "        SELECT\n",
    "          risk_severity,\n",
    "          win,\n",
    "          SUM(def_cnt) AS severity_total_count,\n",
    "          SUM(def_amt) AS severity_total_amount,\n",
    "          SUM(def_cnt) FILTER (\n",
    "            WHERE risk_category ILIKE\n",
    "              CASE WHEN lower(risk_severity) = 'no risk' THEN 'no risk' ELSE 'procurement risk' END\n",
    "          ) AS category_count,\n",
    "          SUM(def_amt) FILTER (\n",
    "            WHERE risk_category ILIKE\n",
    "              CASE WHEN lower(risk_severity) = 'no risk' THEN 'no risk' ELSE 'procurement risk' END\n",
    "          ) AS category_amount,\n",
    "          jsonb_object_agg(risk_definition, def_cnt)\n",
    "            FILTER (WHERE risk_category ILIKE\n",
    "              CASE WHEN lower(risk_severity) = 'no risk' THEN 'no risk' ELSE 'procurement risk' END\n",
    "            ) AS category_by_definition_counts,\n",
    "          jsonb_object_agg(risk_definition, def_amt)\n",
    "            FILTER (WHERE risk_category ILIKE\n",
    "              CASE WHEN lower(risk_severity) = 'no risk' THEN 'no risk' ELSE 'procurement risk' END\n",
    "            ) AS category_by_definition_amounts\n",
    "        FROM agg_def\n",
    "        GROUP BY risk_severity, win\n",
    "        ORDER BY\n",
    "          CASE win WHEN '24h' THEN 1 WHEN '3d' THEN 2 WHEN '7d' THEN 3\n",
    "                   WHEN '1m'  THEN 4 WHEN '3m' THEN 5 WHEN '6m' THEN 6 END,\n",
    "          CASE risk_severity\n",
    "            WHEN 'no risk' THEN 1\n",
    "            WHEN 'high risk' THEN 2\n",
    "            WHEN 'needs validation' THEN 3\n",
    "            WHEN 'very high risk' THEN 4\n",
    "            ELSE 99\n",
    "          END;\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fmt_int(x: Any) -> str:\n",
    "        try:\n",
    "            v = float(x)\n",
    "            if not math.isfinite(v): \n",
    "                return \"0\"\n",
    "            return f\"{int(round(v)):,}\"\n",
    "        except:\n",
    "            return \"0\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fmt_amt(x: Any) -> str:\n",
    "        try:\n",
    "            v = float(x)\n",
    "            if not math.isfinite(v): \n",
    "                return \"0.00\"\n",
    "            return f\"{v:,.2f}\"\n",
    "        except:\n",
    "            return \"0.00\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fmt_pct(n: Any, d: Any) -> str:\n",
    "        try:\n",
    "            n = float(n)\n",
    "            d = float(d)\n",
    "            if d <= 0 or not math.isfinite(n) or not math.isfinite(d):\n",
    "                return \"0.00%\"\n",
    "            return f\"{(n/d)*100:,.2f}%\"\n",
    "        except:\n",
    "            return \"0.00%\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fmt_outof(n: Any, d: Any, kind: str = \"int\") -> str:\n",
    "        if kind == \"int\":\n",
    "            return f\"{DataProcessor.fmt_int(n)} of {DataProcessor.fmt_int(d)}\"\n",
    "        else:\n",
    "            return f\"{DataProcessor.fmt_amt(n)} of {DataProcessor.fmt_amt(d)}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def nice_sev(s: str) -> str:\n",
    "        s = (s or \"\").lower()\n",
    "        severity_map = {\n",
    "            \"high risk\": \"High Risk\",\n",
    "            \"needs validation\": \"Needs Validation\", \n",
    "            \"very high risk\": \"Very High Risk\",\n",
    "            \"no risk\": \"No Risk\"\n",
    "        }\n",
    "        return severity_map.get(s, s.title())\n",
    "    \n",
    "    @staticmethod\n",
    "    def category_for_severity(sev_label: str) -> str:\n",
    "        return \"No Risk\" if sev_label == \"No Risk\" else \"Procurement Risk\"\n",
    "    \n",
    "    def process_data(self, df: pd.DataFrame):\n",
    "        \"\"\"Process raw data into summary format\"\"\"\n",
    "        for col in [\"category_by_definition_counts\", \"category_by_definition_amounts\"]:\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: json.loads(x) if isinstance(x, str) and x else (x or {})\n",
    "            )\n",
    "        \n",
    "        numeric_cols = [\"severity_total_count\", \"severity_total_amount\", \"category_count\", \"category_amount\"]\n",
    "        for col in numeric_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "        \n",
    "        df[\"sev_label\"] = df[\"risk_severity\"].apply(self.nice_sev)\n",
    "        df.set_index([\"win\", \"sev_label\"], inplace=True)\n",
    "        \n",
    "        win_totals = df.groupby(level=0)[[\"severity_total_count\", \"severity_total_amount\"]].sum()\n",
    "        win_total_cnt = win_totals[\"severity_total_count\"].to_dict()\n",
    "        win_total_amt = win_totals[\"severity_total_amount\"].to_dict()\n",
    "        \n",
    "        summary_text_grid = self._build_summary_grid(df, win_total_cnt, win_total_amt)\n",
    "        final_summary = self._create_final_summary(summary_text_grid)\n",
    "        \n",
    "        return summary_text_grid, final_summary\n",
    "    \n",
    "    def _build_summary_grid(self, df: pd.DataFrame, win_total_cnt: Dict, win_total_amt: Dict) -> pd.DataFrame:\n",
    "        def build_text(win: str, sev: str) -> str:\n",
    "            cat = self.category_for_severity(sev)\n",
    "            \n",
    "            if (win, sev) not in df.index:\n",
    "                return (f\"[{win}] [{sev}] Total: 0 of 0 (0.00% of window, \"\n",
    "                        f\"amount 0.00 of 0.00 â€” 0.00% of window); \"\n",
    "                        f\"{cat}: 0 of 0 (0.00% of {sev}, amount 0.00 of 0.00 â€” 0.00% of {sev}). \"\n",
    "                        f\"Within {cat} â€” counts â†’ â€”; amounts â†’ â€”\")\n",
    "            \n",
    "            r = df.loc[(win, sev)]\n",
    "            total_cnt = r[\"severity_total_count\"]\n",
    "            total_amt = r[\"severity_total_amount\"]\n",
    "            cat_cnt = r[\"category_count\"]\n",
    "            cat_amt = r[\"category_amount\"]\n",
    "            w_cnt = win_total_cnt.get(win, 0)\n",
    "            w_amt = win_total_amt.get(win, 0)\n",
    "            \n",
    "            pct_sev_of_win_cnt = self.fmt_pct(total_cnt, w_cnt)\n",
    "            pct_sev_of_win_amt = self.fmt_pct(total_amt, w_amt)\n",
    "            pct_cat_of_sev_cnt = self.fmt_pct(cat_cnt, total_cnt)\n",
    "            pct_cat_of_sev_amt = self.fmt_pct(cat_amt, total_amt)\n",
    "            \n",
    "            by_def_c = r[\"category_by_definition_counts\"] or {}\n",
    "            by_def_a = r[\"category_by_definition_amounts\"] or {}\n",
    "            \n",
    "            keys = ([k for k in self.PREFERRED_DEFS if k in by_def_c] + \n",
    "                   [k for k in by_def_c if k not in self.PREFERRED_DEFS])\n",
    "            \n",
    "            if keys and cat_cnt > 0:\n",
    "                parts_c = [f\"{k}: {self.fmt_outof(by_def_c.get(k, 0), cat_cnt, 'int')} ({self.fmt_pct(by_def_c.get(k, 0), cat_cnt)})\"\n",
    "                          for k in keys]\n",
    "            else:\n",
    "                parts_c = [\"â€”\"]\n",
    "            \n",
    "            if keys and float(cat_amt) > 0:\n",
    "                parts_a = [f\"{k}: {self.fmt_outof(by_def_a.get(k, 0), cat_amt, 'amt')} ({self.fmt_pct(by_def_a.get(k, 0), cat_amt)})\"\n",
    "                          for k in keys]\n",
    "            else:\n",
    "                parts_a = [\"â€”\"]\n",
    "            \n",
    "            return (\n",
    "                f\"[{win}] [{sev}] \"\n",
    "                f\"Total: {self.fmt_outof(total_cnt, w_cnt, 'int')} ({pct_sev_of_win_cnt} of window, \"\n",
    "                f\"amount {self.fmt_outof(total_amt, w_amt, 'amt')} â€” {pct_sev_of_win_amt} of window); \"\n",
    "                f\"{cat}: {self.fmt_outof(cat_cnt, total_cnt, 'int')} ({pct_cat_of_sev_cnt} of {sev}, \"\n",
    "                f\"amount {self.fmt_outof(cat_amt, total_amt, 'amt')} â€” {pct_cat_of_sev_amt} of {sev}). \"\n",
    "                f\"Within {cat} â€” counts â†’ \" + \", \".join(parts_c) +\n",
    "                \"; amounts â†’ \" + \", \".join(parts_a)\n",
    "            )\n",
    "        \n",
    "        data = {sev: [build_text(win, sev) for win in self.WIN_ORDER] for sev in self.SEV_ORDER}\n",
    "        return pd.DataFrame(data, index=self.WIN_ORDER)\n",
    "    \n",
    "    def _create_final_summary(self, summary_text_grid: pd.DataFrame) -> pd.DataFrame:\n",
    "        def merge_window(df: pd.DataFrame, window: str, severities: list) -> str:\n",
    "            if window not in df.index:\n",
    "                return \"\"\n",
    "            cols = [c for c in severities if c in df.columns]\n",
    "            parts = []\n",
    "            for c in cols:\n",
    "                val = df.at[window, c] if c in df.columns else \"\"\n",
    "                if pd.notna(val) and str(val).strip():\n",
    "                    parts.append(str(val).strip())\n",
    "            return \"\\n\\n\".join(parts)\n",
    "        \n",
    "        all_merged = {w: merge_window(summary_text_grid, w, self.SEV_ORDER) for w in self.WIN_ORDER}\n",
    "        all_summaries_df = pd.DataFrame(\n",
    "            {\"All Summary\": [all_merged[w] for w in self.WIN_ORDER]}, \n",
    "            index=self.WIN_ORDER\n",
    "        )\n",
    "        return all_summaries_df.reset_index().rename(columns={\"index\": \"timeperiod\"})\n",
    "\n",
    "class GeminiSummarizer:\n",
    "    \"\"\"Handles AI summary generation using Gemini\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GeminiConfig):\n",
    "        self.config = config\n",
    "        genai.configure(api_key=config.api_key)\n",
    "        self.model = genai.GenerativeModel(config.model_name)\n",
    "        logger.info(f\"Gemini summarizer initialized with model: {config.model_name}\")\n",
    "    \n",
    "    def create_summary_prompt(self, data_value: str, timeperiod: str = \"6m\") -> str:\n",
    "        time_display = timeperiod.replace('m', ' months').replace('y', ' years')\n",
    "        \n",
    "        return f\"\"\"\n",
    "You are an AI financial risk analysis assistant that writes **executive-style summaries** with clear Markdown formatting suitable for Word export.\n",
    "\n",
    "EXAMPLE INPUT DATA:\n",
    "\"[6m] [High Risk] Total: 275 of 4,567 (6.02% of window, amount 602,229,561.40 of 6,816,098,374.73 â€” 8.84% of window); \n",
    "Procurement Risk: 275 of 275 (100.00% of High Risk, amount 602,229,561.40 of 602,229,561.40 â€” 100.00% of High Risk). \n",
    "Within Procurement Risk â€” counts â†’ Price Variance Risk: 219 of 275 (79.64%), Split PO: 56 of 275 (20.36%); \n",
    "amounts â†’ Price Variance Risk: 153,570,512.31 of 602,229,561.40 (25.50%), Split PO: 448,659,049.09 of 602,229,561.40 (74.50%)\"\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ TASK\n",
    "Generate a **Markdown-formatted report** in the template below, preserving all bolding, bullets, and blank lines.  \n",
    "Use dynamic risk logic:\n",
    "- Only mention risk types (Price Variance, Split PO, Duplicate Invoice, Early Payment, etc.) that are present in the input data.\n",
    "- For each detected risk, automatically create a relevant bullet in **Risk Breakdown**, **Key Insight**, and **AI Recommendation**.\n",
    "- Maintain consistent tone and structure (executive summary style).\n",
    "\n",
    "### ðŸ’  CRITICAL FORMAT RULES\n",
    "- Use **bold** for all section titles and key metrics.  \n",
    "- Use bullet points (`â€¢`) for lists and recommendations.  \n",
    "- Maintain one blank line between sections.  \n",
    "- Preserve symbols: em dash â€”, â‰ˆ, â†’, and smart quotes â€œ â€.  \n",
    "- All percentages must have **two decimal places**.  \n",
    "- Do **not** include any risk that doesnâ€™t appear in the input.\n",
    "- Display amount in INR\n",
    "---\n",
    "\n",
    "### ðŸ§© PLACEHOLDERS\n",
    "(Same placeholders as before â€” use them as available.)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  INTERPRETATION LOGIC (for {{PROC_SPEND_INTERPRETATION}})\n",
    "Use the same logic as before, based on {{PROC_RISK_SPEND_PCT}}.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ RISK-TYPE LOGIC\n",
    "Analyze input text and automatically detect which risk types are mentioned.  \n",
    "Below are **sample explanations** you may choose/adapt based on detected risk types:\n",
    "\n",
    "- **Price Variance Risk:** Indicates possible overpayments, inconsistent pricing, or weak vendor controls.  \n",
    "  â†’ Recommendation: Prioritize vendor-level and material-level pricing audits.\n",
    "\n",
    "- **Split PO Risk:** Suggests potential attempts to bypass approval thresholds or fragmented ordering behavior.  \n",
    "  â†’ Recommendation: Consolidate similar POs and investigate repeated PO creation below approval limits.\n",
    "\n",
    "- **Duplicate Invoice Risk:** Points to possible double billing or missed duplicate checks.  \n",
    "  â†’ Recommendation: Strengthen invoice validation rules and automate duplicate detection.\n",
    "\n",
    "- **Early Payment Risk:** Reflects premature payment releases, potentially breaching payment terms.  \n",
    "  â†’ Recommendation: Reinforce payment term compliance and review exception approvals.\n",
    "\n",
    "- **Blocked Vendor Risk:** Indicates procurement activities with suspended or flagged vendors.  \n",
    "  â†’ Recommendation: Review vendor master compliance and reinforce approval workflows.\n",
    "\n",
    "You may combine multiple insights if several risks coexist.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§¾ OUTPUT TEMPLATE (MUST MATCH EXACTLY)\n",
    "\n",
    "**FortifAI â€” AI Risk Summary (Last {{WINDOW_LABEL}})**\n",
    "\n",
    "**Overall Risk Landscape**\n",
    "\n",
    " â€¢ Out of **{{TOTAL_TXN}} transactions** (â‰ˆ**{{TOTAL_VALUE_ABBR}}** value), the majority (**{{NO_RISK_PCT}}%**) carried No Risk.\n",
    "\n",
    " â€¢ However, **{{FLAGGED_PCT}}%** of transactions were flagged (**{{VERY_HIGH_PCT}}%** Very High Risk, **{{HIGH_PCT}}%** High Risk) and **{{NEEDS_VALIDATION_PCT}}%** need manual validation.\n",
    "\n",
    "**Procurement Risk Hotspot**\n",
    "\n",
    " â€¢ Procurement risks form a disproportionate share:\n",
    "\n",
    "   â€¢ **{{PROC_RISK_TXN_PCT}}%** of transactions (**{{PROC_RISK_TXN_COUNT}}**) but **{{PROC_RISK_SPEND_PCT}}%** of total spend (â‰ˆ**{{PROC_RISK_VALUE_ABBR}}**).\n",
    "\n",
    " â€¢ {{PROC_SPEND_INTERPRETATION}}\n",
    "\n",
    "**Risk Breakdown**\n",
    "\n",
    "{{RISK_BREAKDOWN_DYNAMIC}}\n",
    "\n",
    "**Key Insight**\n",
    "\n",
    "{{RISK_INSIGHT_DYNAMIC}}\n",
    "\n",
    "**AI Recommendation**\n",
    "\n",
    "{{RISK_RECOMMENDATION_DYNAMIC}}\n",
    "\n",
    "---\n",
    "\n",
    "*In the last {{WINDOW_LABEL}}, FortifAIâ€™s AI engine SARAâ„¢ analyzed enterprise procurement data to uncover the above patterns.*  \n",
    "*These insights highlight where reviews should focus and support early risk mitigation across procurement operations.*\n",
    "\n",
    "---\n",
    "\n",
    "**DATA TO ANALYZE:**  \n",
    "{data_value}\n",
    "\"\"\"\n",
    "    \n",
    "    def generate_summary(self, data_value: str, timeperiod: str = \"6m\") -> Optional[str]:\n",
    "        for attempt in range(self.config.max_retries):\n",
    "            try:\n",
    "                prompt = self.create_summary_prompt(data_value, timeperiod)\n",
    "                response = self.model.generate_content(prompt)\n",
    "                \n",
    "                if response and response.text:\n",
    "                    generated_summary = response.text.strip()\n",
    "                    \n",
    "                    # Remove quotes if present\n",
    "                    if generated_summary.startswith('\"') and generated_summary.endswith('\"'):\n",
    "                        generated_summary = generated_summary[1:-1]\n",
    "                    \n",
    "                    # Extract only the SOLUTION part (remove template with placeholders)\n",
    "                    if \"SOLUTION:\" in generated_summary:\n",
    "                        # Split on \"SOLUTION:\" and take everything after it\n",
    "                        parts = generated_summary.split(\"SOLUTION:\")\n",
    "                        if len(parts) > 1:\n",
    "                            generated_summary = parts[-1].strip()  # Take the last part after SOLUTION:\n",
    "                            logger.info(\"Extracted solution part from Gemini response\")\n",
    "                    \n",
    "                    logger.info(f\"Successfully generated summary for timeperiod: {timeperiod}\")\n",
    "                    return generated_summary\n",
    "                else:\n",
    "                    logger.warning(f\"Empty response from Gemini on attempt {attempt + 1}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Gemini API error on attempt {attempt + 1}: {e}\")\n",
    "                if attempt == self.config.max_retries - 1:\n",
    "                    raise\n",
    "        \n",
    "        return None\n",
    "\n",
    "class FortifAIRiskSummarizer:\n",
    "    \"\"\"Main class orchestrating the risk summarization process\"\"\"\n",
    "    \n",
    "    def __init__(self, db_config: Optional[DatabaseConfig] = None, \n",
    "                 gemini_config: Optional[GeminiConfig] = None):\n",
    "        self.db_config = db_config or ConfigManager.get_db_config()\n",
    "        self.gemini_config = gemini_config or ConfigManager.get_gemini_config()\n",
    "        \n",
    "        self.db_manager = DatabaseManager(self.db_config)\n",
    "        self.data_processor = DataProcessor()\n",
    "        self.gemini_summarizer = GeminiSummarizer(self.gemini_config)\n",
    "        \n",
    "        logger.info(\"FortifAI Risk Summarizer initialized\")\n",
    "    \n",
    "    def fetch_risk_data(self) -> pd.DataFrame:\n",
    "        query = self.data_processor.get_risk_query(self.data_processor.SEVERITIES)\n",
    "        \n",
    "        with self.db_manager.get_connection() as conn:\n",
    "            logger.info(\"Executing risk analysis query\")\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            logger.info(f\"Retrieved {len(df)} rows from database\")\n",
    "            return df\n",
    "    \n",
    "    def process_risk_data(self):\n",
    "        raw_data = self.fetch_risk_data()\n",
    "        return self.data_processor.process_data(raw_data)\n",
    "    \n",
    "    def generate_ai_summary(self, timeperiod: str = \"6m\"):\n",
    "        try:\n",
    "            logger.info(f\"Starting AI summary generation for timeperiod: {timeperiod}\")\n",
    "            \n",
    "            _, final_summary = self.process_risk_data()\n",
    "            \n",
    "            matching_rows = final_summary[final_summary[\"timeperiod\"] == timeperiod]\n",
    "            if matching_rows.empty:\n",
    "                logger.error(f\"No data found for timeperiod: {timeperiod}\")\n",
    "                return None, None\n",
    "            \n",
    "            data_value = matching_rows[\"All Summary\"].iloc[0]\n",
    "            result = self.gemini_summarizer.generate_summary(data_value, timeperiod)\n",
    "            \n",
    "            if result:\n",
    "                logger.info(f\"Successfully generated AI summary for timeperiod: {timeperiod}\")\n",
    "                return result, data_value\n",
    "            else:\n",
    "                logger.error(f\"Failed to generate AI summary for timeperiod: {timeperiod}\")\n",
    "                return None, None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in generate_ai_summary: {e}\")\n",
    "            return None, None\n",
    "\n",
    "# Global summarizer instance\n",
    "summarizer_instance = None\n",
    "\n",
    "def get_summarizer():\n",
    "    global summarizer_instance\n",
    "    if summarizer_instance is None:\n",
    "        summarizer_instance = FortifAIRiskSummarizer()\n",
    "    return summarizer_instance\n",
    "\n",
    "# API Endpoints\n",
    "@app.get(\"/\", response_model=Dict[str, str])\n",
    "async def root():\n",
    "    \"\"\"Root endpoint with API information\"\"\"\n",
    "    return {\n",
    "        \"message\": \"FortifAI Risk Analysis API\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"docs\": \"/docs\",\n",
    "        \"health\": \"/health\"\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthResponse)\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return HealthResponse(\n",
    "        status=\"healthy\",\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        version=\"1.0.0\"\n",
    "    )\n",
    "\n",
    "@app.get(\"/api/v1/summary\", response_model=SummaryResponse)\n",
    "async def get_summary(\n",
    "    timeperiod: str = Query(\n",
    "        default=\"6m\",\n",
    "        description=\"Time period for analysis\",\n",
    "        regex=\"^(24h|3d|7d|1m|3m|6m)$\"\n",
    "    ),\n",
    "    api_key: Optional[str] = Query(None, description=\"Optional Gemini API key override\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate AI-powered risk summary for specified time period\n",
    "    \n",
    "    - **timeperiod**: Time period (24h, 3d, 7d, 1m, 3m, 6m)\n",
    "    - **api_key**: Optional Gemini API key (uses default if not provided)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        summarizer = get_summarizer()\n",
    "        \n",
    "        # Override API key if provided\n",
    "        if api_key:\n",
    "            summarizer.gemini_config.api_key = api_key\n",
    "            summarizer.gemini_summarizer = GeminiSummarizer(summarizer.gemini_config)\n",
    "        \n",
    "        summary, raw_data = summarizer.generate_ai_summary(timeperiod)\n",
    "        \n",
    "        if summary:\n",
    "            return SummaryResponse(\n",
    "                success=True,\n",
    "                timeperiod=timeperiod,\n",
    "                summary=summary,\n",
    "                raw_data=raw_data,\n",
    "                timestamp=datetime.now().isoformat()\n",
    "            )\n",
    "        else:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "                detail=\"Failed to generate summary\"\n",
    "            )\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in get_summary endpoint: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=str(e)\n",
    "        )\n",
    "\n",
    "@app.post(\"/api/v1/summary\", response_model=SummaryResponse)\n",
    "async def post_summary(request: SummaryRequest):\n",
    "    \"\"\"\n",
    "    Generate AI-powered risk summary (POST method)\n",
    "    \n",
    "    Request body:\n",
    "    - **timeperiod**: Time period (24h, 3d, 7d, 1m, 3m, 6m)\n",
    "    - **api_key**: Optional Gemini API key\n",
    "    \"\"\"\n",
    "    try:\n",
    "        summarizer = get_summarizer()\n",
    "        \n",
    "        if request.api_key:\n",
    "            summarizer.gemini_config.api_key = request.api_key\n",
    "            summarizer.gemini_summarizer = GeminiSummarizer(summarizer.gemini_config)\n",
    "        \n",
    "        summary, raw_data = summarizer.generate_ai_summary(request.timeperiod)\n",
    "        \n",
    "        if summary:\n",
    "            return SummaryResponse(\n",
    "                success=True,\n",
    "                timeperiod=request.timeperiod,\n",
    "                summary=summary,\n",
    "                raw_data=raw_data,\n",
    "                timestamp=datetime.now().isoformat()\n",
    "            )\n",
    "        else:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "                detail=\"Failed to generate summary\"\n",
    "            )\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in post_summary endpoint: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=str(e)\n",
    "        )\n",
    "\n",
    "@app.get(\"/api/v1/timeperiods\", response_model=Dict[str, list])\n",
    "async def get_timeperiods():\n",
    "    \"\"\"Get list of available time periods\"\"\"\n",
    "    return {\n",
    "        \"timeperiods\": [\"24h\", \"3d\", \"7d\", \"1m\", \"3m\", \"6m\"],\n",
    "        \"descriptions\": {\n",
    "            \"24h\": \"Last 24 hours\",\n",
    "            \"3d\": \"Last 3 days\",\n",
    "            \"7d\": \"Last 7 days\",\n",
    "            \"1m\": \"Last 1 month\",\n",
    "            \"3m\": \"Last 3 months\",\n",
    "            \"6m\": \"Last 6 months\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Exception handlers\n",
    "@app.exception_handler(HTTPException)\n",
    "async def http_exception_handler(request, exc):\n",
    "    return JSONResponse(\n",
    "        status_code=exc.status_code,\n",
    "        content=ErrorResponse(\n",
    "            error=exc.detail,\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        ).dict()\n",
    "    )\n",
    "\n",
    "@app.exception_handler(Exception)\n",
    "async def general_exception_handler(request, exc):\n",
    "    logger.error(f\"Unhandled exception: {exc}\")\n",
    "    return JSONResponse(\n",
    "        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "        content=ErrorResponse(\n",
    "            error=\"Internal server error\",\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        ).dict()\n",
    "    )\n",
    "\n",
    "# Startup and shutdown events\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    logger.info(\"FortifAI Risk Analysis API starting up...\")\n",
    "    logger.info(\"API documentation available at /docs\")\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "async def shutdown_event():\n",
    "    logger.info(\"FortifAI Risk Analysis API shutting down...\")\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    # Run with uvicorn\n",
    "#    uvicorn.run(\n",
    "#        \"main:app\",\n",
    "#        host=\"0.0.0.0\",\n",
    "#        port=8000,\n",
    "##        reload=True,\n",
    " #       log_level=\"info\"\n",
    " #   )\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    PORT = 8008\n",
    "    NGROK_AUTH_TOKEN = \"32kD6Q00UD6x6pYP59hlhlgEeyH_7idEtL4ThLT7TWuSZBALR\"  # Replace with your actual token\n",
    "\n",
    "    # # Set your ngrok token (only needed once)\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "    # Start ngrok tunnel BEFORE launching app\n",
    "    print(\"ðŸŒ Creating ngrok tunnel...\")\n",
    "    public_url = ngrok.connect(PORT)\n",
    "    print(f\"âœ… Public URL: {public_url.public_url}\")\n",
    "    print(\"ðŸ’¡ Open this URL in your browser to access the API\")\n",
    "\n",
    "    # Delay a bit before launching server (optional)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Run FastAPI app with uvicorn\n",
    "    uvicorn.run(\n",
    "        \"AI_Summary_FastAPI_Tushar:app\",\n",
    "        host=\"0.0.0.0\",\n",
    "        port=PORT,\n",
    "        reload=True,\n",
    "        log_level=\"info\"\n",
    "    )\n",
    "\n",
    "    # After app exits, clean up ngrok\n",
    "    print(\"ðŸ›‘ Shutting down ngrok...\")\n",
    "    ngrok.disconnect(public_url.public_url)\n",
    "    ngrok.kill()\n",
    "AI_Summary_FastAPI_Tushar.py\n",
    "External\n",
    "Displaying AI_Summary_FastAPI_Tushar.py."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (py312env)",
   "language": "python",
   "name": "py312env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
