{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84fa8673-1b60-4ed2-a4b4-6082619a7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import joblib\n",
    "import logging\n",
    "from typing import List\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import logging\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "# Set options to show full DataFrame output\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "def data_load_and_cleaning_po():\n",
    "    ### setting connection\n",
    "    #conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\t#database='baldota-dev-db',\n",
    "    \t\t\t#user='fortifai_ng_ai_user_rw',\n",
    "    \t\t\t#password='AIPwd@123!',\n",
    "    \t\t\t#port='5432',\n",
    "                #sslmode=\"require\"\n",
    "    \t\t#)\n",
    "    \n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_user_ro',\n",
    "    \t\t\tpassword='user@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "                SELECT table_name \n",
    "                FROM information_schema.tables \n",
    "            \"\"\")\n",
    "    tables = [row[0] for row in cur.fetchall()]\n",
    "    \n",
    "    ## table == name of table from transform_db or semantic db\n",
    "    #data= pd.read_sql_query(f\"SELECT * FROM transform_db.{table}\", conn)\n",
    "    #data= pd.read_sql_query(f\"SELECT * FROM semantic_db.{table}\", conn)\n",
    "    \n",
    "     # --- Step 4: Commit and close ---\n",
    "    #conn.commit()\n",
    "    #cur.close()\n",
    "    #conn.close()\n",
    "    \n",
    "    ## matching dictionary of client Data and data in our corresponding DB\n",
    "    line_item_keys = [\"Purch.Doc.\",\"Item\", \"Purch.Req.\",\"Item.1\",\n",
    "        \"Buyer Name\", \"Changed On\", \"CoCd\", \"Eq. To\", \"Denom.\", \"Conv.\", \"Conv..1\", \"Ct\", \"Customer\", \"D\",\n",
    "        \"GR\", \"GR Date\", \"GR-IV\", \"Gross value\", \"IR\",\"MTyp\", \"Material\", \"Matl Group\",\n",
    "        \"Net Price\", \"Net Value\", \"PO Quantity.1\", \"PO Quantity\", \"Plnt\",\n",
    "        \"Reference Document for PO Trac\", \"S\", \"Short Text\", \"Targ. Qty\", \"Target Value\", \"TrackingNo\",\n",
    "        \"EKPO-CHG_FPLNR\", \"Acknowledgment\", \"Agmt.\", \"Item.2\", \"Status of purchasing doc. item\"\n",
    "    ]\n",
    "    line_item_values = [\"purch_doc_no\", \"purch_doc_item_no\",\"pr_no\", \"pr_item_no\",\n",
    "        \"requester_name\", \"doc_change_date\", \"company_code\", \"p2o_unit_conv_denom\", \"o2b_unit_conv_denom\",\n",
    "        \"p2o_unit_conv_num\", \"o2b_unit_conv_num\", \"acct_assgnmt_category\", \"customer_no\", \"po_item_del_flag\",\n",
    "        \"gr_indicator\", \"latest_gr_dt\", \"gr_invoice_verif_flag\", \"gross_val_po_curr\", \"inv_receipt_indicator\",\n",
    "        \"material_type\", \"material_no\", \"matl_group\", \"net_price_doc_curr\",\n",
    "        \"net_val_po_curr\", \"order_uom\", \"quantity\", \"plant\", \"tpop_crm_ref_ordr_no\",\n",
    "        \"rfq_status\", \"short_text\", \"target_qty\", \"outline_agrmt_tgt_val_doc_curr\", \"reqmt_tracking_no\",\n",
    "        \"no_invoice_flag\", \"order_ack_no\", \"principal_purch_agrmt_no\", \"principal_purch_agrmt_item_no\",\n",
    "        \"purch_doc_item_status\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Step 2: Create the dictionary\n",
    "    my_dict_line_item = dict(zip(line_item_keys, line_item_values))\n",
    "    \n",
    "    # Step 3: Print the result\n",
    "    print(\"Line items key with baldota: mapped values SAP fixed\",my_dict_line_item)\n",
    "    \n",
    "    header_keys=[\"Purch.Doc.\",\"Supplier\",\n",
    "        \"C\", \"CoCd\", \"Crcy\", \"Created By\", \"Created On\", \"Ctl\", \"D\", \"Doc. Date\",\n",
    "        \"Doc.Cond.\", \"Exch. Rate\", \"PGr\", \"POrg\", \"PayT\", \"Proc.state\",\n",
    "        \"R\", \"Rel\", \"Release\", \"S\", \"Salespers.\", \"Tot. value\", \"Type\",\n",
    "        \"VP Start\", \"VPer.End\"]\n",
    "    header_values=[\"purch_doc_no\",\"vendor_or_creditor_acct_no\",\n",
    "        \"purch_doc_category\", \"company_code\", \"currency\", \"object_created_by\", \"doc_change_date\",\n",
    "        \"control_indicator\", \"po_item_del_flag\", \"purch_doc_date\", \"principal_purch_agrmt_no\",\n",
    "        \"exchange_rate\", \"purch_group\", \"purch_org\", \"pymnt_terms\", \"processing_status\", \"doc_release_incompl_flag\", \"release_indicator\", \"release_status\",\n",
    "        \"rfq_status\", \"resp_vendor_salesperson\",\n",
    "        \"on_release_total_value\", \"purch_doc_type\", \"validity_start_dt\", \"validity_end_dt\"\n",
    "    ]\n",
    "    # Step 2: Create the dictionary\n",
    "    my_header_dict = dict(zip(header_keys, header_values))\n",
    "    \n",
    "    # Step 3: Print the result\n",
    "    print(\"Header key with baldota: mapped values SAP fixed\",my_header_dict)\n",
    "    \n",
    "    \n",
    "    ##merging header data and line item data\n",
    "    \n",
    "    #tables = [row[0] for row in cur.fetchall()]\n",
    "    #p2p_line_item_po_data\n",
    "    \n",
    "    table = 'purchasing_document_item'\n",
    "    p2p_line_item_po_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{table}\", conn)\n",
    "            \n",
    "    df_line_item_po_data=p2p_line_item_po_data.copy()\n",
    "    ## dropping '4500009180^00030' at index 0 as its order_uom is 0.0 must be added for testing\n",
    "    df_line_item_po_data.drop(index=0, inplace=True)\n",
    "    \n",
    "    # convert to str to maintain\n",
    "    df_line_item_po_data['purch_doc_no'] = df_line_item_po_data['purch_doc_no'].astype(float).astype('int64').astype(str)\n",
    "    #df_line_item_po_data['pr_no'] = df_line_item_po_data['pr_no'].astype(float).astype('int64').astype(str)\n",
    "    \n",
    "    ### bring values for item number to orginal form, 10.0 to 00010 etc\n",
    "    df_line_item_po_data['purch_doc_item_no'] = df_line_item_po_data['purch_doc_item_no'].fillna(0).astype(float).astype(int).astype(str).str.zfill(5)\n",
    "    df_line_item_po_data['pr_item_no'] = df_line_item_po_data['pr_item_no'].fillna(0).astype(float).astype(int).astype(str).str.zfill(5)\n",
    "    #df_line_item_po_data['principal_purch_agrmt_item_no'] = df_line_item_po_data['principal_purch_agrmt_item_no'].fillna(0).astype(float).astype(int).astype(str).str.zfill(5)\n",
    "    \n",
    "    ## converting all column values dtype to string except for ingestion_timestamp :: will be updating dtype for values in code when those are required\n",
    "    #df_line_item_po_data.loc[:, df_line_item_po_data.columns != 'ingestion_timestamp'] = df_line_item_po_data.loc[:, df_line_item_po_data.columns != 'ingestion_timestamp'].astype(str)\n",
    "    \n",
    "    ##main data using the mapped columns from baldota\n",
    "    df_line_item=df_line_item_po_data[line_item_values]\n",
    "    ## to merge with label data later on\n",
    "    df_line_item['base_id']=df_line_item[\"purch_doc_no\"] + \"^\" + df_line_item[\"purch_doc_item_no\"]\n",
    "    # adding src to line item data columns to distinguish with header column data\n",
    "    df_line_item_renamed = df_line_item.rename(columns={col: f\"{col}_src\" for col in df_line_item.columns})\n",
    "    \n",
    "    \n",
    "    #p2p_header_po_data\n",
    "    \n",
    "    table ='purchasing_document_header'\n",
    "    p2p_header_po_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{table}\", conn)\n",
    "    df_header_po_data=p2p_header_po_data.copy()\n",
    "    \n",
    "    df_header_po_data['purch_doc_no'] = df_header_po_data['purch_doc_no'].astype(float).astype('int64').astype(str)\n",
    "    ## converting all column values dtype to string except for ingestion_timestamp :: will be updating dtype for values in code when those are required\n",
    "    #df_header_po_data.loc[:, df_header_po_data.columns != 'ingestion_timestamp'] = df_header_po_data.loc[:, df_header_po_data.columns != 'ingestion_timestamp'].astype(str)\n",
    "    ##main data using the mapped columns from baldota\n",
    "    df_header=df_header_po_data[header_values]\n",
    "    # adding hpd to header data columns to distinguish with line_item column data\n",
    "    df_header_renamed = df_header.rename(columns={col: f\"{col}_hpd\" for col in df_header.columns})\n",
    "    \n",
    "    merged_df_before_label=pd.merge(df_line_item_renamed,df_header_renamed,left_on='purch_doc_no_src',right_on='purch_doc_no_hpd',how='outer')\n",
    "    merged_df_before_label.shape\n",
    "    \n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    ## rule label output\n",
    "    #line_item=pd.read_excel(\"baldota rule label data/Line Item Transaction Summary.xlsx\")\n",
    "    line_item=pd.read_csv(\"Line Item Transaction Summary 24_07.csv\")\n",
    "    ## taking output po label at line item\n",
    "    po_line_item_label=line_item[line_item['stage']=='PO']\n",
    "    \n",
    "    ## merging main data df with label data df\n",
    "    merged_df_after_label=pd.merge(merged_df_before_label,po_line_item_label[['base_id','rule_ids','rft_by_engine']],left_on='base_id_src',right_on='base_id',how=\"outer\")\n",
    "    \n",
    "    ### adding false to rft_by_engine where rft by engine in null nan\n",
    "    df=merged_df_after_label.copy()\n",
    "    null_count = df['rft_by_engine'].isna().sum()\n",
    "    none_str_count = (df['rft_by_engine'] == 'None').sum()\n",
    "    \n",
    "    total_missing = null_count + none_str_count\n",
    "    print(f\"Total missing values in 'rft_by_engine': {total_missing}\")\n",
    "    \n",
    "    # Step 2: Replace NaN and 'None' with False\n",
    "    df['rft_by_engine'] = df['rft_by_engine'].replace('None', pd.NA)\n",
    "    df['rft_by_engine'] = df['rft_by_engine'].fillna(False)\n",
    "    \n",
    "    #df = df.drop(columns=['id', 'run_id',\n",
    "           #'config_id', 'transaction_id', 'base_id', 'header_id', 'header_base_id',\n",
    "           #'is_data_quality_approved', 'transaction_date', 'transaction_value',\n",
    "           #'stage', 'location', 'department', 'vendor_code', 'employee_code',\n",
    "           #'rule_ids', 'risk_score', 'price', 'people', 'process','flag_by_audit'])\n",
    "    \n",
    "    # Convert dates\n",
    "    df[\"doc_change_date_src\"] = pd.to_datetime(df[\"doc_change_date_src\"], errors='coerce')\n",
    "    df[\"doc_change_date_hpd\"] = pd.to_datetime(df[\"doc_change_date_hpd\"], errors='coerce')\n",
    "    df[\"purch_doc_date_hpd\"] = pd.to_datetime(df[\"purch_doc_date_hpd\"], errors='coerce')\n",
    "    \n",
    "    # drop all rows where all values are Nan\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    #df.info()\n",
    "    #df['purch_doc_mapping']=df[\"purch_doc_no_src\"] + \"^\" + df[\"purch_doc_item_no_src\"]\n",
    "    df_final = df.rename(columns={col: f\"{col}_po\" for col in df.columns})\n",
    "    \n",
    "    # Step 1: Get unique rule IDs from all rows (comma-separated strings)\n",
    "    rule_sets = df_final['rule_ids_po'].dropna().apply(lambda x: [r.strip() for r in x.split(',')])\n",
    "    unique_rules = sorted(set(r for sublist in rule_sets for r in sublist))\n",
    "    \n",
    "    # Step 2: Create columns for each rule ID with 1 if present, else 0\n",
    "    for rule in unique_rules:\n",
    "        df_final[rule] = df_final['rule_ids_po'].apply(lambda x: int(rule in x.split(',')) if pd.notna(x) else 0)\n",
    "    #df_final.info()\n",
    "    ## data for model training\n",
    "    #df_final.to_pickle(\"po_output.pkl\")\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a08ac-3e00-4647-8428-f77a77735c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (py312env)",
   "language": "python",
   "name": "py312env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
