{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8d79e6-22b8-4967-b817-e0f3a2ba08f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Loading cached features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayn\\anaconda3\\envs\\py312env\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['central_purch_blk_flag']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Avg Loss: 6.7847\n",
      "Epoch 001 | Avg Loss: 4.4520\n",
      "Epoch 002 | Avg Loss: 0.1210\n",
      "Epoch 003 | Avg Loss: 0.0204\n",
      "Epoch 004 | Avg Loss: 0.0190\n",
      "Epoch 005 | Avg Loss: 0.0187\n",
      "Epoch 006 | Avg Loss: 0.0185\n",
      "Epoch 007 | Avg Loss: 0.0183\n",
      "Epoch 008 | Avg Loss: 0.0182\n",
      "Epoch 009 | Avg Loss: 0.0181\n",
      "Epoch 010 | Avg Loss: 0.0180\n",
      "Epoch 011 | Avg Loss: 0.0179\n",
      "Epoch 012 | Avg Loss: 0.0178\n",
      "Epoch 013 | Avg Loss: 0.0178\n",
      "Epoch 014 | Avg Loss: 0.0178\n",
      "Epoch 015 | Avg Loss: 0.0177\n",
      "Epoch 016 | Avg Loss: 0.0177\n",
      "Epoch 017 | Avg Loss: 0.0177\n",
      "Epoch 018 | Avg Loss: 0.0176\n",
      "Epoch 019 | Avg Loss: 0.0176\n",
      "Epoch 020 | Avg Loss: 0.0176\n",
      "Epoch 021 | Avg Loss: 0.0176\n",
      "Epoch 022 | Avg Loss: 0.0175\n",
      "Epoch 023 | Avg Loss: 0.0175\n",
      "Epoch 024 | Avg Loss: 0.0175\n",
      "Epoch 025 | Avg Loss: 0.0175\n",
      "Epoch 026 | Avg Loss: 0.0175\n",
      "Epoch 027 | Avg Loss: 0.0174\n",
      "Epoch 028 | Avg Loss: 0.0174\n",
      "Epoch 029 | Avg Loss: 0.0174\n",
      "‚èπÔ∏è Early stopping triggered.\n",
      "\n",
      "üö® Reconstruction Error Threshold: 0.0186\n",
      "\n",
      "üìä Classification Report (Autoencoder):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91     10906\n",
      "           1       0.76      0.77      0.76      4101\n",
      "\n",
      "    accuracy                           0.87     15007\n",
      "   macro avg       0.84      0.84      0.84     15007\n",
      "weighted avg       0.87      0.87      0.87     15007\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Baseline model saved\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "##############################################################################\n",
    "#  Cell 1 ‚Äî Imports & common paths\n",
    "##############################################################################\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import joblib\n",
    "import logging\n",
    "from typing import List\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import logging\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set options to show full DataFrame output\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import joblib, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from typing import Any, Dict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import sys, os\n",
    "\n",
    "# Ensure stdout uses UTF-8\n",
    "try:\n",
    "    sys.stdout.reconfigure(encoding=\"utf-8\")\n",
    "except Exception:\n",
    "    os.environ[\"PYTHONIOENCODING\"] = \"utf-8\"\n",
    "\n",
    "RAW_PO      = Path(\"po_data_02_10.pkl\") #(\"../Po_Invoice_Data/po_output_tushar.pkl\")      # raw PO data (input)\n",
    "FEAT_PO     = Path(\"Po_Invoice_Data/po_output_features_df_auto_model_02_10.pkl\")         # engineered features\n",
    "MODEL_PKL   = Path(\"Po_Invoice_Data/po_autoencoder_model_02_10.pkl\")         # tuned model file\n",
    "#CV_REPORT   = Path(\"Po_Invoice_Data/cv_results.csv\")            # param grid results\n",
    "#SCORING_OUT = Path(\"Po_Invoice_Data/scored_po.pkl\")             # predictions file\n",
    "\n",
    "\n",
    "def data_load_and_cleaning_po():\n",
    "        ### setting connection\n",
    "    #conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\t#database='baldota-dev-db',\n",
    "    \t\t\t#user='fortifai_ng_ai_user_rw',\n",
    "    \t\t\t#password='AIPwd@123!',\n",
    "    \t\t\t#port='5432',\n",
    "                #sslmode=\"require\"\n",
    "    \t\t#)\n",
    "    \n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_user_ro',\n",
    "    \t\t\tpassword='user@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "                SELECT table_name \n",
    "                FROM information_schema.tables \n",
    "            \"\"\")\n",
    "    tables = [row[0] for row in cur.fetchall()]\n",
    "    \n",
    "     # --- Step 4: Commit and close ---\n",
    "    #conn.commit()\n",
    "    #cur.close()\n",
    "    #conn.close()\n",
    "    \n",
    "    line_item_keys = [\"Purch.Doc.\",\"Item\", \"Purch.Req.\",\"Item.1\",\n",
    "        \"Buyer Name\", \"Changed On\", \"CoCd\", \"Eq. To\", \"Denom.\", \"Conv.\", \"Conv..1\", \"Ct\", \"Customer\", \"D\",\n",
    "        \"GR\", \"GR Date\", \"GR-IV\", \"Gross value\", \"IR\",\"MTyp\", \"Material\", \"Matl Group\",\n",
    "        \"Net Price\", \"Net Value\", \"PO Quantity.1\", \"PO Quantity\", \"Plnt\",\n",
    "        \"Reference Document for PO Trac\", \"S\", \"Short Text\", \"Targ. Qty\", \"Target Value\", \"TrackingNo\",\n",
    "        \"EKPO-CHG_FPLNR\", \"Acknowledgment\", \"Agmt.\", \"Item.2\", \"Status of purchasing doc. item\"\n",
    "    ]\n",
    "    line_item_values = [\"purch_doc_no\", \"purch_doc_item_no\",\"pr_no\", \"pr_item_no\",\n",
    "        \"requester_name\", \"doc_change_date\", \"company_code\", \"p2o_unit_conv_denom\", \"o2b_unit_conv_denom\",\n",
    "        \"p2o_unit_conv_num\", \"o2b_unit_conv_num\", \"acct_assgnmt_category\", \"customer_no\", \"po_item_del_flag\",\n",
    "        \"gr_indicator\", \"latest_gr_dt\", \"gr_invoice_verif_flag\", \"gross_val_po_curr\", \"inv_receipt_indicator\",\n",
    "        \"material_type\", \"material_no\", \"matl_group\", \"net_price_doc_curr\",\n",
    "        \"net_val_po_curr\", \"order_uom\", \"quantity\", \"plant\", \"tpop_crm_ref_ordr_no\",\n",
    "        \"rfq_status\", \"short_text\", \"target_qty\", \"outline_agrmt_tgt_val_doc_curr\", \"reqmt_tracking_no\",\n",
    "        \"no_invoice_flag\", \"order_ack_no\", \"principal_purch_agrmt_no\", \"principal_purch_agrmt_item_no\",\n",
    "        \"purch_doc_item_status\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Step 2: Create the dictionary\n",
    "    my_dict_line_item = dict(zip(line_item_keys, line_item_values))\n",
    "    \n",
    "    # Step 3: Print the result\n",
    "    print(\"Line items key with baldota: mapped values SAP fixed\",my_dict_line_item)\n",
    "    \n",
    "    header_keys=[\"Purch.Doc.\",\"Supplier\",\n",
    "        \"C\", \"CoCd\", \"Crcy\", \"Created By\", \"Created On\", \"Ctl\", \"D\", \"Doc. Date\",\n",
    "        \"Doc.Cond.\", \"Exch. Rate\", \"PGr\", \"POrg\", \"PayT\", \"Proc.state\",\n",
    "        \"R\", \"Rel\", \"Release\", \"S\", \"Salespers.\", \"Tot. value\", \"Type\",\n",
    "        \"VP Start\", \"VPer.End\"]\n",
    "    header_values=[\"purch_doc_no\",\"vendor_or_creditor_acct_no\",\n",
    "        \"purch_doc_category\", \"company_code\", \"currency\", \"object_created_by\", \"doc_change_date\",\n",
    "        \"control_indicator\", \"po_item_del_flag\", \"purch_doc_date\", \"principal_purch_agrmt_no\",\n",
    "        \"exchange_rate\", \"purch_group\", \"purch_org\", \"pymnt_terms\", \"processing_status\", \"doc_release_incompl_flag\", \"release_indicator\", \"release_status\",\n",
    "        \"rfq_status\", \"resp_vendor_salesperson\",\n",
    "        \"on_release_total_value\", \"purch_doc_type\", \"validity_start_dt\", \"validity_end_dt\"\n",
    "    ]\n",
    "    # Step 2: Create the dictionary\n",
    "    my_header_dict = dict(zip(header_keys, header_values))\n",
    "    \n",
    "    # Step 3: Print the result\n",
    "    print(\"Header key with baldota: mapped values SAP fixed\",my_header_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #tables = [row[0] for row in cur.fetchall()]\n",
    "    #p2p_line_item_po_data\n",
    "    for table in tables:\n",
    "        if table == 'purchasing_document_item':\n",
    "            p2p_line_item_po_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{table}\", conn)\n",
    "            \n",
    "    df_line_item_po_data=p2p_line_item_po_data.copy()\n",
    "    ## dropping '4500009180^00030' at index 0 as its order_uom is 0.0 must be added for testing\n",
    "    df_line_item_po_data.drop(index=0, inplace=True)\n",
    "    \n",
    "    # convert to str to maintain\n",
    "    df_line_item_po_data['purch_doc_no'] = df_line_item_po_data['purch_doc_no'].astype(float).astype('int64').astype(str)\n",
    "    #df_line_item_po_data['pr_no'] = df_line_item_po_data['pr_no'].astype(float).astype('int64').astype(str)\n",
    "    \n",
    "    ### bring values for item number to orginal form, 10.0 to 00010 etc\n",
    "    df_line_item_po_data['purch_doc_item_no'] = df_line_item_po_data['purch_doc_item_no'].fillna(0).astype(float).astype(int).astype(str).str.zfill(5)\n",
    "    df_line_item_po_data['pr_item_no'] = df_line_item_po_data['pr_item_no'].fillna(0).astype(float).astype(int).astype(str).str.zfill(5)\n",
    "    #df_line_item_po_data['principal_purch_agrmt_item_no'] = df_line_item_po_data['principal_purch_agrmt_item_no'].fillna(0).astype(float).astype(int).astype(str).str.zfill(5)\n",
    "    \n",
    "    ## converting all column values dtype to string except for ingestion_timestamp :: will be updating dtype for values in code when those are required\n",
    "    #df_line_item_po_data.loc[:, df_line_item_po_data.columns != 'ingestion_timestamp'] = df_line_item_po_data.loc[:, df_line_item_po_data.columns != 'ingestion_timestamp'].astype(str)\n",
    "    \n",
    "    ##main data using the mapped columns from baldota\n",
    "    df_line_item=df_line_item_po_data[line_item_values]\n",
    "    ## to merge with label data later on\n",
    "    df_line_item['base_id']=df_line_item[\"purch_doc_no\"] + \"^\" + df_line_item[\"purch_doc_item_no\"]\n",
    "    # adding src to line item data columns to distinguish with header column data\n",
    "    df_line_item_renamed = df_line_item.rename(columns={col: f\"{col}_src\" for col in df_line_item.columns})\n",
    "    \n",
    "    \n",
    "    #p2p_header_po_data\n",
    "    for table in tables:\n",
    "        if table == 'purchasing_document_header':\n",
    "            p2p_header_po_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{table}\", conn)\n",
    "    df_header_po_data=p2p_header_po_data.copy()\n",
    "    \n",
    "    df_header_po_data['purch_doc_no'] = df_header_po_data['purch_doc_no'].astype(float).astype('int64').astype(str)\n",
    "    ## converting all column values dtype to string except for ingestion_timestamp :: will be updating dtype for values in code when those are required\n",
    "    #df_header_po_data.loc[:, df_header_po_data.columns != 'ingestion_timestamp'] = df_header_po_data.loc[:, df_header_po_data.columns != 'ingestion_timestamp'].astype(str)\n",
    "    ##main data using the mapped columns from baldota\n",
    "    df_header=df_header_po_data[header_values]\n",
    "    # adding hpd to header data columns to distinguish with line_item column data\n",
    "    df_header_renamed = df_header.rename(columns={col: f\"{col}_hpd\" for col in df_header.columns})\n",
    "    \n",
    "    merged_df_before_label=pd.merge(df_line_item_renamed,df_header_renamed,left_on='purch_doc_no_src',right_on='purch_doc_no_hpd',how='outer')\n",
    "    merged_df_before_label.shape\n",
    "    \n",
    "    \n",
    "    df=merged_df_before_label.copy()\n",
    "    # Convert dates\n",
    "    df[\"doc_change_date_src\"] = pd.to_datetime(df[\"doc_change_date_src\"], errors='coerce')\n",
    "    df[\"doc_change_date_hpd\"] = pd.to_datetime(df[\"doc_change_date_hpd\"], errors='coerce')\n",
    "    df[\"purch_doc_date_hpd\"] = pd.to_datetime(df[\"purch_doc_date_hpd\"], errors='coerce')\n",
    "    \n",
    "    # drop all rows where all values are Nan\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    #df.info()\n",
    "    #df['purch_doc_mapping']=df[\"purch_doc_no_src\"] + \"^\" + df[\"purch_doc_item_no_src\"]\n",
    "    df_final_po= df.rename(columns={col: f\"{col}_po\" for col in df.columns})\n",
    "    df_final_po.info()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    # there are 3 po that have data in header but not in line item\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Invoice Data ####\n",
    "    \n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_user_ro',\n",
    "    \t\t\tpassword='user@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "                SELECT table_name \n",
    "                FROM information_schema.tables \n",
    "            \"\"\")\n",
    "    tables = [row[0] for row in cur.fetchall()]\n",
    "    \n",
    "    \n",
    "    line_item_keys = [\"DocumentNo\",\"InvItem\",\"Purch.Doc.\",\"Item\", \"Amount\", \"BlR\", \"Central Contract\", \"Central Contract Item\", \"CoCd\", \"D/C\", \"FIn\",\n",
    "        \"GR/IR Clrg\", \"Indicator for Differential Invoicing\", \"Material\", \"Plnt\",\n",
    "         \"OUn\", \"Quantity\", \"OPUn\", \"Qty in OPUn\", \"Reference\", \"SAA\", \"Supplier\",\n",
    "        \"Tax Jur.\", \"Year\", \"Year.1\"\n",
    "    ]\n",
    "    line_item_values = [\"accounting_doc_no\",\"doc_line_item_no\",\"purch_doc_no\", \"purch_doc_item_no\",\"amt_doc_curr\", \"block_reason_field\", \"central_contract\",\n",
    "                        \"central_contract_item_no\",\"company_code\", \"debit_credit_flag\", \"final_inv_flag\",\n",
    "        \"ext_gr_ir_clrg_flag\", \"diff_invoicing_flag\", \n",
    "        \"material_no\", \"plant\", \"order_uom\", \"quantity\", \"po_uom\",\n",
    "        \"po_qty_order_uom\", \"ref_doc_no\", \"acct_assgnmt_seq_no\", \"vendor_or_creditor_acct_no\",\n",
    "        \"tax_jurisdiction_code\", \"fiscal_year\", \"ref_doc_fiscal_year\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Step 2: Create the dictionary\n",
    "    my_dict_line_item = dict(zip(line_item_keys, line_item_values))\n",
    "    \n",
    "    # Step 3: Print the result\n",
    "    print(\"Line items key with baldota: mapped values SAP fixed\",my_dict_line_item)\n",
    "    \n",
    "    header_keys= [\"Doc. No.\",\n",
    "        \"Bline Date\", \"CoCd\", \"Crcy\", \"Del.Costs\", \"Doc. Date\",  \"Doc.Header Text\",\n",
    "        \"Entry Dte\", \"Exch.rate\", \"G/L\", \"Gross Amnt\", \"I\", \"IV cat\", \"InR.Ref.no\", \"Inv. Pty\",\n",
    "        \"PBk\", \"PM\", \"PayT\", \"Payer\", \"Paymt Ref.\", \"Reference\", \"Rel.\", \"Rvrsd by\",\n",
    "        \"St\", \"TCode\", \"Time\", \"Type\", \"User Name\"\n",
    "    ]\n",
    "    header_values=[\"accounting_doc_no\",\"baseline_date\", \"company_code\", \"currency\", \"unplanned_dlvry_costs\", \"doc_date\",\n",
    "         \"doc_header_text\", \"doc_entry_date\", \"exchange_rate\", \"gl_account\",\n",
    "        \"gross_inv_amt_doc_curr\", \"post_inv_flag\", \"logistics_inv_verif_orig_type\", \"txn_invoice_no\",\n",
    "        \"vendor_or_creditor_acct_no\", \"house_bank_short_key\", \"pymnt_method\", \"pymnt_terms\",\n",
    "        \"payee_or_payer_name\", \"assignment_no\", \"ref_doc_no\", \"sap_release\", \"reversal_doc_no\",\n",
    "        \"invoice_doc_status\", \"txn_code\", \"entry_time\", \"doc_type\", \"username\"\n",
    "    ]\n",
    "    # Step 2: Create the dictionary\n",
    "    my_header_dict = dict(zip(header_keys, header_values))\n",
    "    \n",
    "    # Step 3: Print the result\n",
    "    print(\"Header key with baldota: mapped values SAP fixed\",my_header_dict)\n",
    "    \n",
    "    \n",
    "    #tables = [row[0] for row in cur.fetchall()]\n",
    "    #p2p_line_item_invoice_data\n",
    "    for table in tables:\n",
    "        if table == 'invoice_receipt_items':\n",
    "            p2p_line_item_invoice_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{table}\", conn)\n",
    "            \n",
    "    df_line_item_invoice_data=p2p_line_item_invoice_data.copy()\n",
    "    # there are na values in accounting_doc_no--> dropping\n",
    "    df_line_item_invoice_data= df_line_item_invoice_data.dropna(subset=['accounting_doc_no'])\n",
    "    \n",
    "    # convert to str to maintain\n",
    "    df_line_item_invoice_data['accounting_doc_no'] = df_line_item_invoice_data['accounting_doc_no'].astype(float).astype('int64').astype(str)\n",
    "    df_line_item_invoice_data['purch_doc_no'] = df_line_item_invoice_data['purch_doc_no'].astype(float).astype('int64').astype(str)\n",
    "    \n",
    "    ### bring values for item number to orginal form, 10.0 to 00010 etc\n",
    "    df_line_item_invoice_data['purch_doc_item_no'] = df_line_item_invoice_data['purch_doc_item_no'].fillna(0).astype(float).astype(int).astype(str).str.zfill(5)\n",
    "    # convert doc_line_item_no to str\n",
    "    df_line_item_invoice_data['doc_line_item_no'] = df_line_item_invoice_data['doc_line_item_no'].fillna(0).astype(float).astype(int).astype(str)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##main data using the mapped columns from baldota\n",
    "    df_line_item=df_line_item_invoice_data[line_item_values]\n",
    "    ## to merge with label data later on\n",
    "    df_line_item['base_id']=df_line_item[\"accounting_doc_no\"] + \"^\" + df_line_item[\"doc_line_item_no\"]\n",
    "    # adding src to line item data columns to distinguish with header column data\n",
    "    df_line_item_renamed = df_line_item.rename(columns={col: f\"{col}_src\" for col in df_line_item.columns})\n",
    "    \n",
    "    \n",
    "    #p2p_header_invoice_data\n",
    "    for table in tables:\n",
    "        if table == 'invoice_receipt_header':\n",
    "            p2p_header_invoice_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{table}\", conn)\n",
    "    df_header_invoice_data=p2p_header_invoice_data.copy()\n",
    "    \n",
    "    # there are na values in accounting_doc_no--> dropping\n",
    "    df_header_invoice_data= df_header_invoice_data.dropna(subset=['accounting_doc_no'])\n",
    "    \n",
    "    # str maintain\n",
    "    df_header_invoice_data['accounting_doc_no'] = df_header_invoice_data['accounting_doc_no'].astype(float).astype('int64').astype(str)\n",
    "    ## converting all column values dtype to string except for ingestion_timestamp :: will be updating dtype for values in code when those are required\n",
    "    #df_header_po_data.loc[:, df_header_po_data.columns != 'ingestion_timestamp'] = df_header_po_data.loc[:, df_header_po_data.columns != 'ingestion_timestamp'].astype(str)\n",
    "    ##main data using the mapped columns from baldota\n",
    "    df_header=df_header_invoice_data[header_values]\n",
    "    # adding hpd to header data columns to distinguish with line_item column data\n",
    "    df_header_renamed = df_header.rename(columns={col: f\"{col}_hpd\" for col in df_header.columns})\n",
    "    \n",
    "    merged_df_before_label=pd.merge(df_line_item_renamed,df_header_renamed,left_on='accounting_doc_no_src',right_on='accounting_doc_no_hpd',how='outer')\n",
    "    merged_df_before_label.shape\n",
    "    \n",
    "    df=merged_df_before_label.copy()\n",
    "    # Convert dates\n",
    "    df[\"baseline_date_hpd\"] = pd.to_datetime(df[\"baseline_date_hpd\"], errors='coerce')\n",
    "    df[\"doc_date_hpd\"] = pd.to_datetime(df[\"doc_date_hpd\"], errors='coerce')\n",
    "    df[\"doc_entry_date_hpd\"] = pd.to_datetime(df[\"doc_entry_date_hpd\"], errors='coerce')\n",
    "    \n",
    "    # drop all rows where all values are Nan\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    #df.info()\n",
    "    df['purch_doc_mapping']=df[\"purch_doc_no_src\"] + \"^\" + df[\"purch_doc_item_no_src\"]\n",
    "    df_final_invoice = df.rename(columns={col: f\"{col}_invoice\" for col in df.columns})\n",
    "    df_final_invoice.info()\n",
    "    \n",
    "    ## there are 175 invoice that have data in header but not in line item\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    \n",
    "    po_invoice=pd.merge(df_final_po,df_final_invoice, left_on='base_id_src_po',right_on='purch_doc_mapping_invoice',how='outer')\n",
    "    \n",
    "    ## cleaning .0 from vendor\n",
    "    s = po_invoice['vendor_or_creditor_acct_no_hpd_po'].astype(str).str.strip()           # ensure string & tidy spaces\n",
    "    mask = s.str.upper().eq('UNKNOWN')                 # rows to leave as-is\n",
    "    po_invoice['vendor_or_creditor_acct_no_hpd_po'] = s.where(mask, s.str.replace(r'\\.0$', '', regex=True))\n",
    "    \n",
    "    ## getting vendor name from vendor master DB\n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_user_ro',\n",
    "    \t\t\tpassword='user@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "                SELECT table_name \n",
    "                FROM information_schema.tables \n",
    "            \"\"\")\n",
    "    tables = [row[0] for row in cur.fetchall()]\n",
    "    vendor_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{'vendor_master_general_section'}\", conn)\n",
    "    lfa_1=pd.read_excel('LFA1 - Vendor Master.xlsx')\n",
    "    lfa_1[\"Supplier\"] = lfa_1[\"Supplier\"].astype(str)\n",
    "    lfa_1_1=lfa_1[['Supplier','DelF','DeBl','B','B.1','Status']]\n",
    "    lfa_1_2 = lfa_1_1.rename(columns={\n",
    "        \"Supplier\": \"vendor_or_creditor_acct_no\",\n",
    "        'DelF':\"central_deletion_flag\",\n",
    "        \"DeBl\": \"central_del_block_flg\",\n",
    "        \"B\": \"central_posting_blk_flag\",\n",
    "        \"B.1\": \"central_purch_blk_flag\",\n",
    "        \"Status\": \"data_transfer_status\",\n",
    "    })\n",
    "    \n",
    "    KEY = \"vendor_or_creditor_acct_no\"\n",
    "    cols_to_update = [\"central_deletion_flag\",\n",
    "        \"central_del_block_flg\",\n",
    "        \"central_posting_blk_flag\",\n",
    "        \"central_purch_blk_flag\",\n",
    "        \"data_transfer_status\",\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    vendor= vendor_data.drop(columns=cols_to_update)\n",
    "    \n",
    "    \n",
    "    # Merge and overwrite columns\n",
    "    merged = vendor.merge(\n",
    "        lfa_1_2,\n",
    "        on=KEY,\n",
    "        how=\"outer\")\n",
    "    vendor_data_updated = merged\n",
    "    # drop all rows where all values are Nan\n",
    "    vendor_data_1 = vendor_data_updated.dropna(axis=1, how='all')\n",
    "    po_invoice_vendor=pd.merge(po_invoice,vendor_data_1,left_on='vendor_or_creditor_acct_no_hpd_po',right_on='vendor_or_creditor_acct_no',how='outer')\n",
    "    \n",
    "    # ---------- 0) Invoice reversed flag ----------\n",
    "    po_invoice_vendor[\"invoice_reversed\"] = np.where(\n",
    "        po_invoice_vendor[\"reversal_doc_no_hpd_invoice\"].isnull(), 0, 1\n",
    "    )\n",
    "    \n",
    "    # ---------- 1) Make sure inputs are numeric ----------\n",
    "    for col in [\"amt_doc_curr_src_invoice\", \"quantity_src_invoice\",\n",
    "                \"net_val_po_curr_src_po\", \"quantity_src_po\"]:\n",
    "        po_invoice_vendor[col] = pd.to_numeric(po_invoice_vendor[col], errors=\"coerce\")\n",
    "    \n",
    "    # ---------- 2) Build the gate ----------\n",
    "    if \"reversal_doc_no_hpd_invoice\" in po_invoice_vendor.columns:\n",
    "        inv_rev_ok = po_invoice_vendor[\"reversal_doc_no_hpd_invoice\"].isna()\n",
    "    elif \"invoice_reversed\" in po_invoice_vendor.columns:\n",
    "        inv_rev_ok = po_invoice_vendor[\"invoice_reversed\"].fillna(0).eq(0)\n",
    "    else:\n",
    "        inv_rev_ok = True  # broadcasts\n",
    "    \n",
    "    gate = (\n",
    "        (po_invoice_vendor[\"release_indicator_hpd_po\"] == \"R\") &\n",
    "        (po_invoice_vendor[\"po_item_del_flag_src_po\"].isna()) &\n",
    "        inv_rev_ok\n",
    "    )\n",
    "    \n",
    "    KEY = \"purch_doc_mapping_invoice\"\n",
    "    \n",
    "    # ---------- 3) Work only on gated rows ----------\n",
    "    cols_needed = [KEY, \"amt_doc_curr_src_invoice\", \"quantity_src_invoice\",\n",
    "                   \"net_val_po_curr_src_po\", \"quantity_src_po\"]\n",
    "    gated = po_invoice_vendor.loc[gate, cols_needed].copy()\n",
    "    \n",
    "    # Group totals ONLY from gated rows\n",
    "    gated[\"invoice_total_amount\"]   = gated.groupby(KEY)[\"amt_doc_curr_src_invoice\"].transform(\"sum\")\n",
    "    gated[\"invoice_total_quantity\"] = gated.groupby(KEY)[\"quantity_src_invoice\"].transform(\"sum\")\n",
    "    \n",
    "    # Pick PO references within gated rows (use max to be conservative)\n",
    "    gated[\"po_value_ref\"] = gated.groupby(KEY)[\"net_val_po_curr_src_po\"].transform(\"max\")\n",
    "    gated[\"po_qty_ref\"]   = gated.groupby(KEY)[\"quantity_src_po\"].transform(\"max\")\n",
    "    \n",
    "    # ---------- 4) Kill float noise and compare with tolerance ----------\n",
    "    AMT_ATOL = 0.01   # ‚Çπ0.01 tolerance\n",
    "    QTY_ATOL = 1e-9   # effectively exact for integers\n",
    "    \n",
    "    # Round values before comparing (money: 2dp; qty: 6dp)\n",
    "    gated[\"invoice_total_amount\"]   = gated[\"invoice_total_amount\"].round(2)\n",
    "    gated[\"po_value_ref\"]           = gated[\"po_value_ref\"].round(2)\n",
    "    gated[\"invoice_total_quantity\"] = gated[\"invoice_total_quantity\"].round(6)\n",
    "    gated[\"po_qty_ref\"]             = gated[\"po_qty_ref\"].round(6)\n",
    "    \n",
    "    amt_breach = gated[\"invoice_total_amount\"]   > (gated[\"po_value_ref\"] + AMT_ATOL)\n",
    "    qty_breach = gated[\"invoice_total_quantity\"] > (gated[\"po_qty_ref\"]   + QTY_ATOL)\n",
    "    \n",
    "    gated[\"invoice_more_than_po_flag\"] = (amt_breach | qty_breach).astype(int)\n",
    "    \n",
    "    # If any row in a gated group breaches ‚Üí mark ALL gated rows in that group\n",
    "    gated[\"invoice_more_than_po_flag\"] = gated.groupby(KEY)[\"invoice_more_than_po_flag\"].transform(\"max\")\n",
    "    \n",
    "    # ---------- 5) Map results back; non-gated rows remain 0 ----------\n",
    "    po_invoice_vendor[\"invoice_total_amount\"]          = 0.0\n",
    "    po_invoice_vendor[\"invoice_total_quantity\"]        = 0.0\n",
    "    po_invoice_vendor[\"invoice_more_than_po_flag\"]     = 0\n",
    "    \n",
    "    po_invoice_vendor.loc[gated.index, \"invoice_total_amount\"]   = gated[\"invoice_total_amount\"].values\n",
    "    po_invoice_vendor.loc[gated.index, \"invoice_total_quantity\"] = gated[\"invoice_total_quantity\"].values\n",
    "    po_invoice_vendor.loc[gated.index, \"invoice_more_than_po_flag\"] = gated[\"invoice_more_than_po_flag\"].values\n",
    "    \n",
    "    # (Optional) ensure the flag is int, not float\n",
    "    #po_invoice_vendor[\"invoice_more_than_po_flag\"] = po_invoice_vendor[\"invoice_more_than_po_flag\"].astype(int)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    label_data=pd.read_excel(\"Rulewise summary - 6 rules.xlsx\")\n",
    "    one=pd.merge(po_invoice_vendor,label_data,left_on='base_id_src_po',right_on='base_id',how='left')\n",
    "    \n",
    "    \n",
    "    \n",
    "    one_1=one[['purch_doc_no_src_po', 'purch_doc_item_no_src_po', 'pr_no_src_po',\n",
    "           'pr_item_no_src_po', 'requester_name_src_po', 'doc_change_date_src_po',\n",
    "           'company_code_src_po', 'p2o_unit_conv_denom_src_po',\n",
    "           'o2b_unit_conv_denom_src_po', 'p2o_unit_conv_num_src_po',\n",
    "           'o2b_unit_conv_num_src_po', 'po_item_del_flag_src_po',\n",
    "           'gr_indicator_src_po', 'gr_invoice_verif_flag_src_po',\n",
    "           'gross_val_po_curr_src_po', 'inv_receipt_indicator_src_po',\n",
    "           'material_type_src_po', 'material_no_src_po', 'matl_group_src_po',\n",
    "           'net_price_doc_curr_src_po', 'net_val_po_curr_src_po',\n",
    "           'order_uom_src_po', 'quantity_src_po', 'plant_src_po',\n",
    "           'short_text_src_po', 'target_qty_src_po',\n",
    "           'outline_agrmt_tgt_val_doc_curr_src_po', 'reqmt_tracking_no_src_po',\n",
    "           'principal_purch_agrmt_item_no_src_po', 'base_id_src_po',\n",
    "           'purch_doc_no_hpd_po', 'vendor_or_creditor_acct_no_hpd_po',\n",
    "           'purch_doc_category_hpd_po', 'company_code_hpd_po', 'currency_hpd_po',\n",
    "           'object_created_by_hpd_po', 'doc_change_date_hpd_po',\n",
    "           'control_indicator_hpd_po', 'purch_doc_date_hpd_po',\n",
    "           'principal_purch_agrmt_no_hpd_po', 'exchange_rate_hpd_po',\n",
    "           'purch_group_hpd_po', 'purch_org_hpd_po', 'pymnt_terms_hpd_po',\n",
    "           'processing_status_hpd_po', 'doc_release_incompl_flag_hpd_po',\n",
    "           'release_indicator_hpd_po', 'release_status_hpd_po',\n",
    "           'rfq_status_hpd_po', 'resp_vendor_salesperson_hpd_po',\n",
    "           'on_release_total_value_hpd_po', 'purch_doc_type_hpd_po','vendor_or_creditor_acct_no', 'country_code', 'vendor_name_1',\n",
    "           'vendor_name_2', 'vendor_name_3', 'vendor_name_4', 'city',\n",
    "           'postal_code', 'street_address', 'item_manual_addr_no',\n",
    "           'matchcode_search_term_1', 'record_creation_dt', 'object_created_by',\n",
    "           'vendor_acct_group',\n",
    "           'tax_no_1', 'vendor_telephone_no', 'second_telephone_no', 'tax_no_3',\n",
    "           'tax_no_5','central_deletion_flag', 'central_purch_blk_flag','central_posting_blk_flag','data_transfer_status',\n",
    "               \"invoice_more_than_po_flag\"]]\n",
    "    \n",
    "    #one_1.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #rule_2\n",
    "    # ensure the flag is numeric 0/1 (or numeric)\n",
    "    one_1[\"invoice_more_than_po_flag\"] = (\n",
    "        pd.to_numeric(one_1[\"invoice_more_than_po_flag\"], errors=\"coerce\")\n",
    "          .fillna(0).astype(int)\n",
    "    )\n",
    "    \n",
    "    # keep rows where the flag equals the group's max; drop the rest\n",
    "    grp_max = one_1.groupby(\"base_id_src_po\")[\"invoice_more_than_po_flag\"].transform(\"max\")\n",
    "    one_1 = one_1.loc[\n",
    "        one_1[\"invoice_more_than_po_flag\"].eq(grp_max)\n",
    "    ].copy()\n",
    "    one_1 = (one_1\n",
    "             .sort_values([\"base_id_src_po\", \"invoice_more_than_po_flag\"], ascending=[True, False])\n",
    "             .drop_duplicates(\"base_id_src_po\", keep=\"first\"))\n",
    "    \n",
    "    \n",
    "    # rule label output\n",
    "    #line_item=pd.read_excel(\"baldota rule label data/Line Item Transaction Summary.xlsx\")\n",
    "    rft=pd.read_csv(r\"C:\\Users\\ajayn\\Downloads\\FortifAI_NEW\\Codes\\New Codes\\Model pipeline\\08-07-25\\13-07-2025\\ML Codes\\baldota rule label data\\Line Item Transaction Summary 24_07.csv\")\n",
    "    ## taking output po label at line item\n",
    "    rft_po=rft[rft['stage']=='PO']\n",
    "    one_2=one_1[one_1['base_id_src_po'].notna()].copy()\n",
    "    ## merging main data df with label data df\n",
    "    one_3=pd.merge(one_2,rft_po[['base_id','rule_ids','rft_by_engine']],left_on='base_id_src_po',right_on='base_id',how=\"left\")\n",
    "    #one_3.head()\n",
    "    ## rule_3\n",
    "    block_cols = [\"central_deletion_flag\", \"central_purch_blk_flag\", \"central_posting_blk_flag\"]\n",
    "    \n",
    "    one_3[\"po_to_blocked_vendor\"] = np.where(\n",
    "        (one_3[block_cols].eq(\"X\").any(axis=1)) &\n",
    "        (one_3[\"release_indicator_hpd_po\"] == \"R\") &\n",
    "        (one_3[\"po_item_del_flag_src_po\"].isna()),\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ## rule_4\n",
    "    # Ensure both columns are datetime\n",
    "    one_3[\"purch_doc_date_hpd_po\"] = pd.to_datetime(one_3[\"purch_doc_date_hpd_po\"], errors=\"coerce\")\n",
    "    one_3[\"record_creation_dt\"] = pd.to_datetime(one_3[\"record_creation_dt\"], errors=\"coerce\")\n",
    "    \n",
    "    # Create flag: 1 if purch_doc_date_hpd_po is within 30 days of record_creation_dt\n",
    "    one_3[\"purch_doc_within_30days\"] = (\n",
    "        (one_3[\"purch_doc_date_hpd_po\"] - one_3[\"record_creation_dt\"]).abs().dt.days <= 30\n",
    "    ).astype(int)\n",
    "    \n",
    "    \n",
    "    # Ensure numeric types\n",
    "    one_3[\"on_release_total_value_hpd_po\"] = pd.to_numeric(one_3[\"on_release_total_value_hpd_po\"], errors=\"coerce\")\n",
    "    one_3[\"exchange_rate_hpd_po\"] = pd.to_numeric(one_3[\"exchange_rate_hpd_po\"], errors=\"coerce\")\n",
    "    \n",
    "    # Calculate PO value in INR (or base currency)\n",
    "    one_3[\"po_value_inr\"] = one_3[\"on_release_total_value_hpd_po\"] * one_3[\"exchange_rate_hpd_po\"]\n",
    "    \n",
    "    # Create flag: 1 if > 1 crore (1 Cr = 10,000,000)\n",
    "    one_3[\"po_gt_1cr_flag\"] = np.where(one_3[\"po_value_inr\"] > 1e7, 1, 0)\n",
    "    \n",
    "    one_3[\"po_to_new_vendor_gt_tolerance\"] = np.where(\n",
    "        (one_3[\"purch_doc_within_30days\"] == 1) & (one_3[\"po_gt_1cr_flag\"] == 1) & (one_3[\"release_indicator_hpd_po\"] == 'R')\n",
    "        & (one_3[\"po_item_del_flag_src_po\"].isna()),\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    ## rule_5\n",
    "    df = one_3.copy() # or your dataframe name\n",
    "    \n",
    "    # 1) Extract PAN: remove first 2 chars and last 3 chars, clean up casing/whitespace\n",
    "    df[\"pan_extracted\"] = (\n",
    "        df[\"tax_no_3\"]\n",
    "          .astype(str)\n",
    "          .str.strip()\n",
    "          .str.upper()\n",
    "          .str[2:-3]                 # remove first 2 and last 3\n",
    "    )\n",
    "    \n",
    "    # (Optional) Keep only plausible PANs (10 chars, alphanumeric)\n",
    "    pan_valid = df[\"pan_extracted\"].str.len().eq(10) & df[\"pan_extracted\"].str.isalnum()\n",
    "    df.loc[~pan_valid, \"pan_extracted\"] = np.nan\n",
    "    \n",
    "    # 2) Amount in INR (or base) and >= 1 Cr flag\n",
    "    df[\"on_release_total_value_hpd_po\"] = pd.to_numeric(df[\"on_release_total_value_hpd_po\"], errors=\"coerce\")\n",
    "    df[\"exchange_rate_hpd_po\"] = pd.to_numeric(df[\"exchange_rate_hpd_po\"], errors=\"coerce\")\n",
    "    df[\"po_value_inr\"] = df[\"on_release_total_value_hpd_po\"] * df[\"exchange_rate_hpd_po\"]\n",
    "    \n",
    "    amount_ge_1cr = df[\"po_value_inr\"] >= 1e7  # 1 crore = 10,000,000\n",
    "    \n",
    "    # 3) 4th PAN character == 'P'\n",
    "    fourth_char_is_P = df[\"pan_extracted\"].str[3].eq(\"P\")\n",
    "    \n",
    "    # 4) Final flag: 4th PAN char 'P' AND amount ‚â• 1 Cr  -> 1 else 0\n",
    "    df[\"pan4P_amt_ge_1cr_flag\"] = np.where(fourth_char_is_P & amount_ge_1cr &  (df[\"release_indicator_hpd_po\"] == 'R')\n",
    "        & (df[\"po_item_del_flag_src_po\"].isna()), 1, 0)\n",
    "    \n",
    "    ##rule_6\n",
    "    df[\"missing_tax_id_flag\"] = np.where(df[\"tax_no_3\"].isna() &  (df[\"release_indicator_hpd_po\"] == 'R')\n",
    "        & (df[\"po_item_del_flag_src_po\"].isna()), 1, 0)\n",
    "    \n",
    "    \n",
    "    one_3=df.copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1) Flag: True if either column is true\n",
    "    # List your 5 columns\n",
    "    cols_to_check = [\"invoice_more_than_po_flag\", \"po_to_blocked_vendor\", \"po_to_new_vendor_gt_tolerance\", \"pan4P_amt_ge_1cr_flag\",\"missing_tax_id_flag\"]\n",
    "    \n",
    "    # output_flag will be True if any column == 1, else False\n",
    "    one_3[\"new_model_output\"] = df[cols_to_check].eq(1).any(axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # List the columns to check\n",
    "    cols_to_check = [\"invoice_more_than_po_flag\", \"po_to_blocked_vendor\", \"po_to_new_vendor_gt_tolerance\", \"pan4P_amt_ge_1cr_flag\",\"missing_tax_id_flag\"]\n",
    "    \n",
    "    # Build rule_summary by joining column names where value == 1\n",
    "    one_3[\"new_rule_summary\"] = (\n",
    "        df[cols_to_check]\n",
    "        .apply(lambda row: \",\".join(row.index[row.eq(1)]), axis=1)\n",
    "    )\n",
    "    \n",
    "    # If you prefer NaN instead of empty string when no rules matched:\n",
    "    # df[\"rule_summary\"] = df[\"rule_summary\"].replace(\"\", np.nan)\n",
    "    #one_3.head()\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        to_bool\n",
    "    except NameError:\n",
    "        TRUEY = {\"true\",\"t\",\"yes\",\"y\",\"1\",\"ok\",\"on\"}\n",
    "        def to_bool(x):\n",
    "            if pd.isna(x):\n",
    "                return False\n",
    "            if isinstance(x, (bool, np.bool_)):\n",
    "                return bool(x)\n",
    "            if isinstance(x, (int, np.integer, float, np.floating)):\n",
    "                # treat 1 or True-like as True\n",
    "                return x == 1 or x is True\n",
    "            return str(x).strip().lower() in TRUEY\n",
    "    \n",
    "    try:\n",
    "        split_rules\n",
    "    except NameError:\n",
    "        def split_rules(val):\n",
    "            if pd.isna(val):\n",
    "                return []\n",
    "            if isinstance(val, (list, tuple, set)):\n",
    "                parts = list(val)\n",
    "            else:\n",
    "                # accept commas/semicolons\n",
    "                parts = [p.strip() for chunk in str(val).split(';') for p in chunk.split(',')]\n",
    "            parts = [p for p in parts if p and p.lower() != \"nan\"]\n",
    "            # de-dupe preserve order\n",
    "            seen, out = set(), []\n",
    "            for p in parts:\n",
    "                if p not in seen:\n",
    "                    seen.add(p); out.append(p)\n",
    "            return out\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1) Flag: True if either column is true\n",
    "    one_3[\"rft_by_engine_7\"] = (\n",
    "        one_3[\"rft_by_engine\"].map(to_bool) | one_3[\"new_model_output\"].map(to_bool)\n",
    "    )\n",
    "    \n",
    "    # (Optional) whitelist to avoid free-text becoming a \"rule\"\n",
    "    ALLOWED_RULES = None  # e.g., {\"invoice_before_po_flag\", \"invoice_more_than_po_flag\"}\n",
    "    \n",
    "    def merge_rules_guarded(row):\n",
    "        # Build candidate rules from both columns\n",
    "        parts = split_rules(row.get(\"new_rule_summary\")) + split_rules(row.get(\"rule_ids\"))\n",
    "        parts = list(dict.fromkeys(parts))  # de-dupe, keep order\n",
    "        if ALLOWED_RULES is not None:\n",
    "            parts = [p for p in parts if p in ALLOWED_RULES]\n",
    "    \n",
    "        # ENFORCE: if flag is False -> no rules\n",
    "        if not to_bool(row.get(\"rft_by_engine_7\", False)):\n",
    "            return []\n",
    "    \n",
    "        return parts\n",
    "    \n",
    "    # Build final columns with gating\n",
    "    one_3[\"rule_ids_7_list\"] = one_3.apply(merge_rules_guarded, axis=1)\n",
    "    one_3[\"rule_ids_7\"] = one_3[\"rule_ids_7_list\"].apply(lambda lst: \",\".join(lst) if lst else np.nan)\n",
    "    \n",
    "    \n",
    "    # Step 1: Get unique rule IDs from all rows (comma-separated strings)\n",
    "    rule_sets = one_3['rule_ids_7'].dropna().apply(lambda x: [r.strip() for r in x.split(',')])\n",
    "    unique_rules = sorted(set(r for sublist in rule_sets for r in sublist))\n",
    "    \n",
    "    # Step 2: Create columns for each rule ID with 1 if present, else 0\n",
    "    for rule in unique_rules:\n",
    "        one_3[rule] = one_3['rule_ids_7'].apply(lambda x: int(rule in x.split(',')) if pd.notna(x) else 0)\n",
    "    \n",
    "    #Invoice before PO date\n",
    "    #Invoice Exceeds PO\n",
    "    # Final rule to sub-risk mapping\n",
    "    def get_sub_risks(row):\n",
    "        rule_to_subrisk = {\"P2P02067\": \"Price Variance Risk\",\n",
    "            \"P2P02068\": \"Price Variance Risk\",\n",
    "            \"P2P02070\": \"Split PO\",\n",
    "            \"P2P02072\": \"Split PO\",\n",
    "            \"invoice_more_than_po_flag\": \"Invoice Exceeds PO\",\n",
    "            \"po_to_blocked_vendor\":\"PO to block vendor\",\n",
    "            \"po_to_new_vendor_gt_tolerance\":\"PO to new vendor > tolerance level\",\n",
    "            \"pan4P_amt_ge_1cr_flag\":\"PO to Non Company Vendors\",\n",
    "            \"missing_tax_id_flag\":\"PO to Vendor with missing KYC\",\n",
    "                           \n",
    "                           \n",
    "            #rule_1\": \"Invoice before PO date\",\n",
    "        }\n",
    "        risks = {rule_to_subrisk[rule] for rule in rule_to_subrisk if row.get(rule, 0) == 1}\n",
    "        return list(risks) if risks else [\"No Risk\"]\n",
    "    def sub_risk(df):    \n",
    "        \n",
    "        # Step 1: Assign Main Risk Scenario\n",
    "        df[\"main_risk_scenario\"] = df[\"rft_by_engine_7\"].apply(\n",
    "            lambda x: \"Procurement Risk\" if x else \"No Risk\"\n",
    "        )\n",
    "        \n",
    "        # Step 2: Generate clean list of sub risks\n",
    "        #def get_sub_risks(row):\n",
    "            #risks = {rule_to_subrisk[rule] for rule in rule_to_subrisk if row.get(rule, 0) == 1}\n",
    "            #return list(risks) if risks else [\"No Risk\"]\n",
    "        \n",
    "        df[\"sub_risks\"] = df.apply(get_sub_risks, axis=1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    invoice_data_2=sub_risk(one_3)\n",
    "    #invoice_data_2.head()\n",
    "    \n",
    "    \n",
    "    \n",
    "    po_data_2=invoice_data_2[['purch_doc_no_src_po', 'purch_doc_item_no_src_po', 'pr_no_src_po',\n",
    "           'pr_item_no_src_po', 'requester_name_src_po', 'doc_change_date_src_po',\n",
    "           'company_code_src_po', 'p2o_unit_conv_denom_src_po',\n",
    "           'o2b_unit_conv_denom_src_po', 'p2o_unit_conv_num_src_po',\n",
    "           'o2b_unit_conv_num_src_po', 'po_item_del_flag_src_po',\n",
    "           'gr_indicator_src_po', 'gr_invoice_verif_flag_src_po',\n",
    "           'gross_val_po_curr_src_po', 'inv_receipt_indicator_src_po',\n",
    "           'material_type_src_po', 'material_no_src_po', 'matl_group_src_po',\n",
    "           'net_price_doc_curr_src_po', 'net_val_po_curr_src_po',\n",
    "           'order_uom_src_po', 'quantity_src_po', 'plant_src_po',\n",
    "           'short_text_src_po', 'target_qty_src_po',\n",
    "           'outline_agrmt_tgt_val_doc_curr_src_po', 'reqmt_tracking_no_src_po',\n",
    "           'principal_purch_agrmt_item_no_src_po', 'base_id_src_po',\n",
    "           'purch_doc_no_hpd_po', 'vendor_or_creditor_acct_no_hpd_po',\n",
    "           'purch_doc_category_hpd_po', 'company_code_hpd_po', 'currency_hpd_po',\n",
    "           'object_created_by_hpd_po', 'doc_change_date_hpd_po',\n",
    "           'control_indicator_hpd_po', 'purch_doc_date_hpd_po',\n",
    "           'principal_purch_agrmt_no_hpd_po', 'exchange_rate_hpd_po',\n",
    "           'purch_group_hpd_po', 'purch_org_hpd_po', 'pymnt_terms_hpd_po',\n",
    "           'processing_status_hpd_po', 'doc_release_incompl_flag_hpd_po',\n",
    "           'release_indicator_hpd_po', 'release_status_hpd_po',\n",
    "           'rfq_status_hpd_po', 'resp_vendor_salesperson_hpd_po',\n",
    "                                  'on_release_total_value_hpd_po', 'purch_doc_type_hpd_po',\n",
    "           'vendor_or_creditor_acct_no', 'country_code', 'vendor_name_1',\n",
    "           'vendor_name_2', 'vendor_name_3', 'vendor_name_4', 'city',\n",
    "           'postal_code', 'street_address', 'item_manual_addr_no',\n",
    "           'matchcode_search_term_1', 'record_creation_dt', 'object_created_by',\n",
    "           'vendor_acct_group', 'tax_no_1', 'vendor_telephone_no',\n",
    "           'second_telephone_no', 'tax_no_3', 'tax_no_5', 'central_deletion_flag',\n",
    "           'central_purch_blk_flag', 'central_posting_blk_flag',\n",
    "           'data_transfer_status', 'invoice_more_than_po_flag', \n",
    "           'po_to_blocked_vendor', 'purch_doc_within_30days', 'po_value_inr',\n",
    "           'po_gt_1cr_flag', 'po_to_new_vendor_gt_tolerance', 'pan_extracted',\n",
    "           'pan4P_amt_ge_1cr_flag', 'missing_tax_id_flag','rft_by_engine_7','main_risk_scenario', 'sub_risks']]\n",
    "    \n",
    "    #po_data_2.head()po_invoice\n",
    "    #po_data_2.to_pickle('po_data_02_10.pkl')\n",
    "    return po_data_2\n",
    "\n",
    "\n",
    "COL_MAP: Dict[str, str] = {\n",
    "    # raw_column                          # internal name\n",
    "    \"vendor_or_creditor_acct_no_hpd_po\": \"vendor_id\",\n",
    "    \"material_no_src_po\": \"material_id\",\n",
    "    \"purch_doc_date_hpd_po\": \"po_date\",\n",
    "    \"doc_change_date_src_po\": \"po_change_date\",\n",
    "    \"net_price_doc_curr_src_po\": \"net_price\",\n",
    "    \"gross_val_po_curr_src_po\": \"gross_val\",\n",
    "    \"exchange_rate_hpd_po\": \"exch_rate\",\n",
    "    \"requester_name_src_po\": \"requester\",\n",
    "    # unit‚Äëconversion numerators / denominators\n",
    "    \"p2o_unit_conv_num_src_po\": \"p2o_num\",\n",
    "    \"p2o_unit_conv_denom_src_po\": \"p2o_den\",\n",
    "    \"o2b_unit_conv_num_src_po\": \"o2b_num\",\n",
    "    \"o2b_unit_conv_denom_src_po\": \"o2b_den\",\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Feature engineering functions (pure, chainable)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _prep(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Rename key columns, parse dates & fill obvious nulls.\"\"\"\n",
    "    #df = df.rename(columns={k: v for k, v in COL_MAP.items() if k in df.columns})\n",
    "    df[\"purch_doc_date_hpd_po\"] = pd.to_datetime(df[\"purch_doc_date_hpd_po\"], errors=\"coerce\")\n",
    "    df[\"doc_change_date_src_po\"] = pd.to_datetime(df.get(\"doc_change_date_src_po\"), errors=\"coerce\")\n",
    "    df[\"vendor_or_creditor_acct_no_hpd_po\"] = df.get(\"vendor_or_creditor_acct_no_hpd_po\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"requester_name_src_po\"] = df.get(\"requester_name_src_po\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"exchange_rate_hpd_po\"] = df.get(\"exchange_rate_hpd_po\", 1.0).replace({0: np.nan}).fillna(1.0)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2a) Rule‚Äëbased features (rules¬†1,2,3,5)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def add_rule_metrics(df: pd.DataFrame,\n",
    "                     split_days: int = 60,\n",
    "                     price_var_days: int = 365) -> pd.DataFrame:\n",
    "    \"\"\"Add features mirroring Baldota P2P rules.\n",
    "\n",
    "    * vm_count/value = aggregation for **same vendor+material** within *split_days*\n",
    "    * vm_price_var_pct = price deviation (%) vs mean of past *price_var_days*\n",
    "    * mat_vendor_cnt & mat_price_var_pct analogues for material across vendors\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.sort_values(\"purch_doc_date_hpd_po\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Initialise\n",
    "    df[\"vm_count_%dd\" % split_days] = 0\n",
    "    df[\"vm_value_%dd\" % split_days] = 0.0\n",
    "    df[\"vm_price_var_pct_%dd\" % price_var_days] = 0.0\n",
    "    df[\"mat_vendor_cnt_%dd\" % split_days] = 0\n",
    "    df[\"mat_price_var_pct_%dd\" % price_var_days] = 0.0\n",
    "\n",
    "    # Pre‚Äëextract convenient arrays for speed\n",
    "    po_dates = df[\"purch_doc_date_hpd_po\"].values\n",
    "    prices = df.get(\"net_price_doc_curr_src_po\").astype(float).values\n",
    "    vals = df.get(\"gross_val_po_curr_src_po\").astype(float).values\n",
    "\n",
    "    # --- Same vendor + material group logic ----------------------------------\n",
    "    for (v, m), idx in df.groupby([\"vendor_or_creditor_acct_no_hpd_po\", \"material_no_src_po\"]).groups.items():\n",
    "        d = po_dates[idx]\n",
    "        p = prices[idx]\n",
    "        vls = vals[idx]\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = d[loc]\n",
    "            # split window\n",
    "            win_mask = (d >= cur - np.timedelta64(split_days, \"D\")) & (d <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(\"vm_count_%dd\" % split_days)] = int(win_mask.sum())\n",
    "            df.iat[ridx, df.columns.get_loc(\"vm_value_%dd\" % split_days)] = float(vls[win_mask].sum())\n",
    "            # price variance window\n",
    "            var_mask = (d >= cur - np.timedelta64(price_var_days, \"D\")) & (d <= cur)\n",
    "            if var_mask.sum() > 1:\n",
    "                mean_price = p[var_mask].mean()\n",
    "                if mean_price:\n",
    "                    pct = abs(p[loc] - mean_price) / mean_price * 100\n",
    "                    df.iat[ridx, df.columns.get_loc(\"vm_price_var_pct_%dd\" % price_var_days)] = pct\n",
    "\n",
    "    # --- Material‚Äëonly group logic -------------------------------------------\n",
    "    for m, idx in df.groupby(\"material_no_src_po\").groups.items():\n",
    "        d = po_dates[idx]\n",
    "        p = prices[idx]\n",
    "        vendors = df.loc[idx, \"vendor_or_creditor_acct_no_hpd_po\"].values\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = d[loc]\n",
    "            win_mask = (d >= cur - np.timedelta64(split_days, \"D\")) & (d <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(\"mat_vendor_cnt_%dd\" % split_days)] = int(len(set(vendors[win_mask])))\n",
    "            var_mask = (d >= cur - np.timedelta64(price_var_days, \"D\")) & (d <= cur)\n",
    "            if var_mask.sum() > 1:\n",
    "                mean_price = p[var_mask].mean()\n",
    "                if mean_price:\n",
    "                    pct = abs(p[loc] - mean_price) / mean_price * 100\n",
    "                    df.iat[ridx, df.columns.get_loc(\"mat_price_var_pct_%dd\" % price_var_days)] = pct\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2b) Value & process metrics\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def add_value_and_timing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"conv_factor_p2o\"] = (\n",
    "        df.get(\"p2o_unit_conv_num_src_po\") / df.get(\"p2o_unit_conv_denom_src_po\")\n",
    "    ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "    df[\"conv_factor_o2b\"] = (\n",
    "        df.get(\"o2b_unit_conv_num_src_po\") / df.get(\"o2b_unit_conv_denom_src_po\")\n",
    "    ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "    df[\"po_change_lag_days\"] = (df.get(\"doc_change_date_src_po\") - df[\"purch_doc_date_hpd_po\"]).dt.days\n",
    "    df[\"base_value\"] = df.get(\"gross_val_po_curr_src_po\") * df.get(\"exchange_rate_hpd_po\")\n",
    "    p95 = df[\"gross_val_po_curr_src_po\"].quantile(0.95)\n",
    "    df[\"high_value_flag\"] = (df[\"gross_val_po_curr_src_po\"] >= p95).astype(int)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2c) Behavioural rolling windows\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _rolling_stats(df: pd.DataFrame, group_col: str, days: int,\n",
    "                   count_col: str, sum_col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[count_col] = 0\n",
    "    df[sum_col] = 0.0\n",
    "\n",
    "    po_dates = df[\"purch_doc_date_hpd_po\"].values\n",
    "    gross_vals = df[\"gross_val_po_curr_src_po\"].values\n",
    "\n",
    "    for key, idx in df.groupby(group_col).groups.items():\n",
    "        dates = po_dates[idx]\n",
    "        vals = gross_vals[idx]\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = dates[loc]\n",
    "            mask = (dates >= cur - np.timedelta64(days, \"D\")) & (dates <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(count_col)] = int(mask.sum())\n",
    "            df.iat[ridx, df.columns.get_loc(sum_col)] = float(vals[mask].sum())\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_behavioural_stats(df: pd.DataFrame, window_days: int = 30) -> pd.DataFrame:\n",
    "    df = _rolling_stats(df, \"requester_name_src_po\", window_days,\n",
    "                        \"req_po_count_%dd\" % window_days,\n",
    "                        \"req_val_sum_%dd\" % window_days)\n",
    "    df = _rolling_stats(df, \"vendor_or_creditor_acct_no_hpd_po\", window_days,\n",
    "                        \"vendor_po_count_%dd\" % window_days,\n",
    "                        \"vendor_val_sum_%dd\" % window_days)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Public orchestrator\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def build_features(raw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"End‚Äëto‚Äëend feature generator (no target leakage).\"\"\"\n",
    "    df = _prep(raw_df)\n",
    "    df = add_rule_metrics(df)\n",
    "    df = add_value_and_timing(df)\n",
    "    df = add_behavioural_stats(df)\n",
    "    return df\n",
    "def flag_split_po(df):\n",
    "    df = df.copy()\n",
    "    df['split_po_flag'] = 0  # Default 0\n",
    "\n",
    "    # Apply exclusion filters only for computation\n",
    "    exclude_doc_types = [\"AN\", \"AR\", \"MN\", \"QC\", \"QI\", \"QS\", \"RS\", \"SC\", \"SG\", \"SR\", \"SS\", \"ST\", \"TP\", \"TR\", \"UB\", \"WK\"]\n",
    "\n",
    "    filtered = df[\n",
    "        (~df['purch_doc_type_hpd_po'].isin(exclude_doc_types)) &\n",
    "        (df['purch_doc_type_hpd_po'].notna()) &\n",
    "        (~(df['po_item_del_flag_src_po'] == 'L')) &\n",
    "        (~df['plant_src_po'].fillna(\"\").astype(str).str.startswith(\"4\")) &\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['gross_val_po_curr_src_po'] >= 10) &\n",
    "        (df['purch_doc_date_hpd_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['company_code_src_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    filtered['purch_doc_date_hpd_po'] = pd.to_datetime(filtered['purch_doc_date_hpd_po'])\n",
    "\n",
    "    def get_group_key(row):\n",
    "        if pd.notna(row['material_no_src_po']):\n",
    "            return f\"{row['vendor_or_creditor_acct_no_hpd_po']}__{row['material_no_src_po']}\"\n",
    "        elif pd.notna(row['short_text_src_po']):\n",
    "            return f\"{row['vendor_or_creditor_acct_no_hpd_po']}__{row['short_text_src_po']}\"\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    filtered['split_key'] = filtered.apply(get_group_key, axis=1)\n",
    "    filtered = filtered[filtered['split_key'].notna()].copy()\n",
    "    filtered.sort_values(['split_key', 'purch_doc_date_hpd_po'], inplace=True)\n",
    "\n",
    "    flagged_po_set = set()\n",
    "\n",
    "    for key, group in filtered.groupby('split_key'):\n",
    "        dates = group['purch_doc_date_hpd_po'].reset_index(drop=True)\n",
    "        po_nos = group['purch_doc_no_src_po'].reset_index(drop=True)\n",
    "\n",
    "        for i in range(len(dates)):\n",
    "            date_i = dates[i]\n",
    "            po_i = po_nos[i]\n",
    "            mask = (\n",
    "                (dates >= date_i - pd.Timedelta(days=14)) &\n",
    "                (dates <= date_i + pd.Timedelta(days=14)) &\n",
    "                (po_nos != po_i)\n",
    "            )\n",
    "            if mask.sum() > 0:\n",
    "                flagged_po_set.add(po_i)\n",
    "\n",
    "    # Assign flag only to matching rows in original df\n",
    "    df['split_po_flag'] = df['purch_doc_no_src_po'].isin(flagged_po_set).astype(int)\n",
    "    return df\n",
    "def flag_intra_po_split(df, gross_threshold=10):\n",
    "    df = df.copy()\n",
    "    df['intra_po_split_flag'] = 0  # Default\n",
    "\n",
    "    df_valid = df[\n",
    "        df['gross_val_po_curr_src_po'].notna() &\n",
    "        df['vendor_or_creditor_acct_no_hpd_po'].notna() &\n",
    "        df['material_no_src_po'].notna() & \n",
    "        df['purch_doc_no_src_po'].notna() \n",
    "    \n",
    "    ].copy()\n",
    "\n",
    "    group_cols = ['purch_doc_no_src_po', 'vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po']\n",
    "    grouped = df_valid.groupby(group_cols)\n",
    "\n",
    "    flagged_indexes = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        total_gross = group['gross_val_po_curr_src_po'].sum()\n",
    "        num_items = len(group)\n",
    "        all_below_threshold = group['gross_val_po_curr_src_po'].all()\n",
    "\n",
    "        if total_gross >= gross_threshold and num_items > 1 and all_below_threshold:\n",
    "            flagged_indexes.extend(group.index.tolist())\n",
    "\n",
    "    df.loc[flagged_indexes, 'intra_po_split_flag'] = 1\n",
    "    return df\n",
    "def flag_multiple_pos_per_pr_item(df):\n",
    "    df = df.copy()\n",
    "    df['multi_po_per_pr_flag'] = 0  # Default\n",
    "\n",
    "    # Only consider approved POs with valid PR and PR item\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        df['pr_no_src_po'].notna() &\n",
    "        df['pr_item_no_src_po'].notna() &\n",
    "        df['purch_doc_no_src_po'].notna()\n",
    "    ][['pr_no_src_po', 'pr_item_no_src_po', 'purch_doc_no_src_po']].drop_duplicates()\n",
    "\n",
    "    # Count number of unique POs per PR+Item\n",
    "    po_counts = df_valid.groupby(['pr_no_src_po', 'pr_item_no_src_po'])['purch_doc_no_src_po'].nunique()\n",
    "\n",
    "    # Identify PR+Items linked to more than one PO\n",
    "    multi_po_keys = po_counts[po_counts > 1].index.tolist()\n",
    "\n",
    "    # Create a set for fast lookup\n",
    "    multi_po_set = set(multi_po_keys)\n",
    "\n",
    "    # Flag in the main DataFrame\n",
    "    df['multi_po_per_pr_flag'] = df.apply(\n",
    "        lambda row: 1 if (row['pr_no_src_po'], row['pr_item_no_src_po']) in multi_po_set else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "def flag_same_vendor_price_increase(df, price_increase_threshold=0.05, months_range=6, flag_column='same_vendor_price_increase_flag'):\n",
    "    df = df.copy()\n",
    "    df[flag_column] = 0\n",
    "\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['material_no_src_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['net_price_doc_curr_src_po'].notna()) &\n",
    "        (df['purch_doc_date_hpd_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    df_valid['purch_doc_date_hpd_po'] = pd.to_datetime(df_valid['purch_doc_date_hpd_po'])\n",
    "    df_valid.sort_values(['vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po', 'purch_doc_date_hpd_po'], inplace=True)\n",
    "\n",
    "    group_cols = ['vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po']\n",
    "    flagged_indices = []\n",
    "\n",
    "    for _, group in df_valid.groupby(group_cols):\n",
    "        group = group.sort_values('purch_doc_date_hpd_po').reset_index()\n",
    "\n",
    "        for i in range(1, len(group)):\n",
    "            current_row = group.loc[i]\n",
    "            current_date = current_row['purch_doc_date_hpd_po']\n",
    "            current_price = current_row['net_price_doc_curr_src_po']\n",
    "\n",
    "            mask = group.loc[:i-1, 'purch_doc_date_hpd_po'] >= current_date - pd.DateOffset(months=months_range)\n",
    "            past_group = group.loc[:i-1][mask]\n",
    "\n",
    "            if not past_group.empty:\n",
    "                last_price = past_group['net_price_doc_curr_src_po'].iloc[-1]\n",
    "                if last_price > 0 and ((current_price - last_price) / last_price) >= price_increase_threshold:\n",
    "                    flagged_indices.append(current_row['index'])\n",
    "\n",
    "    df.loc[flagged_indices, flag_column] = 1\n",
    "    return df\n",
    "def flag_diff_vendor_price_variance(df, price_variance_threshold=0.05, months_range=6, flag_column='diff_vendor_price_variance_flag'):\n",
    "    df = df.copy()\n",
    "    df[flag_column] = 0\n",
    "\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['material_no_src_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['net_price_doc_curr_src_po'].notna()) &\n",
    "        (df['purch_doc_date_hpd_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    df_valid['purch_doc_date_hpd_po'] = pd.to_datetime(df_valid['purch_doc_date_hpd_po'])\n",
    "    df_valid = df_valid.sort_values(['material_no_src_po', 'purch_doc_date_hpd_po'])\n",
    "\n",
    "    for material, mat_group in df_valid.groupby('material_no_src_po'):\n",
    "        mat_group = mat_group.sort_values('purch_doc_date_hpd_po').reset_index()\n",
    "\n",
    "        for i in range(len(mat_group)):\n",
    "            current_row = mat_group.loc[i]\n",
    "            current_date = current_row['purch_doc_date_hpd_po']\n",
    "            current_price = current_row['net_price_doc_curr_src_po']\n",
    "\n",
    "            past_window = mat_group[\n",
    "                (mat_group['purch_doc_date_hpd_po'] < current_date) &\n",
    "                (mat_group['purch_doc_date_hpd_po'] >= current_date - pd.DateOffset(months=months_range))\n",
    "            ]\n",
    "\n",
    "            vendor_prices = past_window.groupby('vendor_or_creditor_acct_no_hpd_po')['net_price_doc_curr_src_po'].mean()\n",
    "            if not vendor_prices.empty:\n",
    "                max_price = vendor_prices.max()\n",
    "                min_price = vendor_prices.min()\n",
    "                if max_price > 0 and (max_price - min_price) / max_price >= price_variance_threshold:\n",
    "                    df.loc[current_row['index'], flag_column] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def final_parsing(df):\n",
    "        # === Parse Date Columns ===\n",
    "    date_cols = [\n",
    "        \"doc_change_date_src_po\",\n",
    "        \"doc_change_date_hpd_po\",\n",
    "        \"purch_doc_date_hpd_po\"\n",
    "    ]\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    \n",
    "    # === Feature Engineering ===\n",
    "    \n",
    "    # A. Date Features\n",
    "    df[\"po_doc_age_days\"] = (df[\"doc_change_date_hpd_po\"] - df[\"purch_doc_date_hpd_po\"]).dt.days\n",
    "    df[\"lead_time_po_vs_pr\"] = (df[\"purch_doc_date_hpd_po\"] - df[\"doc_change_date_src_po\"]).dt.days\n",
    "    df[\"po_day_of_week\"] = df[\"purch_doc_date_hpd_po\"].dt.dayofweek\n",
    "    df[\"po_day_of_month\"] = df[\"purch_doc_date_hpd_po\"].dt.day\n",
    "    df[\"po_month\"] = df[\"purch_doc_date_hpd_po\"].dt.month\n",
    "    \n",
    "    # B. Price & Value Features\n",
    "    df[\"price_per_unit\"] = df[\"net_val_po_curr_src_po\"] / df[\"quantity_src_po\"].replace(0, np.nan)\n",
    "    df[\"net_vs_gross_delta\"] = df[\"gross_val_po_curr_src_po\"] - df[\"net_val_po_curr_src_po\"]\n",
    "    df[\"price_variance_percent\"] = (df[\"net_val_po_curr_src_po\"] - df[\"gross_val_po_curr_src_po\"]) / df[\"gross_val_po_curr_src_po\"].replace(0, np.nan)\n",
    "    df[\"outline_agrmt_coverage\"] = df[\"outline_agrmt_tgt_val_doc_curr_src_po\"] / df[\"net_val_po_curr_src_po\"].replace(0, np.nan)\n",
    "    \n",
    "    # C. PO-PR Linkage Flags\n",
    "    df[\"has_pr_link\"] = df[\"pr_no_src_po\"].notna().astype(int)\n",
    "    df[\"has_pr_item_link\"] = df[\"pr_item_no_src_po\"].notna().astype(int)\n",
    "    \n",
    "    # D. Flags & Indicators\n",
    "    binary_cols = [\n",
    "        \"gr_indicator_src_po\", \"gr_invoice_verif_flag_src_po\",\n",
    "        \"inv_receipt_indicator_src_po\", \"release_indicator_hpd_po\",\n",
    "        \"release_status_hpd_po\", \"doc_release_incompl_flag_hpd_po\"\n",
    "    ]\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            df[col + \"_flag\"] = df[col].notna().astype(int)\n",
    "    \n",
    "    # E. Missing Signal\n",
    "    critical_cols = [\n",
    "        \"material_type_src_po\", \"material_no_src_po\",\n",
    "        \"vendor_or_creditor_acct_no_hpd_po\", \"gross_val_po_curr_src_po\",\n",
    "        \"net_price_doc_curr_src_po\"\n",
    "    ]\n",
    "    df[\"missing_critical_fields\"] = df[critical_cols].isnull().sum(axis=1)\n",
    "    \n",
    "    # F. Currency & Exchange Rate\n",
    "    df[\"has_exchange_rate\"] = df[\"exchange_rate_hpd_po\"].notna().astype(int)\n",
    "    df[\"log_exchange_rate\"] = np.log1p(df[\"exchange_rate_hpd_po\"].fillna(0))\n",
    "    \n",
    "    # G. Unit Conversion Features\n",
    "    df[\"p2o_conversion_ratio\"] = df[\"p2o_unit_conv_num_src_po\"] / df[\"p2o_unit_conv_denom_src_po\"].replace(0, np.nan)\n",
    "    df[\"o2b_conversion_ratio\"] = df[\"o2b_unit_conv_num_src_po\"] / df[\"o2b_unit_conv_denom_src_po\"].replace(0, np.nan)\n",
    "    \n",
    "    # H. Behavioral Flags\n",
    "    df[\"is_same_vendor_pr_po\"] = (\n",
    "        df[\"vendor_or_creditor_acct_no_hpd_po\"].notna() & df[\"base_id_src_po\"].notna()\n",
    "    ).astype(int)\n",
    "    df[\"has_rfq_status\"] = df[\"rfq_status_hpd_po\"].notna().astype(int)\n",
    "    df[\"purch_group_org_same\"] = (df[\"purch_group_hpd_po\"] == df[\"purch_org_hpd_po\"]).astype(int)\n",
    "    \n",
    "    # I. Rare Category Flagging\n",
    "    rare_cat_cols = [\"purch_doc_type_hpd_po\", \"purch_group_hpd_po\", \"vendor_or_creditor_acct_no_hpd_po\"]\n",
    "    for col in rare_cat_cols:\n",
    "        if col in df.columns:\n",
    "            freq_map = df[col].value_counts(normalize=True)\n",
    "            df[f\"{col}_is_rare\"] = df[col].map(freq_map) < 0.01\n",
    "\n",
    "    return df\n",
    "def _prep_invoice(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean and standardize invoice dataset.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # === Drop unwanted ===\n",
    "    drop_cols = [\"ingestion_timestamp\"]\n",
    "    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # === Date columns ===\n",
    "    date_cols = [\n",
    "        \"baseline_date_hpd_invoice\",\n",
    "        \"doc_date_hpd_invoice\",\n",
    "        \"doc_entry_date_hpd_invoice\",\n",
    "        \"entry_time_hpd_invoice\",\n",
    "        \"record_creation_dt\"\n",
    "    ]\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "    # === Fill Vendor & Company ===\n",
    "    df[\"vendor_or_creditor_acct_no_src_invoice\"] = df.get(\"vendor_or_creditor_acct_no_src_invoice\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"vendor_or_creditor_acct_no_hpd_invoice\"] = df.get(\"vendor_or_creditor_acct_no_hpd_invoice\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"company_code_src_invoice\"] = df.get(\"company_code_src_invoice\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "\n",
    "    # === Exchange rate ===\n",
    "    if \"exchange_rate_hpd_invoice\" in df.columns:\n",
    "        df[\"exchange_rate_hpd_invoice\"] = df[\"exchange_rate_hpd_invoice\"].replace({0: np.nan}).fillna(1.0)\n",
    "\n",
    "    # === Numeric cleaning ===\n",
    "    num_cols = [\n",
    "        \"amt_doc_curr_src_invoice\", \"quantity_src_invoice\",\n",
    "        \"po_qty_order_uom_src_invoice\", \"gross_inv_amt_doc_curr_hpd_invoice\",\n",
    "        \"unplanned_dlvry_costs_hpd_invoice\"\n",
    "    ]\n",
    "    for col in num_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def final_parsing_invoice(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Feature engineering for invoice dataset.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # A. Document Age\n",
    "    if \"doc_date_hpd_invoice\" in df.columns and \"baseline_date_hpd_invoice\" in df.columns:\n",
    "        df[\"invoice_doc_age_days\"] = (df[\"baseline_date_hpd_invoice\"] - df[\"doc_date_hpd_invoice\"]).dt.days\n",
    "\n",
    "    # B. Monetary Ratios\n",
    "    if {\"amt_doc_curr_src_invoice\", \"gross_inv_amt_doc_curr_hpd_invoice\"} <= set(df.columns):\n",
    "        df[\"invoice_net_vs_gross_delta\"] = df[\"gross_inv_amt_doc_curr_hpd_invoice\"] - df[\"amt_doc_curr_src_invoice\"]\n",
    "\n",
    "    if {\"amt_doc_curr_src_invoice\", \"quantity_src_invoice\"} <= set(df.columns):\n",
    "        df[\"invoice_price_per_unit\"] = df[\"amt_doc_curr_src_invoice\"] / df[\"quantity_src_invoice\"].replace(0, np.nan)\n",
    "\n",
    "    # C. Linkage Flags\n",
    "    df[\"has_po_link\"] = df[\"purch_doc_no_src_invoice\"].notna().astype(int)\n",
    "    df[\"has_po_item_link\"] = df[\"purch_doc_item_no_src_invoice\"].notna().astype(int)\n",
    "    df[\"has_vendor_link\"] = df[\"vendor_or_creditor_acct_no_src_invoice\"].notna().astype(int)\n",
    "\n",
    "    # D. Missing signal\n",
    "    critical_cols = [\"vendor_or_creditor_acct_no_src_invoice\", \"material_no_src_invoice\", \"amt_doc_curr_src_invoice\"]\n",
    "    df[\"missing_invoice_critical\"] = df[critical_cols].isnull().sum(axis=1)\n",
    "\n",
    "    # E. Rare category\n",
    "    if \"doc_type_hpd_invoice\" in df.columns:\n",
    "        freq_map = df[\"doc_type_hpd_invoice\"].value_counts(normalize=True)\n",
    "        df[\"doc_type_hpd_invoice_is_rare\"] = df[\"doc_type_hpd_invoice\"].map(freq_map) < 0.01\n",
    "\n",
    "    return df\n",
    "\n",
    "##############################################################################\n",
    "#  Cell 3 ‚Äî Create / load features\n",
    "##############################################################################\n",
    "if FEAT_PO.exists():\n",
    "    print(\"‚ö° Loading cached features\")\n",
    "    df = pd.read_pickle(FEAT_PO)\n",
    "else:\n",
    "    print(\"üöß Generating features ‚Ä¶\")\n",
    "    #raw_df  = pd.read_pickle(RAW_PO)\n",
    "    raw_df  = data_load_and_cleaning_po()\n",
    "    columns_to_drop = ['main_risk_scenario', 'sub_risks']\n",
    "    raw_df.drop(columns=[col for col in columns_to_drop if col in raw_df.columns], inplace=True)\n",
    "    #invoice_df = _prep_invoice(raw_df)\n",
    "    #invoice_df_2 = final_parsing_invoice(invoice_df)\n",
    "    feat_df = build_features(raw_df)\n",
    "    df=flag_split_po(feat_df)\n",
    "    df=flag_intra_po_split(df)\n",
    "    df = flag_multiple_pos_per_pr_item(df)\n",
    "    # Same vendor price jump\n",
    "    df = flag_same_vendor_price_increase(df, months_range=6, flag_column='same_vendor_price_increase_6m_flag')\n",
    "    df = flag_same_vendor_price_increase(df, months_range=12, flag_column='same_vendor_price_increase_12m_flag')\n",
    "    \n",
    "    # Diff vendor price variance\n",
    "    df = flag_diff_vendor_price_variance(df, months_range=6, flag_column='diff_vendor_price_variance_6m_flag')\n",
    "    df = flag_diff_vendor_price_variance(df, months_range=12, flag_column='diff_vendor_price_variance_12m_flag')\n",
    "    df=final_parsing(df)\n",
    "    df.to_pickle(FEAT_PO)\n",
    "    #print(\"Feature shape:\", df.shape)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#  Cell 4 ‚Äî Fast train (no hyper-parameter search, no RFECV)\n",
    "##############################################################################\n",
    "# =======================\n",
    "# Reproducibility\n",
    "# =======================\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# =======================\n",
    "# Step 1: Prepare Data\n",
    "# =======================\n",
    "target = \"rft_by_engine_7\"\n",
    "df[target] = df[target].fillna(0).astype(int)\n",
    "y = df[target]\n",
    "X = df.drop(columns=[target])\n",
    "\n",
    "num_cols = [col for col in X.columns if X[col].dtype.kind in 'if']\n",
    "cat_cols = [col for col in X.columns if X[col].dtype.kind not in 'if']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ]), num_cols),\n",
    "\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, min_frequency=10))\n",
    "    ]), cat_cols)\n",
    "])\n",
    "\n",
    "X_scaled = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split for training and testing\n",
    "X_train_unsupervised = X_scaled[y == 0]\n",
    "X_test = X_scaled\n",
    "y_test = y.values\n",
    "\n",
    "# =======================\n",
    "# Step 2: Define Autoencoder\n",
    "# =======================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_1, hidden_2, bottleneck):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_1, hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_2, bottleneck)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck, hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_2, hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_1, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# =======================\n",
    "# Step 3: Train Final Model\n",
    "# =======================\n",
    "\n",
    "#{'hidden_1': 138, 'hidden_2': 33, 'bottleneck': 10, 'lr': 0.00036672950388598225, 'batch_size': 256}\n",
    "\n",
    "#Best Hyperparameters:\n",
    "#{'hidden_1': 64, 'hidden_2': 98, 'bottleneck': 14, 'lr': 0.0004853968880470261, 'batch_size': 64}\n",
    "best_params = {\n",
    "    'hidden_1': 138,\n",
    "    'hidden_2': 33,\n",
    "    'bottleneck': 10,\n",
    "    'lr':  0.00036672950388598225,\n",
    "    'batch_size': 256\n",
    "}\n",
    "\n",
    "input_dim = X_train_unsupervised.shape[1]\n",
    "model = Autoencoder(input_dim, best_params['hidden_1'], best_params['hidden_2'], best_params['bottleneck'])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_unsupervised, dtype=torch.float32)\n",
    "batch_size = best_params['batch_size']\n",
    "n_epochs = 100\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "        batch = X_train_tensor[i:i+batch_size]\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / (X_train_tensor.size(0) // batch_size)\n",
    "    print(f\"Epoch {epoch:03d} | Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if avg_loss < best_loss - 1e-4:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"‚èπÔ∏è Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# =======================\n",
    "# Step 4: Evaluate\n",
    "# =======================\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed = model(X_test_tensor)\n",
    "    recon_error = torch.mean((X_test_tensor - reconstructed) ** 2, dim=1).numpy()\n",
    "\n",
    "# Dynamic threshold based on normal (non-fraud) population\n",
    "threshold = np.percentile(recon_error[y_test == 0], 91)\n",
    "print(f\"\\nüö® Reconstruction Error Threshold: {threshold:.4f}\")\n",
    "\n",
    "predicted_labels = (recon_error > threshold).astype(int)\n",
    "\n",
    "# =======================\n",
    "# Step 5: Report & Confusion Matrix\n",
    "# =======================\n",
    "print(\"\\nüìä Classification Report (Autoencoder):\")\n",
    "print(classification_report(y_test, predicted_labels))\n",
    "\n",
    "cm = confusion_matrix(y_test, predicted_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Risk\", \"Fraud\"])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Final Autoencoder\")\n",
    "plt.show()\n",
    "\n",
    "# =======================\n",
    "# Step 6: Add Back to DataFrame\n",
    "# =======================\n",
    "df[\"ae_fraud_score\"] = recon_error\n",
    "df[\"ae_predicted_flag\"] = predicted_labels\n",
    "\n",
    "torch.save(model.state_dict(), \"Po_Invoice_Data/po_autoencoder_model_02_10.pth\")\n",
    "joblib.dump(preprocessor, \"Po_Invoice_Data/po_preprocessor_02_10.pkl\")\n",
    "joblib.dump(best_params, \"Po_Invoice_Data/po_best_params_02_10.pkl\")\n",
    "joblib.dump(threshold, \"Po_Invoice_Data/po_autoencoder_threshold_02_10.pkl\")\n",
    "print(\"‚úÖ Baseline model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeadafd-e0f1-408b-bd77-0a0c32aaa949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789428dc-8d11-4e52-93a8-fc233c9ab852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Loading cached features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajayn\\anaconda3\\envs\\py312env\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['central_purch_blk_flag']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "[I 2025-10-02 20:19:02,931] A new study created in memory with name: no-name-ebf5a2c0-6e63-42cf-8049-06b2a934ec1d\n",
      "[I 2025-10-02 20:19:22,995] Trial 0 finished with value: 0.693929173693086 and parameters: {'hidden_1': 117, 'hidden_2': 58, 'bottleneck': 21, 'lr': 0.00022512953432070822, 'batch_size': 128}. Best is trial 0 with value: 0.693929173693086.\n",
      "[I 2025-10-02 20:19:43,062] Trial 1 finished with value: 0.5990352728369008 and parameters: {'hidden_1': 128, 'hidden_2': 16, 'bottleneck': 6, 'lr': 0.0006021065315915255, 'batch_size': 64}. Best is trial 0 with value: 0.693929173693086.\n",
      "[I 2025-10-02 20:19:58,828] Trial 2 finished with value: 0.21440922190201728 and parameters: {'hidden_1': 238, 'hidden_2': 123, 'bottleneck': 18, 'lr': 0.0032161136637307635, 'batch_size': 64}. Best is trial 0 with value: 0.693929173693086.\n",
      "[I 2025-10-02 20:20:16,235] Trial 3 finished with value: 0.16758675078864355 and parameters: {'hidden_1': 232, 'hidden_2': 125, 'bottleneck': 11, 'lr': 0.0008835300619109094, 'batch_size': 64}. Best is trial 0 with value: 0.693929173693086.\n",
      "[I 2025-10-02 20:20:22,845] Trial 4 finished with value: 0.10635696821515893 and parameters: {'hidden_1': 220, 'hidden_2': 26, 'bottleneck': 22, 'lr': 0.00405532935264243, 'batch_size': 256}. Best is trial 0 with value: 0.693929173693086.\n",
      "[I 2025-10-02 20:20:30,954] Trial 5 finished with value: 0.17621664050235478 and parameters: {'hidden_1': 192, 'hidden_2': 46, 'bottleneck': 7, 'lr': 0.004370449462478169, 'batch_size': 128}. Best is trial 0 with value: 0.693929173693086.\n",
      "[I 2025-10-02 20:21:51,318] Trial 6 finished with value: 0.5319854683304376 and parameters: {'hidden_1': 222, 'hidden_2': 22, 'bottleneck': 28, 'lr': 0.00010498421863553215, 'batch_size': 64}. Best is trial 0 with value: 0.693929173693086.\n",
      "[I 2025-10-02 20:22:06,993] Trial 7 finished with value: 0.6979546091342113 and parameters: {'hidden_1': 73, 'hidden_2': 18, 'bottleneck': 14, 'lr': 0.00027617899108732855, 'batch_size': 256}. Best is trial 7 with value: 0.6979546091342113.\n",
      "[I 2025-10-02 20:22:29,604] Trial 8 finished with value: 0.6544085710149125 and parameters: {'hidden_1': 251, 'hidden_2': 77, 'bottleneck': 14, 'lr': 0.00030816619922817185, 'batch_size': 128}. Best is trial 7 with value: 0.6979546091342113.\n",
      "[I 2025-10-02 20:22:51,417] Trial 9 finished with value: 0.6963108430354888 and parameters: {'hidden_1': 186, 'hidden_2': 24, 'bottleneck': 30, 'lr': 0.00016191756709260787, 'batch_size': 256}. Best is trial 7 with value: 0.6979546091342113.\n",
      "[I 2025-10-02 20:22:56,588] Trial 10 finished with value: 0.4416498993963783 and parameters: {'hidden_1': 66, 'hidden_2': 87, 'bottleneck': 14, 'lr': 0.0017238948752193103, 'batch_size': 256}. Best is trial 7 with value: 0.6979546091342113.\n",
      "[I 2025-10-02 20:23:17,925] Trial 11 finished with value: 0.6963108430354888 and parameters: {'hidden_1': 164, 'hidden_2': 43, 'bottleneck': 31, 'lr': 0.00010904635399531642, 'batch_size': 256}. Best is trial 7 with value: 0.6979546091342113.\n",
      "[I 2025-10-02 20:23:32,190] Trial 12 finished with value: 0.7082696316886727 and parameters: {'hidden_1': 75, 'hidden_2': 41, 'bottleneck': 26, 'lr': 0.0003689196253782742, 'batch_size': 256}. Best is trial 12 with value: 0.7082696316886727.\n",
      "[I 2025-10-02 20:23:45,396] Trial 13 finished with value: 0.6891396332863188 and parameters: {'hidden_1': 71, 'hidden_2': 42, 'bottleneck': 25, 'lr': 0.00043295999750840255, 'batch_size': 256}. Best is trial 12 with value: 0.7082696316886727.\n",
      "[I 2025-10-02 20:23:51,418] Trial 14 finished with value: 0.5263992389408594 and parameters: {'hidden_1': 99, 'hidden_2': 60, 'bottleneck': 17, 'lr': 0.0014104484381127933, 'batch_size': 256}. Best is trial 12 with value: 0.7082696316886727.\n",
      "[I 2025-10-02 20:23:55,510] Trial 15 finished with value: 0.26798360044726055 and parameters: {'hidden_1': 96, 'hidden_2': 98, 'bottleneck': 25, 'lr': 0.009160549801176742, 'batch_size': 256}. Best is trial 12 with value: 0.7082696316886727.\n",
      "[I 2025-10-02 20:24:15,596] Trial 16 finished with value: 0.7082696316886727 and parameters: {'hidden_1': 138, 'hidden_2': 35, 'bottleneck': 11, 'lr': 0.0004655431491065417, 'batch_size': 256}. Best is trial 12 with value: 0.7082696316886727.\n",
      "[I 2025-10-02 20:24:34,041] Trial 17 finished with value: 0.7104204245872069 and parameters: {'hidden_1': 141, 'hidden_2': 36, 'bottleneck': 9, 'lr': 0.0006105958920533591, 'batch_size': 256}. Best is trial 17 with value: 0.7104204245872069.\n",
      "[I 2025-10-02 20:24:43,284] Trial 18 finished with value: 0.6216817440308468 and parameters: {'hidden_1': 151, 'hidden_2': 59, 'bottleneck': 4, 'lr': 0.0008572576046499999, 'batch_size': 256}. Best is trial 17 with value: 0.7104204245872069.\n",
      "[I 2025-10-02 20:24:48,382] Trial 19 finished with value: 0.11097560975609756 and parameters: {'hidden_1': 104, 'hidden_2': 69, 'bottleneck': 26, 'lr': 0.00148763902368898, 'batch_size': 128}. Best is trial 17 with value: 0.7104204245872069.\n",
      "[I 2025-10-02 20:25:17,806] Trial 20 finished with value: 0.6994122586062133 and parameters: {'hidden_1': 174, 'hidden_2': 34, 'bottleneck': 22, 'lr': 0.0005085786221409294, 'batch_size': 256}. Best is trial 17 with value: 0.7104204245872069.\n",
      "[I 2025-10-02 20:25:40,421] Trial 21 finished with value: 0.7230008244023083 and parameters: {'hidden_1': 146, 'hidden_2': 35, 'bottleneck': 11, 'lr': 0.000411413939943405, 'batch_size': 256}. Best is trial 21 with value: 0.7230008244023083.\n",
      "[I 2025-10-02 20:26:05,212] Trial 22 finished with value: 0.6103468899521531 and parameters: {'hidden_1': 155, 'hidden_2': 49, 'bottleneck': 9, 'lr': 0.0006793443770911289, 'batch_size': 256}. Best is trial 21 with value: 0.7230008244023083.\n",
      "[I 2025-10-02 20:26:27,082] Trial 23 finished with value: 0.7240527182866557 and parameters: {'hidden_1': 138, 'hidden_2': 33, 'bottleneck': 10, 'lr': 0.00036672950388598225, 'batch_size': 256}. Best is trial 23 with value: 0.7240527182866557.\n",
      "[I 2025-10-02 20:26:44,314] Trial 24 finished with value: 0.6942961506041023 and parameters: {'hidden_1': 137, 'hidden_2': 31, 'bottleneck': 10, 'lr': 0.0001709300224635813, 'batch_size': 256}. Best is trial 23 with value: 0.7240527182866557.\n",
      "[I 2025-10-02 20:26:59,175] Trial 25 finished with value: 0.6970419178466284 and parameters: {'hidden_1': 117, 'hidden_2': 51, 'bottleneck': 8, 'lr': 0.00018683128845923576, 'batch_size': 256}. Best is trial 23 with value: 0.7240527182866557.\n",
      "[I 2025-10-02 20:27:07,586] Trial 26 finished with value: 0.6953958450308815 and parameters: {'hidden_1': 199, 'hidden_2': 32, 'bottleneck': 5, 'lr': 0.00117900856267621, 'batch_size': 256}. Best is trial 23 with value: 0.7240527182866557.\n",
      "[I 2025-10-02 20:27:15,166] Trial 27 finished with value: 0.26927374301675977 and parameters: {'hidden_1': 170, 'hidden_2': 72, 'bottleneck': 12, 'lr': 0.0019878037985125714, 'batch_size': 128}. Best is trial 23 with value: 0.7240527182866557.\n",
      "[I 2025-10-02 20:27:30,717] Trial 28 finished with value: 0.29467889908256883 and parameters: {'hidden_1': 148, 'hidden_2': 55, 'bottleneck': 17, 'lr': 0.0006501970213128373, 'batch_size': 64}. Best is trial 23 with value: 0.7240527182866557.\n",
      "[I 2025-10-02 20:27:48,690] Trial 29 finished with value: 0.6876588534312341 and parameters: {'hidden_1': 115, 'hidden_2': 108, 'bottleneck': 13, 'lr': 0.0002816160814454497, 'batch_size': 256}. Best is trial 23 with value: 0.7240527182866557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'hidden_1': 138, 'hidden_2': 33, 'bottleneck': 10, 'lr': 0.00036672950388598225, 'batch_size': 256}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "##############################################################################\n",
    "#  Cell 1 ‚Äî Imports & common paths\n",
    "##############################################################################\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import joblib\n",
    "import logging\n",
    "from typing import List\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import logging\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "import optuna\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set options to show full DataFrame output\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import joblib, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from typing import Any, Dict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "RAW_PO      = Path(\"po_data.pkl\") #(\"../Po_Invoice_Data/po_output_tushar.pkl\")      # raw PO data (input)\n",
    "FEAT_PO     = Path(\"Po_Invoice_Data/po_output_features_df_auto_model_02_10.pkl\")         # engineered features\n",
    "MODEL_PKL   = Path(\"Po_Invoice_Data/po_autoencoder_model_02_10.pkl\")         # tuned model file\n",
    "#CV_REPORT   = Path(\"Po_Invoice_Data/cv_results.csv\")            # param grid results\n",
    "#SCORING_OUT = Path(\"Po_Invoice_Data/scored_po.pkl\")             # predictions file\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Column standardisation helper\n",
    "# -----------------------------------------------------------------------------\n",
    "def data_load_and_cleaning_invoice():\n",
    "        ### setting connection\n",
    "    #conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\t#database='baldota-dev-db',\n",
    "    \t\t\t#user='fortifai_ng_ai_user_rw',\n",
    "    \t\t\t#password='AIPwd@123!',\n",
    "    \t\t\t#port='5432',\n",
    "                #sslmode=\"require\"\n",
    "    \t\t#)\n",
    "    \n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_user_ro',\n",
    "    \t\t\tpassword='user@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "                SELECT table_name \n",
    "                FROM information_schema.tables \n",
    "            \"\"\")\n",
    "    tables = [row[0] for row in cur.fetchall()]\n",
    "    \n",
    "     # --- Step 4: Commit and close ---\n",
    "    #conn.commit()\n",
    "    #cur.close()\n",
    "    #conn.close()\n",
    "    \n",
    "    line_item_keys = [\"Purch.Doc.\",\"Item\", \"Purch.Req.\",\"Item.1\",\n",
    "        \"Buyer Name\", \"Changed On\", \"CoCd\", \"Eq. To\", \"Denom.\", \"Conv.\", \"Conv..1\", \"Ct\", \"Customer\", \"D\",\n",
    "        \"GR\", \"GR Date\", \"GR-IV\", \"Gross value\", \"IR\",\"MTyp\", \"Material\", \"Matl Group\",\n",
    "        \"Net Price\", \"Net Value\", \"PO Quantity.1\", \"PO Quantity\", \"Plnt\",\n",
    "        \"Reference Document for PO Trac\", \"S\", \"Short Text\", \"Targ. Qty\", \"Target Value\", \"TrackingNo\",\n",
    "        \"EKPO-CHG_FPLNR\", \"Acknowledgment\", \"Agmt.\", \"Item.2\", \"Status of purchasing doc. item\"\n",
    "    ]\n",
    "    line_item_values = [\"purch_doc_no\", \"purch_doc_item_no\",\"pr_no\", \"pr_item_no\",\n",
    "        \"requester_name\", \"doc_change_date\", \"company_code\", \"p2o_unit_conv_denom\", \"o2b_unit_conv_denom\",\n",
    "        \"p2o_unit_conv_num\", \"o2b_unit_conv_num\", \"acct_assgnmt_category\", \"customer_no\", \"po_item_del_flag\",\n",
    "        \"gr_indicator\", \"latest_gr_dt\", \"gr_invoice_verif_flag\", \"gross_val_po_curr\", \"inv_receipt_indicator\",\n",
    "        \"material_type\", \"material_no\", \"matl_group\", \"net_price_doc_curr\",\n",
    "        \"net_val_po_curr\", \"order_uom\", \"quantity\", \"plant\", \"tpop_crm_ref_ordr_no\",\n",
    "        \"rfq_status\", \"short_text\", \"target_qty\", \"outline_agrmt_tgt_val_doc_curr\", \"reqmt_tracking_no\",\n",
    "        \"no_invoice_flag\", \"order_ack_no\", \"principal_purch_agrmt_no\", \"principal_purch_agrmt_item_no\",\n",
    "        \"purch_doc_item_status\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Step 2: Create the dictionary\n",
    "    my_dict_line_item = dict(zip(line_item_keys, line_item_values))\n",
    "    \n",
    "    # Step 3: Print the result\n",
    "    print(\"Line items key with baldota: mapped values SAP fixed\",my_dict_line_item)\n",
    "    \n",
    "    header_keys=[\"Purch.Doc.\",\"Supplier\",\n",
    "        \"C\", \"CoCd\", \"Crcy\", \"Created By\", \"Created On\", \"Ctl\", \"D\", \"Doc. Date\",\n",
    "        \"Doc.Cond.\", \"Exch. Rate\", \"PGr\", \"POrg\", \"PayT\", \"Proc.state\",\n",
    "        \"R\", \"Rel\", \"Release\", \"S\", \"Salespers.\", \"Tot. value\", \"Type\",\n",
    "        \"VP Start\", \"VPer.End\"]\n",
    "    header_values=[\"purch_doc_no\",\"vendor_or_creditor_acct_no\",\n",
    "        \"purch_doc_category\", \"company_code\", \"currency\", \"object_created_by\", \"doc_change_date\",\n",
    "        \"control_indicator\", \"po_item_del_flag\", \"purch_doc_date\", \"principal_purch_agrmt_no\",\n",
    "        \"exchange_rate\", \"purch_group\", \"purch_org\", \"pymnt_terms\", \"processing_status\", \"doc_release_incompl_flag\", \"release_indicator\", \"release_status\",\n",
    "        \"rfq_status\", \"resp_vendor_salesperson\",\n",
    "        \"on_release_total_value\", \"purch_doc_type\", \"validity_start_dt\", \"validity_end_dt\"\n",
    "    ]\n",
    "    # Step 2: Create the dictionary\n",
    "    my_header_dict = dict(zip(header_keys, header_values))\n",
    "    \n",
    "    # Step 3: Print the result\n",
    "    print(\"Header key with baldota: mapped values SAP fixed\",my_header_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #tables = [row[0] for row in cur.fetchall()]\n",
    "    #p2p_line_item_po_data\n",
    "    for table in tables:\n",
    "        if table == 'purchasing_document_item':\n",
    "            p2p_line_item_po_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{table}\", conn)\n",
    "            \n",
    "    df_line_item_po_data=p2p_line_item_po_data.copy()\n",
    "    ## dropping '4500009180^00030' at index 0 as its order_uom is 0.0 must be added for testing\n",
    "    df_line_item_po_data.drop(index=0, inplace=True)\n",
    "    \n",
    "    # convert to str to maintain\n",
    "    df_line_item_po_data['purch_doc_no'] = df_line_item_po_data['purch_doc_no'].astype(float).astype('int64').astype(str)\n",
    "    #df_line_item_po_data['pr_no'] = df_line_item_po_data['pr_no'].astype(float).astype('int64').astype(str)\n",
    "    \n",
    "    ### bring values for item number to orginal form, 10.0 to 00010 etc\n",
    "    df_line_item_po_data['purch_doc_item_no'] = df_line_item_po_data['purch_doc_item_no'].fillna(0).astype(float).astype(int).astype(str).str.zfill(5)\n",
    "    df_line_item_po_data['pr_item_no'] = df_line_item_po_data['pr_item_no'].fillna(0).astype(float).astype(int).astype(str).str.zfill(5)\n",
    "    #df_line_item_po_data['principal_purch_agrmt_item_no'] = df_line_item_po_data['principal_purch_agrmt_item_no'].fillna(0).astype(float).astype(int).astype(str).str.zfill(5)\n",
    "    \n",
    "    ## converting all column values dtype to string except for ingestion_timestamp :: will be updating dtype for values in code when those are required\n",
    "    #df_line_item_po_data.loc[:, df_line_item_po_data.columns != 'ingestion_timestamp'] = df_line_item_po_data.loc[:, df_line_item_po_data.columns != 'ingestion_timestamp'].astype(str)\n",
    "    \n",
    "    ##main data using the mapped columns from baldota\n",
    "    df_line_item=df_line_item_po_data[line_item_values]\n",
    "    ## to merge with label data later on\n",
    "    df_line_item['base_id']=df_line_item[\"purch_doc_no\"] + \"^\" + df_line_item[\"purch_doc_item_no\"]\n",
    "    # adding src to line item data columns to distinguish with header column data\n",
    "    df_line_item_renamed = df_line_item.rename(columns={col: f\"{col}_src\" for col in df_line_item.columns})\n",
    "    \n",
    "    \n",
    "    #p2p_header_po_data\n",
    "    for table in tables:\n",
    "        if table == 'purchasing_document_header':\n",
    "            p2p_header_po_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{table}\", conn)\n",
    "    df_header_po_data=p2p_header_po_data.copy()\n",
    "    \n",
    "    df_header_po_data['purch_doc_no'] = df_header_po_data['purch_doc_no'].astype(float).astype('int64').astype(str)\n",
    "    ## converting all column values dtype to string except for ingestion_timestamp :: will be updating dtype for values in code when those are required\n",
    "    #df_header_po_data.loc[:, df_header_po_data.columns != 'ingestion_timestamp'] = df_header_po_data.loc[:, df_header_po_data.columns != 'ingestion_timestamp'].astype(str)\n",
    "    ##main data using the mapped columns from baldota\n",
    "    df_header=df_header_po_data[header_values]\n",
    "    # adding hpd to header data columns to distinguish with line_item column data\n",
    "    df_header_renamed = df_header.rename(columns={col: f\"{col}_hpd\" for col in df_header.columns})\n",
    "    \n",
    "    merged_df_before_label=pd.merge(df_line_item_renamed,df_header_renamed,left_on='purch_doc_no_src',right_on='purch_doc_no_hpd',how='outer')\n",
    "    merged_df_before_label.shape\n",
    "    \n",
    "    \n",
    "    df=merged_df_before_label.copy()\n",
    "    # Convert dates\n",
    "    df[\"doc_change_date_src\"] = pd.to_datetime(df[\"doc_change_date_src\"], errors='coerce')\n",
    "    df[\"doc_change_date_hpd\"] = pd.to_datetime(df[\"doc_change_date_hpd\"], errors='coerce')\n",
    "    df[\"purch_doc_date_hpd\"] = pd.to_datetime(df[\"purch_doc_date_hpd\"], errors='coerce')\n",
    "    \n",
    "    # drop all rows where all values are Nan\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    #df.info()\n",
    "    #df['purch_doc_mapping']=df[\"purch_doc_no_src\"] + \"^\" + df[\"purch_doc_item_no_src\"]\n",
    "    df_final_po= df.rename(columns={col: f\"{col}_po\" for col in df.columns})\n",
    "    df_final_po.info()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    # there are 3 po that have data in header but not in line item\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Invoice Data ####\n",
    "    \n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_user_ro',\n",
    "    \t\t\tpassword='user@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "                SELECT table_name \n",
    "                FROM information_schema.tables \n",
    "            \"\"\")\n",
    "    tables = [row[0] for row in cur.fetchall()]\n",
    "    \n",
    "    \n",
    "    line_item_keys = [\"DocumentNo\",\"InvItem\",\"Purch.Doc.\",\"Item\", \"Amount\", \"BlR\", \"Central Contract\", \"Central Contract Item\", \"CoCd\", \"D/C\", \"FIn\",\n",
    "        \"GR/IR Clrg\", \"Indicator for Differential Invoicing\", \"Material\", \"Plnt\",\n",
    "         \"OUn\", \"Quantity\", \"OPUn\", \"Qty in OPUn\", \"Reference\", \"SAA\", \"Supplier\",\n",
    "        \"Tax Jur.\", \"Year\", \"Year.1\"\n",
    "    ]\n",
    "    line_item_values = [\"accounting_doc_no\",\"doc_line_item_no\",\"purch_doc_no\", \"purch_doc_item_no\",\"amt_doc_curr\", \"block_reason_field\", \"central_contract\",\n",
    "                        \"central_contract_item_no\",\"company_code\", \"debit_credit_flag\", \"final_inv_flag\",\n",
    "        \"ext_gr_ir_clrg_flag\", \"diff_invoicing_flag\", \n",
    "        \"material_no\", \"plant\", \"order_uom\", \"quantity\", \"po_uom\",\n",
    "        \"po_qty_order_uom\", \"ref_doc_no\", \"acct_assgnmt_seq_no\", \"vendor_or_creditor_acct_no\",\n",
    "        \"tax_jurisdiction_code\", \"fiscal_year\", \"ref_doc_fiscal_year\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Step 2: Create the dictionary\n",
    "    my_dict_line_item = dict(zip(line_item_keys, line_item_values))\n",
    "    \n",
    "    # Step 3: Print the result\n",
    "    print(\"Line items key with baldota: mapped values SAP fixed\",my_dict_line_item)\n",
    "    \n",
    "    header_keys= [\"Doc. No.\",\n",
    "        \"Bline Date\", \"CoCd\", \"Crcy\", \"Del.Costs\", \"Doc. Date\",  \"Doc.Header Text\",\n",
    "        \"Entry Dte\", \"Exch.rate\", \"G/L\", \"Gross Amnt\", \"I\", \"IV cat\", \"InR.Ref.no\", \"Inv. Pty\",\n",
    "        \"PBk\", \"PM\", \"PayT\", \"Payer\", \"Paymt Ref.\", \"Reference\", \"Rel.\", \"Rvrsd by\",\n",
    "        \"St\", \"TCode\", \"Time\", \"Type\", \"User Name\"\n",
    "    ]\n",
    "    header_values=[\"accounting_doc_no\",\"baseline_date\", \"company_code\", \"currency\", \"unplanned_dlvry_costs\", \"doc_date\",\n",
    "         \"doc_header_text\", \"doc_entry_date\", \"exchange_rate\", \"gl_account\",\n",
    "        \"gross_inv_amt_doc_curr\", \"post_inv_flag\", \"logistics_inv_verif_orig_type\", \"txn_invoice_no\",\n",
    "        \"vendor_or_creditor_acct_no\", \"house_bank_short_key\", \"pymnt_method\", \"pymnt_terms\",\n",
    "        \"payee_or_payer_name\", \"assignment_no\", \"ref_doc_no\", \"sap_release\", \"reversal_doc_no\",\n",
    "        \"invoice_doc_status\", \"txn_code\", \"entry_time\", \"doc_type\", \"username\"\n",
    "    ]\n",
    "    # Step 2: Create the dictionary\n",
    "    my_header_dict = dict(zip(header_keys, header_values))\n",
    "    \n",
    "    # Step 3: Print the result\n",
    "    print(\"Header key with baldota: mapped values SAP fixed\",my_header_dict)\n",
    "    \n",
    "    \n",
    "    #tables = [row[0] for row in cur.fetchall()]\n",
    "    #p2p_line_item_invoice_data\n",
    "    for table in tables:\n",
    "        if table == 'invoice_receipt_items':\n",
    "            p2p_line_item_invoice_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{table}\", conn)\n",
    "            \n",
    "    df_line_item_invoice_data=p2p_line_item_invoice_data.copy()\n",
    "    # there are na values in accounting_doc_no--> dropping\n",
    "    df_line_item_invoice_data= df_line_item_invoice_data.dropna(subset=['accounting_doc_no'])\n",
    "    \n",
    "    # convert to str to maintain\n",
    "    df_line_item_invoice_data['accounting_doc_no'] = df_line_item_invoice_data['accounting_doc_no'].astype(float).astype('int64').astype(str)\n",
    "    df_line_item_invoice_data['purch_doc_no'] = df_line_item_invoice_data['purch_doc_no'].astype(float).astype('int64').astype(str)\n",
    "    \n",
    "    ### bring values for item number to orginal form, 10.0 to 00010 etc\n",
    "    df_line_item_invoice_data['purch_doc_item_no'] = df_line_item_invoice_data['purch_doc_item_no'].fillna(0).astype(float).astype(int).astype(str).str.zfill(5)\n",
    "    # convert doc_line_item_no to str\n",
    "    df_line_item_invoice_data['doc_line_item_no'] = df_line_item_invoice_data['doc_line_item_no'].fillna(0).astype(float).astype(int).astype(str)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##main data using the mapped columns from baldota\n",
    "    df_line_item=df_line_item_invoice_data[line_item_values]\n",
    "    ## to merge with label data later on\n",
    "    df_line_item['base_id']=df_line_item[\"accounting_doc_no\"] + \"^\" + df_line_item[\"doc_line_item_no\"]\n",
    "    # adding src to line item data columns to distinguish with header column data\n",
    "    df_line_item_renamed = df_line_item.rename(columns={col: f\"{col}_src\" for col in df_line_item.columns})\n",
    "    \n",
    "    \n",
    "    #p2p_header_invoice_data\n",
    "    for table in tables:\n",
    "        if table == 'invoice_receipt_header':\n",
    "            p2p_header_invoice_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{table}\", conn)\n",
    "    df_header_invoice_data=p2p_header_invoice_data.copy()\n",
    "    \n",
    "    # there are na values in accounting_doc_no--> dropping\n",
    "    df_header_invoice_data= df_header_invoice_data.dropna(subset=['accounting_doc_no'])\n",
    "    \n",
    "    # str maintain\n",
    "    df_header_invoice_data['accounting_doc_no'] = df_header_invoice_data['accounting_doc_no'].astype(float).astype('int64').astype(str)\n",
    "    ## converting all column values dtype to string except for ingestion_timestamp :: will be updating dtype for values in code when those are required\n",
    "    #df_header_po_data.loc[:, df_header_po_data.columns != 'ingestion_timestamp'] = df_header_po_data.loc[:, df_header_po_data.columns != 'ingestion_timestamp'].astype(str)\n",
    "    ##main data using the mapped columns from baldota\n",
    "    df_header=df_header_invoice_data[header_values]\n",
    "    # adding hpd to header data columns to distinguish with line_item column data\n",
    "    df_header_renamed = df_header.rename(columns={col: f\"{col}_hpd\" for col in df_header.columns})\n",
    "    \n",
    "    merged_df_before_label=pd.merge(df_line_item_renamed,df_header_renamed,left_on='accounting_doc_no_src',right_on='accounting_doc_no_hpd',how='outer')\n",
    "    merged_df_before_label.shape\n",
    "    \n",
    "    df=merged_df_before_label.copy()\n",
    "    # Convert dates\n",
    "    df[\"baseline_date_hpd\"] = pd.to_datetime(df[\"baseline_date_hpd\"], errors='coerce')\n",
    "    df[\"doc_date_hpd\"] = pd.to_datetime(df[\"doc_date_hpd\"], errors='coerce')\n",
    "    df[\"doc_entry_date_hpd\"] = pd.to_datetime(df[\"doc_entry_date_hpd\"], errors='coerce')\n",
    "    \n",
    "    # drop all rows where all values are Nan\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    #df.info()\n",
    "    df['purch_doc_mapping']=df[\"purch_doc_no_src\"] + \"^\" + df[\"purch_doc_item_no_src\"]\n",
    "    df_final_invoice = df.rename(columns={col: f\"{col}_invoice\" for col in df.columns})\n",
    "    df_final_invoice.info()\n",
    "    \n",
    "    ## there are 175 invoice that have data in header but not in line item\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    \n",
    "    po_invoice=pd.merge(df_final_po,df_final_invoice, left_on='base_id_src_po',right_on='purch_doc_mapping_invoice',how='outer')\n",
    "    \n",
    "    ## cleaning .0 from vendor\n",
    "    s = po_invoice['vendor_or_creditor_acct_no_hpd_po'].astype(str).str.strip()           # ensure string & tidy spaces\n",
    "    mask = s.str.upper().eq('UNKNOWN')                 # rows to leave as-is\n",
    "    po_invoice['vendor_or_creditor_acct_no_hpd_po'] = s.where(mask, s.str.replace(r'\\.0$', '', regex=True))\n",
    "    \n",
    "    ## getting vendor name from vendor master DB\n",
    "    conn = psycopg2.connect(host='fortifai-ng-dev-db.postgres.database.azure.com',\n",
    "    \t\t\tdatabase='baldota-dev-db',\n",
    "    \t\t\tuser='fortifai_ng_user_ro',\n",
    "    \t\t\tpassword='user@123!',\n",
    "    \t\t\tport='5432',\n",
    "                sslmode=\"require\"\n",
    "    \t\t)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "                SELECT table_name \n",
    "                FROM information_schema.tables \n",
    "            \"\"\")\n",
    "    tables = [row[0] for row in cur.fetchall()]\n",
    "    vendor_data= pd.read_sql_query(f\"SELECT * FROM ingest_db.{'vendor_master_general_section'}\", conn)\n",
    "    lfa_1=pd.read_excel('LFA1 - Vendor Master.xlsx')\n",
    "    lfa_1[\"Supplier\"] = lfa_1[\"Supplier\"].astype(str)\n",
    "    lfa_1_1=lfa_1[['Supplier','DelF','DeBl','B','B.1','Status']]\n",
    "    lfa_1_2 = lfa_1_1.rename(columns={\n",
    "        \"Supplier\": \"vendor_or_creditor_acct_no\",\n",
    "        'DelF':\"central_deletion_flag\",\n",
    "        \"DeBl\": \"central_del_block_flg\",\n",
    "        \"B\": \"central_posting_blk_flag\",\n",
    "        \"B.1\": \"central_purch_blk_flag\",\n",
    "        \"Status\": \"data_transfer_status\",\n",
    "    })\n",
    "    \n",
    "    KEY = \"vendor_or_creditor_acct_no\"\n",
    "    cols_to_update = [\"central_deletion_flag\",\n",
    "        \"central_del_block_flg\",\n",
    "        \"central_posting_blk_flag\",\n",
    "        \"central_purch_blk_flag\",\n",
    "        \"data_transfer_status\",\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    vendor= vendor_data.drop(columns=cols_to_update)\n",
    "    \n",
    "    \n",
    "    # Merge and overwrite columns\n",
    "    merged = vendor.merge(\n",
    "        lfa_1_2,\n",
    "        on=KEY,\n",
    "        how=\"outer\")\n",
    "    vendor_data_updated = merged\n",
    "    # drop all rows where all values are Nan\n",
    "    vendor_data_1 = vendor_data_updated.dropna(axis=1, how='all')\n",
    "    po_invoice_vendor=pd.merge(po_invoice,vendor_data_1,left_on='vendor_or_creditor_acct_no_hpd_po',right_on='vendor_or_creditor_acct_no',how='outer')\n",
    "    \n",
    "    ## invoice reversed\n",
    "    po_invoice_vendor[\"invoice_reversed\"] = np.where(\n",
    "        po_invoice_vendor[\"reversal_doc_no_hpd_invoice\"].isnull(), 0, 1\n",
    "    )\n",
    "    \n",
    "    # ---------- 0) Make sure inputs are numeric ----------\n",
    "    for col in [\"amt_doc_curr_src_invoice\", \"quantity_src_invoice\",\n",
    "                \"net_val_po_curr_src_po\", \"quantity_src_po\"]:\n",
    "        po_invoice_vendor[col] = pd.to_numeric(po_invoice_vendor[col], errors=\"coerce\")\n",
    "    \n",
    "    # ---------- 1) Build the gate ----------\n",
    "    if \"reversal_doc_no_hpd_invoice\" in po_invoice_vendor.columns:\n",
    "        inv_rev_ok = po_invoice_vendor[\"reversal_doc_no_hpd_invoice\"].isna()\n",
    "    elif \"invoice_reversed\" in po_invoice_vendor.columns:\n",
    "        inv_rev_ok = po_invoice_vendor[\"invoice_reversed\"].fillna(0).eq(0)\n",
    "    else:\n",
    "        inv_rev_ok = True\n",
    "    \n",
    "    gate = (\n",
    "        (po_invoice_vendor[\"release_indicator_hpd_po\"] == \"R\") &\n",
    "        (po_invoice_vendor[\"po_item_del_flag_src_po\"].isna()) &\n",
    "        inv_rev_ok\n",
    "    )\n",
    "    \n",
    "    KEY = \"purch_doc_mapping_invoice\"\n",
    "    \n",
    "    # ---------- 2) Work only on gated rows ----------\n",
    "    cols_needed = [KEY, \"amt_doc_curr_src_invoice\", \"quantity_src_invoice\",\n",
    "                   \"net_val_po_curr_src_po\", \"quantity_src_po\"]\n",
    "    gated = po_invoice_vendor.loc[gate, cols_needed].copy()\n",
    "    \n",
    "    # group totals ONLY from gated rows\n",
    "    gated[\"invoice_total_amount\"]   = gated.groupby(KEY)[\"amt_doc_curr_src_invoice\"].transform(\"sum\")\n",
    "    gated[\"invoice_total_quantity\"] = gated.groupby(KEY)[\"quantity_src_invoice\"].transform(\"sum\")\n",
    "    \n",
    "    # pick PO references within gated rows (use max to be conservative)\n",
    "    gated[\"po_value_ref\"] = gated.groupby(KEY)[\"net_val_po_curr_src_po\"].transform(\"max\")\n",
    "    gated[\"po_qty_ref\"]   = gated.groupby(KEY)[\"quantity_src_po\"].transform(\"max\")\n",
    "    \n",
    "    # breaches & flag on gated rows\n",
    "    gated[\"invoice_more_than_po_flag\"] = (\n",
    "        (gated[\"invoice_total_amount\"]   > gated[\"po_value_ref\"]) |\n",
    "        (gated[\"invoice_total_quantity\"] > gated[\"po_qty_ref\"])\n",
    "    ).astype(int)\n",
    "    \n",
    "    # if any row in a gated group breaches -> mark ALL gated rows in that group\n",
    "    gated[\"invoice_more_than_po_flag\"] = gated.groupby(KEY)[\"invoice_more_than_po_flag\"].transform(\"max\")\n",
    "    \n",
    "    # ---------- 3) Map results back; non-gated rows remain 0 ----------\n",
    "    po_invoice_vendor[\"invoice_total_amount\"]   = 0.0\n",
    "    po_invoice_vendor[\"invoice_total_quantity\"] = 0.0\n",
    "    po_invoice_vendor[\"invoice_more_than_po_flag\"] = 0\n",
    "    \n",
    "    po_invoice_vendor.loc[gated.index, \"invoice_total_amount\"]   = gated[\"invoice_total_amount\"].values\n",
    "    po_invoice_vendor.loc[gated.index, \"invoice_total_quantity\"] = gated[\"invoice_total_quantity\"].values\n",
    "    po_invoice_vendor.loc[gated.index, \"invoice_more_than_po_flag\"] = gated[\"invoice_more_than_po_flag\"].values\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    label_data=pd.read_excel(\"Rulewise summary - 6 rules.xlsx\")\n",
    "    one=pd.merge(po_invoice_vendor,label_data,left_on='base_id_src_po',right_on='base_id',how='left')\n",
    "    \n",
    "    \n",
    "    \n",
    "    one_1=one[['purch_doc_no_src_po', 'purch_doc_item_no_src_po', 'pr_no_src_po',\n",
    "           'pr_item_no_src_po', 'requester_name_src_po', 'doc_change_date_src_po',\n",
    "           'company_code_src_po', 'p2o_unit_conv_denom_src_po',\n",
    "           'o2b_unit_conv_denom_src_po', 'p2o_unit_conv_num_src_po',\n",
    "           'o2b_unit_conv_num_src_po', 'po_item_del_flag_src_po',\n",
    "           'gr_indicator_src_po', 'gr_invoice_verif_flag_src_po',\n",
    "           'gross_val_po_curr_src_po', 'inv_receipt_indicator_src_po',\n",
    "           'material_type_src_po', 'material_no_src_po', 'matl_group_src_po',\n",
    "           'net_price_doc_curr_src_po', 'net_val_po_curr_src_po',\n",
    "           'order_uom_src_po', 'quantity_src_po', 'plant_src_po',\n",
    "           'short_text_src_po', 'target_qty_src_po',\n",
    "           'outline_agrmt_tgt_val_doc_curr_src_po', 'reqmt_tracking_no_src_po',\n",
    "           'principal_purch_agrmt_item_no_src_po', 'base_id_src_po',\n",
    "           'purch_doc_no_hpd_po', 'vendor_or_creditor_acct_no_hpd_po',\n",
    "           'purch_doc_category_hpd_po', 'company_code_hpd_po', 'currency_hpd_po',\n",
    "           'object_created_by_hpd_po', 'doc_change_date_hpd_po',\n",
    "           'control_indicator_hpd_po', 'purch_doc_date_hpd_po',\n",
    "           'principal_purch_agrmt_no_hpd_po', 'exchange_rate_hpd_po',\n",
    "           'purch_group_hpd_po', 'purch_org_hpd_po', 'pymnt_terms_hpd_po',\n",
    "           'processing_status_hpd_po', 'doc_release_incompl_flag_hpd_po',\n",
    "           'release_indicator_hpd_po', 'release_status_hpd_po',\n",
    "           'rfq_status_hpd_po', 'resp_vendor_salesperson_hpd_po',\n",
    "           'on_release_total_value_hpd_po', 'purch_doc_type_hpd_po','vendor_or_creditor_acct_no', 'country_code', 'vendor_name_1',\n",
    "           'vendor_name_2', 'vendor_name_3', 'vendor_name_4', 'city',\n",
    "           'postal_code', 'street_address', 'item_manual_addr_no',\n",
    "           'matchcode_search_term_1', 'record_creation_dt', 'object_created_by',\n",
    "           'vendor_acct_group',\n",
    "           'tax_no_1', 'vendor_telephone_no', 'second_telephone_no', 'tax_no_3',\n",
    "           'tax_no_5','central_deletion_flag', 'central_purch_blk_flag','central_posting_blk_flag','data_transfer_status',\n",
    "               \"invoice_more_than_po_flag\"]]\n",
    "    \n",
    "    #one_1.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #rule_2\n",
    "    # ensure the flag is numeric 0/1 (or numeric)\n",
    "    one_1[\"invoice_more_than_po_flag\"] = (\n",
    "        pd.to_numeric(one_1[\"invoice_more_than_po_flag\"], errors=\"coerce\")\n",
    "          .fillna(0).astype(int)\n",
    "    )\n",
    "    \n",
    "    # keep rows where the flag equals the group's max; drop the rest\n",
    "    grp_max = one_1.groupby(\"base_id_src_po\")[\"invoice_more_than_po_flag\"].transform(\"max\")\n",
    "    one_1 = one_1.loc[\n",
    "        one_1[\"invoice_more_than_po_flag\"].eq(grp_max)\n",
    "    ].copy()\n",
    "    one_1 = (one_1\n",
    "             .sort_values([\"base_id_src_po\", \"invoice_more_than_po_flag\"], ascending=[True, False])\n",
    "             .drop_duplicates(\"base_id_src_po\", keep=\"first\"))\n",
    "    \n",
    "    \n",
    "    # rule label output\n",
    "    #line_item=pd.read_excel(\"baldota rule label data/Line Item Transaction Summary.xlsx\")\n",
    "    rft=pd.read_csv(r\"C:\\Users\\ajayn\\Downloads\\FortifAI_NEW\\Codes\\New Codes\\Model pipeline\\08-07-25\\13-07-2025\\ML Codes\\baldota rule label data\\Line Item Transaction Summary 24_07.csv\")\n",
    "    ## taking output po label at line item\n",
    "    rft_po=rft[rft['stage']=='PO']\n",
    "    one_2=one_1[one_1['base_id_src_po'].notna()].copy()\n",
    "    ## merging main data df with label data df\n",
    "    one_3=pd.merge(one_2,rft_po[['base_id','rule_ids','rft_by_engine']],left_on='base_id_src_po',right_on='base_id',how=\"left\")\n",
    "    #one_3.head()\n",
    "    ## rule_3\n",
    "    block_cols = [\"central_deletion_flag\", \"central_purch_blk_flag\", \"central_posting_blk_flag\"]\n",
    "    \n",
    "    one_3[\"po_to_blocked_vendor\"] = np.where(\n",
    "        (one_3[block_cols].eq(\"X\").any(axis=1)) &\n",
    "        (one_3[\"release_indicator_hpd_po\"] == \"R\") &\n",
    "        (one_3[\"po_item_del_flag_src_po\"].isna()),\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ## rule_4\n",
    "    # Ensure both columns are datetime\n",
    "    one_3[\"purch_doc_date_hpd_po\"] = pd.to_datetime(one_3[\"purch_doc_date_hpd_po\"], errors=\"coerce\")\n",
    "    one_3[\"record_creation_dt\"] = pd.to_datetime(one_3[\"record_creation_dt\"], errors=\"coerce\")\n",
    "    \n",
    "    # Create flag: 1 if purch_doc_date_hpd_po is within 30 days of record_creation_dt\n",
    "    one_3[\"purch_doc_within_30days\"] = (\n",
    "        (one_3[\"purch_doc_date_hpd_po\"] - one_3[\"record_creation_dt\"]).abs().dt.days <= 30\n",
    "    ).astype(int)\n",
    "    \n",
    "    \n",
    "    # Ensure numeric types\n",
    "    one_3[\"on_release_total_value_hpd_po\"] = pd.to_numeric(one_3[\"on_release_total_value_hpd_po\"], errors=\"coerce\")\n",
    "    one_3[\"exchange_rate_hpd_po\"] = pd.to_numeric(one_3[\"exchange_rate_hpd_po\"], errors=\"coerce\")\n",
    "    \n",
    "    # Calculate PO value in INR (or base currency)\n",
    "    one_3[\"po_value_inr\"] = one_3[\"on_release_total_value_hpd_po\"] * one_3[\"exchange_rate_hpd_po\"]\n",
    "    \n",
    "    # Create flag: 1 if > 1 crore (1 Cr = 10,000,000)\n",
    "    one_3[\"po_gt_1cr_flag\"] = np.where(one_3[\"po_value_inr\"] > 1e7, 1, 0)\n",
    "    \n",
    "    one_3[\"po_to_new_vendor_gt_tolerance\"] = np.where(\n",
    "        (one_3[\"purch_doc_within_30days\"] == 1) & (one_3[\"po_gt_1cr_flag\"] == 1) & (one_3[\"release_indicator_hpd_po\"] == 'R')\n",
    "        & (one_3[\"po_item_del_flag_src_po\"].isna()),\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    ## rule_5\n",
    "    df = one_3.copy() # or your dataframe name\n",
    "    \n",
    "    # 1) Extract PAN: remove first 2 chars and last 3 chars, clean up casing/whitespace\n",
    "    df[\"pan_extracted\"] = (\n",
    "        df[\"tax_no_3\"]\n",
    "          .astype(str)\n",
    "          .str.strip()\n",
    "          .str.upper()\n",
    "          .str[2:-3]                 # remove first 2 and last 3\n",
    "    )\n",
    "    \n",
    "    # (Optional) Keep only plausible PANs (10 chars, alphanumeric)\n",
    "    pan_valid = df[\"pan_extracted\"].str.len().eq(10) & df[\"pan_extracted\"].str.isalnum()\n",
    "    df.loc[~pan_valid, \"pan_extracted\"] = np.nan\n",
    "    \n",
    "    # 2) Amount in INR (or base) and >= 1 Cr flag\n",
    "    df[\"on_release_total_value_hpd_po\"] = pd.to_numeric(df[\"on_release_total_value_hpd_po\"], errors=\"coerce\")\n",
    "    df[\"exchange_rate_hpd_po\"] = pd.to_numeric(df[\"exchange_rate_hpd_po\"], errors=\"coerce\")\n",
    "    df[\"po_value_inr\"] = df[\"on_release_total_value_hpd_po\"] * df[\"exchange_rate_hpd_po\"]\n",
    "    \n",
    "    amount_ge_1cr = df[\"po_value_inr\"] >= 1e7  # 1 crore = 10,000,000\n",
    "    \n",
    "    # 3) 4th PAN character == 'P'\n",
    "    fourth_char_is_P = df[\"pan_extracted\"].str[3].eq(\"P\")\n",
    "    \n",
    "    # 4) Final flag: 4th PAN char 'P' AND amount ‚â• 1 Cr  -> 1 else 0\n",
    "    df[\"pan4P_amt_ge_1cr_flag\"] = np.where(fourth_char_is_P & amount_ge_1cr &  (df[\"release_indicator_hpd_po\"] == 'R')\n",
    "        & (df[\"po_item_del_flag_src_po\"].isna()), 1, 0)\n",
    "    \n",
    "    ##rule_6\n",
    "    df[\"missing_tax_id_flag\"] = np.where(df[\"tax_no_3\"].isna() &  (df[\"release_indicator_hpd_po\"] == 'R')\n",
    "        & (df[\"po_item_del_flag_src_po\"].isna()), 1, 0)\n",
    "    \n",
    "    \n",
    "    one_3=df.copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1) Flag: True if either column is true\n",
    "    # List your 5 columns\n",
    "    cols_to_check = [\"invoice_more_than_po_flag\", \"po_to_blocked_vendor\", \"po_to_new_vendor_gt_tolerance\", \"pan4P_amt_ge_1cr_flag\",\"missing_tax_id_flag\"]\n",
    "    \n",
    "    # output_flag will be True if any column == 1, else False\n",
    "    one_3[\"new_model_output\"] = df[cols_to_check].eq(1).any(axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # List the columns to check\n",
    "    cols_to_check = [\"invoice_more_than_po_flag\", \"po_to_blocked_vendor\", \"po_to_new_vendor_gt_tolerance\", \"pan4P_amt_ge_1cr_flag\",\"missing_tax_id_flag\"]\n",
    "    \n",
    "    # Build rule_summary by joining column names where value == 1\n",
    "    one_3[\"new_rule_summary\"] = (\n",
    "        df[cols_to_check]\n",
    "        .apply(lambda row: \",\".join(row.index[row.eq(1)]), axis=1)\n",
    "    )\n",
    "    \n",
    "    # If you prefer NaN instead of empty string when no rules matched:\n",
    "    # df[\"rule_summary\"] = df[\"rule_summary\"].replace(\"\", np.nan)\n",
    "    #one_3.head()\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        to_bool\n",
    "    except NameError:\n",
    "        TRUEY = {\"true\",\"t\",\"yes\",\"y\",\"1\",\"ok\",\"on\"}\n",
    "        def to_bool(x):\n",
    "            if pd.isna(x):\n",
    "                return False\n",
    "            if isinstance(x, (bool, np.bool_)):\n",
    "                return bool(x)\n",
    "            if isinstance(x, (int, np.integer, float, np.floating)):\n",
    "                # treat 1 or True-like as True\n",
    "                return x == 1 or x is True\n",
    "            return str(x).strip().lower() in TRUEY\n",
    "    \n",
    "    try:\n",
    "        split_rules\n",
    "    except NameError:\n",
    "        def split_rules(val):\n",
    "            if pd.isna(val):\n",
    "                return []\n",
    "            if isinstance(val, (list, tuple, set)):\n",
    "                parts = list(val)\n",
    "            else:\n",
    "                # accept commas/semicolons\n",
    "                parts = [p.strip() for chunk in str(val).split(';') for p in chunk.split(',')]\n",
    "            parts = [p for p in parts if p and p.lower() != \"nan\"]\n",
    "            # de-dupe preserve order\n",
    "            seen, out = set(), []\n",
    "            for p in parts:\n",
    "                if p not in seen:\n",
    "                    seen.add(p); out.append(p)\n",
    "            return out\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1) Flag: True if either column is true\n",
    "    one_3[\"rft_by_engine_7\"] = (\n",
    "        one_3[\"rft_by_engine\"].map(to_bool) | one_3[\"new_model_output\"].map(to_bool)\n",
    "    )\n",
    "    \n",
    "    # (Optional) whitelist to avoid free-text becoming a \"rule\"\n",
    "    ALLOWED_RULES = None  # e.g., {\"invoice_before_po_flag\", \"invoice_more_than_po_flag\"}\n",
    "    \n",
    "    def merge_rules_guarded(row):\n",
    "        # Build candidate rules from both columns\n",
    "        parts = split_rules(row.get(\"new_rule_summary\")) + split_rules(row.get(\"rule_ids\"))\n",
    "        parts = list(dict.fromkeys(parts))  # de-dupe, keep order\n",
    "        if ALLOWED_RULES is not None:\n",
    "            parts = [p for p in parts if p in ALLOWED_RULES]\n",
    "    \n",
    "        # ENFORCE: if flag is False -> no rules\n",
    "        if not to_bool(row.get(\"rft_by_engine_7\", False)):\n",
    "            return []\n",
    "    \n",
    "        return parts\n",
    "    \n",
    "    # Build final columns with gating\n",
    "    one_3[\"rule_ids_7_list\"] = one_3.apply(merge_rules_guarded, axis=1)\n",
    "    one_3[\"rule_ids_7\"] = one_3[\"rule_ids_7_list\"].apply(lambda lst: \",\".join(lst) if lst else np.nan)\n",
    "    \n",
    "    \n",
    "    # Step 1: Get unique rule IDs from all rows (comma-separated strings)\n",
    "    rule_sets = one_3['rule_ids_7'].dropna().apply(lambda x: [r.strip() for r in x.split(',')])\n",
    "    unique_rules = sorted(set(r for sublist in rule_sets for r in sublist))\n",
    "    \n",
    "    # Step 2: Create columns for each rule ID with 1 if present, else 0\n",
    "    for rule in unique_rules:\n",
    "        one_3[rule] = one_3['rule_ids_7'].apply(lambda x: int(rule in x.split(',')) if pd.notna(x) else 0)\n",
    "    \n",
    "    #Invoice before PO date\n",
    "    #Invoice Exceeds PO\n",
    "    # Final rule to sub-risk mapping\n",
    "    def get_sub_risks(row):\n",
    "        rule_to_subrisk = {\"P2P02067\": \"Price Variance Risk\",\n",
    "            \"P2P02068\": \"Price Variance Risk\",\n",
    "            \"P2P02070\": \"Split PO\",\n",
    "            \"P2P02072\": \"Split PO\",\n",
    "            \"invoice_more_than_po_flag\": \"Invoice Exceeds PO\",\n",
    "            \"po_to_blocked_vendor\":\"PO to block vendor\",\n",
    "            \"po_to_new_vendor_gt_tolerance\":\"PO to new vendor > tolerance level\",\n",
    "            \"pan4P_amt_ge_1cr_flag\":\"PO to Non Company Vendors\",\n",
    "            \"missing_tax_id_flag\":\"PO to Vendor with missing KYC\",\n",
    "                           \n",
    "                           \n",
    "            #rule_1\": \"Invoice before PO date\",\n",
    "        }\n",
    "        risks = {rule_to_subrisk[rule] for rule in rule_to_subrisk if row.get(rule, 0) == 1}\n",
    "        return list(risks) if risks else [\"No Risk\"]\n",
    "    def sub_risk(df):    \n",
    "        \n",
    "        # Step 1: Assign Main Risk Scenario\n",
    "        df[\"main_risk_scenario\"] = df[\"rft_by_engine_7\"].apply(\n",
    "            lambda x: \"Procurement Risk\" if x else \"No Risk\"\n",
    "        )\n",
    "        \n",
    "        # Step 2: Generate clean list of sub risks\n",
    "        #def get_sub_risks(row):\n",
    "            #risks = {rule_to_subrisk[rule] for rule in rule_to_subrisk if row.get(rule, 0) == 1}\n",
    "            #return list(risks) if risks else [\"No Risk\"]\n",
    "        \n",
    "        df[\"sub_risks\"] = df.apply(get_sub_risks, axis=1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    invoice_data_2=sub_risk(one_3)\n",
    "    #invoice_data_2.head()\n",
    "    \n",
    "    \n",
    "    \n",
    "    po_data_2=invoice_data_2[['purch_doc_no_src_po', 'purch_doc_item_no_src_po', 'pr_no_src_po',\n",
    "           'pr_item_no_src_po', 'requester_name_src_po', 'doc_change_date_src_po',\n",
    "           'company_code_src_po', 'p2o_unit_conv_denom_src_po',\n",
    "           'o2b_unit_conv_denom_src_po', 'p2o_unit_conv_num_src_po',\n",
    "           'o2b_unit_conv_num_src_po', 'po_item_del_flag_src_po',\n",
    "           'gr_indicator_src_po', 'gr_invoice_verif_flag_src_po',\n",
    "           'gross_val_po_curr_src_po', 'inv_receipt_indicator_src_po',\n",
    "           'material_type_src_po', 'material_no_src_po', 'matl_group_src_po',\n",
    "           'net_price_doc_curr_src_po', 'net_val_po_curr_src_po',\n",
    "           'order_uom_src_po', 'quantity_src_po', 'plant_src_po',\n",
    "           'short_text_src_po', 'target_qty_src_po',\n",
    "           'outline_agrmt_tgt_val_doc_curr_src_po', 'reqmt_tracking_no_src_po',\n",
    "           'principal_purch_agrmt_item_no_src_po', 'base_id_src_po',\n",
    "           'purch_doc_no_hpd_po', 'vendor_or_creditor_acct_no_hpd_po',\n",
    "           'purch_doc_category_hpd_po', 'company_code_hpd_po', 'currency_hpd_po',\n",
    "           'object_created_by_hpd_po', 'doc_change_date_hpd_po',\n",
    "           'control_indicator_hpd_po', 'purch_doc_date_hpd_po',\n",
    "           'principal_purch_agrmt_no_hpd_po', 'exchange_rate_hpd_po',\n",
    "           'purch_group_hpd_po', 'purch_org_hpd_po', 'pymnt_terms_hpd_po',\n",
    "           'processing_status_hpd_po', 'doc_release_incompl_flag_hpd_po',\n",
    "           'release_indicator_hpd_po', 'release_status_hpd_po',\n",
    "           'rfq_status_hpd_po', 'resp_vendor_salesperson_hpd_po',\n",
    "                                  'on_release_total_value_hpd_po', 'purch_doc_type_hpd_po',\n",
    "           'vendor_or_creditor_acct_no', 'country_code', 'vendor_name_1',\n",
    "           'vendor_name_2', 'vendor_name_3', 'vendor_name_4', 'city',\n",
    "           'postal_code', 'street_address', 'item_manual_addr_no',\n",
    "           'matchcode_search_term_1', 'record_creation_dt', 'object_created_by',\n",
    "           'vendor_acct_group', 'tax_no_1', 'vendor_telephone_no',\n",
    "           'second_telephone_no', 'tax_no_3', 'tax_no_5', 'central_deletion_flag',\n",
    "           'central_purch_blk_flag', 'central_posting_blk_flag',\n",
    "           'data_transfer_status', 'invoice_more_than_po_flag', \n",
    "           'po_to_blocked_vendor', 'purch_doc_within_30days', 'po_value_inr',\n",
    "           'po_gt_1cr_flag', 'po_to_new_vendor_gt_tolerance', 'pan_extracted',\n",
    "           'pan4P_amt_ge_1cr_flag', 'missing_tax_id_flag','rft_by_engine_7','main_risk_scenario', 'sub_risks']]\n",
    "    \n",
    "    #po_data_2.head()po_invoice\n",
    "    #po_data_2.to_pickle('po_data_02_10.pkl')\n",
    "    return po_data_2\n",
    "\n",
    "\n",
    "COL_MAP: Dict[str, str] = {\n",
    "    # raw_column                          # internal name\n",
    "    \"vendor_or_creditor_acct_no_hpd_po\": \"vendor_id\",\n",
    "    \"material_no_src_po\": \"material_id\",\n",
    "    \"purch_doc_date_hpd_po\": \"po_date\",\n",
    "    \"doc_change_date_src_po\": \"po_change_date\",\n",
    "    \"net_price_doc_curr_src_po\": \"net_price\",\n",
    "    \"gross_val_po_curr_src_po\": \"gross_val\",\n",
    "    \"exchange_rate_hpd_po\": \"exch_rate\",\n",
    "    \"requester_name_src_po\": \"requester\",\n",
    "    # unit‚Äëconversion numerators / denominators\n",
    "    \"p2o_unit_conv_num_src_po\": \"p2o_num\",\n",
    "    \"p2o_unit_conv_denom_src_po\": \"p2o_den\",\n",
    "    \"o2b_unit_conv_num_src_po\": \"o2b_num\",\n",
    "    \"o2b_unit_conv_denom_src_po\": \"o2b_den\",\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Feature engineering functions (pure, chainable)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _prep(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Rename key columns, parse dates & fill obvious nulls.\"\"\"\n",
    "    #df = df.rename(columns={k: v for k, v in COL_MAP.items() if k in df.columns})\n",
    "    df[\"purch_doc_date_hpd_po\"] = pd.to_datetime(df[\"purch_doc_date_hpd_po\"], errors=\"coerce\")\n",
    "    df[\"doc_change_date_src_po\"] = pd.to_datetime(df.get(\"doc_change_date_src_po\"), errors=\"coerce\")\n",
    "    df[\"vendor_or_creditor_acct_no_hpd_po\"] = df.get(\"vendor_or_creditor_acct_no_hpd_po\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"requester_name_src_po\"] = df.get(\"requester_name_src_po\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"exchange_rate_hpd_po\"] = df.get(\"exchange_rate_hpd_po\", 1.0).replace({0: np.nan}).fillna(1.0)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2a) Rule‚Äëbased features (rules¬†1,2,3,5)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def add_rule_metrics(df: pd.DataFrame,\n",
    "                     split_days: int = 60,\n",
    "                     price_var_days: int = 365) -> pd.DataFrame:\n",
    "    \"\"\"Add features mirroring Baldota P2P rules.\n",
    "\n",
    "    * vm_count/value = aggregation for **same vendor+material** within *split_days*\n",
    "    * vm_price_var_pct = price deviation (%) vs mean of past *price_var_days*\n",
    "    * mat_vendor_cnt & mat_price_var_pct analogues for material across vendors\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.sort_values(\"purch_doc_date_hpd_po\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Initialise\n",
    "    df[\"vm_count_%dd\" % split_days] = 0\n",
    "    df[\"vm_value_%dd\" % split_days] = 0.0\n",
    "    df[\"vm_price_var_pct_%dd\" % price_var_days] = 0.0\n",
    "    df[\"mat_vendor_cnt_%dd\" % split_days] = 0\n",
    "    df[\"mat_price_var_pct_%dd\" % price_var_days] = 0.0\n",
    "\n",
    "    # Pre‚Äëextract convenient arrays for speed\n",
    "    po_dates = df[\"purch_doc_date_hpd_po\"].values\n",
    "    prices = df.get(\"net_price_doc_curr_src_po\").astype(float).values\n",
    "    vals = df.get(\"gross_val_po_curr_src_po\").astype(float).values\n",
    "\n",
    "    # --- Same vendor + material group logic ----------------------------------\n",
    "    for (v, m), idx in df.groupby([\"vendor_or_creditor_acct_no_hpd_po\", \"material_no_src_po\"]).groups.items():\n",
    "        d = po_dates[idx]\n",
    "        p = prices[idx]\n",
    "        vls = vals[idx]\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = d[loc]\n",
    "            # split window\n",
    "            win_mask = (d >= cur - np.timedelta64(split_days, \"D\")) & (d <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(\"vm_count_%dd\" % split_days)] = int(win_mask.sum())\n",
    "            df.iat[ridx, df.columns.get_loc(\"vm_value_%dd\" % split_days)] = float(vls[win_mask].sum())\n",
    "            # price variance window\n",
    "            var_mask = (d >= cur - np.timedelta64(price_var_days, \"D\")) & (d <= cur)\n",
    "            if var_mask.sum() > 1:\n",
    "                mean_price = p[var_mask].mean()\n",
    "                if mean_price:\n",
    "                    pct = abs(p[loc] - mean_price) / mean_price * 100\n",
    "                    df.iat[ridx, df.columns.get_loc(\"vm_price_var_pct_%dd\" % price_var_days)] = pct\n",
    "\n",
    "    # --- Material‚Äëonly group logic -------------------------------------------\n",
    "    for m, idx in df.groupby(\"material_no_src_po\").groups.items():\n",
    "        d = po_dates[idx]\n",
    "        p = prices[idx]\n",
    "        vendors = df.loc[idx, \"vendor_or_creditor_acct_no_hpd_po\"].values\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = d[loc]\n",
    "            win_mask = (d >= cur - np.timedelta64(split_days, \"D\")) & (d <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(\"mat_vendor_cnt_%dd\" % split_days)] = int(len(set(vendors[win_mask])))\n",
    "            var_mask = (d >= cur - np.timedelta64(price_var_days, \"D\")) & (d <= cur)\n",
    "            if var_mask.sum() > 1:\n",
    "                mean_price = p[var_mask].mean()\n",
    "                if mean_price:\n",
    "                    pct = abs(p[loc] - mean_price) / mean_price * 100\n",
    "                    df.iat[ridx, df.columns.get_loc(\"mat_price_var_pct_%dd\" % price_var_days)] = pct\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2b) Value & process metrics\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def add_value_and_timing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"conv_factor_p2o\"] = (\n",
    "        df.get(\"p2o_unit_conv_num_src_po\") / df.get(\"p2o_unit_conv_denom_src_po\")\n",
    "    ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "    df[\"conv_factor_o2b\"] = (\n",
    "        df.get(\"o2b_unit_conv_num_src_po\") / df.get(\"o2b_unit_conv_denom_src_po\")\n",
    "    ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "    df[\"po_change_lag_days\"] = (df.get(\"doc_change_date_src_po\") - df[\"purch_doc_date_hpd_po\"]).dt.days\n",
    "    df[\"base_value\"] = df.get(\"gross_val_po_curr_src_po\") * df.get(\"exchange_rate_hpd_po\")\n",
    "    p95 = df[\"gross_val_po_curr_src_po\"].quantile(0.95)\n",
    "    df[\"high_value_flag\"] = (df[\"gross_val_po_curr_src_po\"] >= p95).astype(int)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2c) Behavioural rolling windows\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _rolling_stats(df: pd.DataFrame, group_col: str, days: int,\n",
    "                   count_col: str, sum_col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[count_col] = 0\n",
    "    df[sum_col] = 0.0\n",
    "\n",
    "    po_dates = df[\"purch_doc_date_hpd_po\"].values\n",
    "    gross_vals = df[\"gross_val_po_curr_src_po\"].values\n",
    "\n",
    "    for key, idx in df.groupby(group_col).groups.items():\n",
    "        dates = po_dates[idx]\n",
    "        vals = gross_vals[idx]\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = dates[loc]\n",
    "            mask = (dates >= cur - np.timedelta64(days, \"D\")) & (dates <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(count_col)] = int(mask.sum())\n",
    "            df.iat[ridx, df.columns.get_loc(sum_col)] = float(vals[mask].sum())\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_behavioural_stats(df: pd.DataFrame, window_days: int = 30) -> pd.DataFrame:\n",
    "    df = _rolling_stats(df, \"requester_name_src_po\", window_days,\n",
    "                        \"req_po_count_%dd\" % window_days,\n",
    "                        \"req_val_sum_%dd\" % window_days)\n",
    "    df = _rolling_stats(df, \"vendor_or_creditor_acct_no_hpd_po\", window_days,\n",
    "                        \"vendor_po_count_%dd\" % window_days,\n",
    "                        \"vendor_val_sum_%dd\" % window_days)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Public orchestrator\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def build_features(raw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"End‚Äëto‚Äëend feature generator (no target leakage).\"\"\"\n",
    "    df = _prep(raw_df)\n",
    "    df = add_rule_metrics(df)\n",
    "    df = add_value_and_timing(df)\n",
    "    df = add_behavioural_stats(df)\n",
    "    return df\n",
    "def flag_split_po(df):\n",
    "    df = df.copy()\n",
    "    df['split_po_flag'] = 0  # Default 0\n",
    "\n",
    "    # Apply exclusion filters only for computation\n",
    "    exclude_doc_types = [\"AN\", \"AR\", \"MN\", \"QC\", \"QI\", \"QS\", \"RS\", \"SC\", \"SG\", \"SR\", \"SS\", \"ST\", \"TP\", \"TR\", \"UB\", \"WK\"]\n",
    "\n",
    "    filtered = df[\n",
    "        (~df['purch_doc_type_hpd_po'].isin(exclude_doc_types)) &\n",
    "        (df['purch_doc_type_hpd_po'].notna()) &\n",
    "        (~(df['po_item_del_flag_src_po'] == 'L')) &\n",
    "        (~df['plant_src_po'].fillna(\"\").astype(str).str.startswith(\"4\")) &\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['gross_val_po_curr_src_po'] >= 10) &\n",
    "        (df['purch_doc_date_hpd_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['company_code_src_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    filtered['purch_doc_date_hpd_po'] = pd.to_datetime(filtered['purch_doc_date_hpd_po'])\n",
    "\n",
    "    def get_group_key(row):\n",
    "        if pd.notna(row['material_no_src_po']):\n",
    "            return f\"{row['vendor_or_creditor_acct_no_hpd_po']}__{row['material_no_src_po']}\"\n",
    "        elif pd.notna(row['short_text_src_po']):\n",
    "            return f\"{row['vendor_or_creditor_acct_no_hpd_po']}__{row['short_text_src_po']}\"\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    filtered['split_key'] = filtered.apply(get_group_key, axis=1)\n",
    "    filtered = filtered[filtered['split_key'].notna()].copy()\n",
    "    filtered.sort_values(['split_key', 'purch_doc_date_hpd_po'], inplace=True)\n",
    "\n",
    "    flagged_po_set = set()\n",
    "\n",
    "    for key, group in filtered.groupby('split_key'):\n",
    "        dates = group['purch_doc_date_hpd_po'].reset_index(drop=True)\n",
    "        po_nos = group['purch_doc_no_src_po'].reset_index(drop=True)\n",
    "\n",
    "        for i in range(len(dates)):\n",
    "            date_i = dates[i]\n",
    "            po_i = po_nos[i]\n",
    "            mask = (\n",
    "                (dates >= date_i - pd.Timedelta(days=14)) &\n",
    "                (dates <= date_i + pd.Timedelta(days=14)) &\n",
    "                (po_nos != po_i)\n",
    "            )\n",
    "            if mask.sum() > 0:\n",
    "                flagged_po_set.add(po_i)\n",
    "\n",
    "    # Assign flag only to matching rows in original df\n",
    "    df['split_po_flag'] = df['purch_doc_no_src_po'].isin(flagged_po_set).astype(int)\n",
    "    return df\n",
    "def flag_intra_po_split(df, gross_threshold=10):\n",
    "    df = df.copy()\n",
    "    df['intra_po_split_flag'] = 0  # Default\n",
    "\n",
    "    df_valid = df[\n",
    "        df['gross_val_po_curr_src_po'].notna() &\n",
    "        df['vendor_or_creditor_acct_no_hpd_po'].notna() &\n",
    "        df['material_no_src_po'].notna() & \n",
    "        df['purch_doc_no_src_po'].notna() \n",
    "    \n",
    "    ].copy()\n",
    "\n",
    "    group_cols = ['purch_doc_no_src_po', 'vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po']\n",
    "    grouped = df_valid.groupby(group_cols)\n",
    "\n",
    "    flagged_indexes = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        total_gross = group['gross_val_po_curr_src_po'].sum()\n",
    "        num_items = len(group)\n",
    "        all_below_threshold = group['gross_val_po_curr_src_po'].all()\n",
    "\n",
    "        if total_gross >= gross_threshold and num_items > 1 and all_below_threshold:\n",
    "            flagged_indexes.extend(group.index.tolist())\n",
    "\n",
    "    df.loc[flagged_indexes, 'intra_po_split_flag'] = 1\n",
    "    return df\n",
    "def flag_multiple_pos_per_pr_item(df):\n",
    "    df = df.copy()\n",
    "    df['multi_po_per_pr_flag'] = 0  # Default\n",
    "\n",
    "    # Only consider approved POs with valid PR and PR item\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        df['pr_no_src_po'].notna() &\n",
    "        df['pr_item_no_src_po'].notna() &\n",
    "        df['purch_doc_no_src_po'].notna()\n",
    "    ][['pr_no_src_po', 'pr_item_no_src_po', 'purch_doc_no_src_po']].drop_duplicates()\n",
    "\n",
    "    # Count number of unique POs per PR+Item\n",
    "    po_counts = df_valid.groupby(['pr_no_src_po', 'pr_item_no_src_po'])['purch_doc_no_src_po'].nunique()\n",
    "\n",
    "    # Identify PR+Items linked to more than one PO\n",
    "    multi_po_keys = po_counts[po_counts > 1].index.tolist()\n",
    "\n",
    "    # Create a set for fast lookup\n",
    "    multi_po_set = set(multi_po_keys)\n",
    "\n",
    "    # Flag in the main DataFrame\n",
    "    df['multi_po_per_pr_flag'] = df.apply(\n",
    "        lambda row: 1 if (row['pr_no_src_po'], row['pr_item_no_src_po']) in multi_po_set else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "def flag_same_vendor_price_increase(df, price_increase_threshold=0.05, months_range=6, flag_column='same_vendor_price_increase_flag'):\n",
    "    df = df.copy()\n",
    "    df[flag_column] = 0\n",
    "\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['material_no_src_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['net_price_doc_curr_src_po'].notna()) &\n",
    "        (df['purch_doc_date_hpd_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    df_valid['purch_doc_date_hpd_po'] = pd.to_datetime(df_valid['purch_doc_date_hpd_po'])\n",
    "    df_valid.sort_values(['vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po', 'purch_doc_date_hpd_po'], inplace=True)\n",
    "\n",
    "    group_cols = ['vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po']\n",
    "    flagged_indices = []\n",
    "\n",
    "    for _, group in df_valid.groupby(group_cols):\n",
    "        group = group.sort_values('purch_doc_date_hpd_po').reset_index()\n",
    "\n",
    "        for i in range(1, len(group)):\n",
    "            current_row = group.loc[i]\n",
    "            current_date = current_row['purch_doc_date_hpd_po']\n",
    "            current_price = current_row['net_price_doc_curr_src_po']\n",
    "\n",
    "            mask = group.loc[:i-1, 'purch_doc_date_hpd_po'] >= current_date - pd.DateOffset(months=months_range)\n",
    "            past_group = group.loc[:i-1][mask]\n",
    "\n",
    "            if not past_group.empty:\n",
    "                last_price = past_group['net_price_doc_curr_src_po'].iloc[-1]\n",
    "                if last_price > 0 and ((current_price - last_price) / last_price) >= price_increase_threshold:\n",
    "                    flagged_indices.append(current_row['index'])\n",
    "\n",
    "    df.loc[flagged_indices, flag_column] = 1\n",
    "    return df\n",
    "def flag_diff_vendor_price_variance(df, price_variance_threshold=0.05, months_range=6, flag_column='diff_vendor_price_variance_flag'):\n",
    "    df = df.copy()\n",
    "    df[flag_column] = 0\n",
    "\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['material_no_src_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['net_price_doc_curr_src_po'].notna()) &\n",
    "        (df['purch_doc_date_hpd_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    df_valid['purch_doc_date_hpd_po'] = pd.to_datetime(df_valid['purch_doc_date_hpd_po'])\n",
    "    df_valid = df_valid.sort_values(['material_no_src_po', 'purch_doc_date_hpd_po'])\n",
    "\n",
    "    for material, mat_group in df_valid.groupby('material_no_src_po'):\n",
    "        mat_group = mat_group.sort_values('purch_doc_date_hpd_po').reset_index()\n",
    "\n",
    "        for i in range(len(mat_group)):\n",
    "            current_row = mat_group.loc[i]\n",
    "            current_date = current_row['purch_doc_date_hpd_po']\n",
    "            current_price = current_row['net_price_doc_curr_src_po']\n",
    "\n",
    "            past_window = mat_group[\n",
    "                (mat_group['purch_doc_date_hpd_po'] < current_date) &\n",
    "                (mat_group['purch_doc_date_hpd_po'] >= current_date - pd.DateOffset(months=months_range))\n",
    "            ]\n",
    "\n",
    "            vendor_prices = past_window.groupby('vendor_or_creditor_acct_no_hpd_po')['net_price_doc_curr_src_po'].mean()\n",
    "            if not vendor_prices.empty:\n",
    "                max_price = vendor_prices.max()\n",
    "                min_price = vendor_prices.min()\n",
    "                if max_price > 0 and (max_price - min_price) / max_price >= price_variance_threshold:\n",
    "                    df.loc[current_row['index'], flag_column] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def final_parsing(df):\n",
    "        # === Parse Date Columns ===\n",
    "    date_cols = [\n",
    "        \"doc_change_date_src_po\",\n",
    "        \"doc_change_date_hpd_po\",\n",
    "        \"purch_doc_date_hpd_po\"\n",
    "    ]\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    \n",
    "    # === Feature Engineering ===\n",
    "    \n",
    "    # A. Date Features\n",
    "    df[\"po_doc_age_days\"] = (df[\"doc_change_date_hpd_po\"] - df[\"purch_doc_date_hpd_po\"]).dt.days\n",
    "    df[\"lead_time_po_vs_pr\"] = (df[\"purch_doc_date_hpd_po\"] - df[\"doc_change_date_src_po\"]).dt.days\n",
    "    df[\"po_day_of_week\"] = df[\"purch_doc_date_hpd_po\"].dt.dayofweek\n",
    "    df[\"po_day_of_month\"] = df[\"purch_doc_date_hpd_po\"].dt.day\n",
    "    df[\"po_month\"] = df[\"purch_doc_date_hpd_po\"].dt.month\n",
    "    \n",
    "    # B. Price & Value Features\n",
    "    df[\"price_per_unit\"] = df[\"net_val_po_curr_src_po\"] / df[\"quantity_src_po\"].replace(0, np.nan)\n",
    "    df[\"net_vs_gross_delta\"] = df[\"gross_val_po_curr_src_po\"] - df[\"net_val_po_curr_src_po\"]\n",
    "    df[\"price_variance_percent\"] = (df[\"net_val_po_curr_src_po\"] - df[\"gross_val_po_curr_src_po\"]) / df[\"gross_val_po_curr_src_po\"].replace(0, np.nan)\n",
    "    df[\"outline_agrmt_coverage\"] = df[\"outline_agrmt_tgt_val_doc_curr_src_po\"] / df[\"net_val_po_curr_src_po\"].replace(0, np.nan)\n",
    "    \n",
    "    # C. PO-PR Linkage Flags\n",
    "    df[\"has_pr_link\"] = df[\"pr_no_src_po\"].notna().astype(int)\n",
    "    df[\"has_pr_item_link\"] = df[\"pr_item_no_src_po\"].notna().astype(int)\n",
    "    \n",
    "    # D. Flags & Indicators\n",
    "    binary_cols = [\n",
    "        \"gr_indicator_src_po\", \"gr_invoice_verif_flag_src_po\",\n",
    "        \"inv_receipt_indicator_src_po\", \"release_indicator_hpd_po\",\n",
    "        \"release_status_hpd_po\", \"doc_release_incompl_flag_hpd_po\"\n",
    "    ]\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            df[col + \"_flag\"] = df[col].notna().astype(int)\n",
    "    \n",
    "    # E. Missing Signal\n",
    "    critical_cols = [\n",
    "        \"material_type_src_po\", \"material_no_src_po\",\n",
    "        \"vendor_or_creditor_acct_no_hpd_po\", \"gross_val_po_curr_src_po\",\n",
    "        \"net_price_doc_curr_src_po\"\n",
    "    ]\n",
    "    df[\"missing_critical_fields\"] = df[critical_cols].isnull().sum(axis=1)\n",
    "    \n",
    "    # F. Currency & Exchange Rate\n",
    "    df[\"has_exchange_rate\"] = df[\"exchange_rate_hpd_po\"].notna().astype(int)\n",
    "    df[\"log_exchange_rate\"] = np.log1p(df[\"exchange_rate_hpd_po\"].fillna(0))\n",
    "    \n",
    "    # G. Unit Conversion Features\n",
    "    df[\"p2o_conversion_ratio\"] = df[\"p2o_unit_conv_num_src_po\"] / df[\"p2o_unit_conv_denom_src_po\"].replace(0, np.nan)\n",
    "    df[\"o2b_conversion_ratio\"] = df[\"o2b_unit_conv_num_src_po\"] / df[\"o2b_unit_conv_denom_src_po\"].replace(0, np.nan)\n",
    "    \n",
    "    # H. Behavioral Flags\n",
    "    df[\"is_same_vendor_pr_po\"] = (\n",
    "        df[\"vendor_or_creditor_acct_no_hpd_po\"].notna() & df[\"base_id_src_po\"].notna()\n",
    "    ).astype(int)\n",
    "    df[\"has_rfq_status\"] = df[\"rfq_status_hpd_po\"].notna().astype(int)\n",
    "    df[\"purch_group_org_same\"] = (df[\"purch_group_hpd_po\"] == df[\"purch_org_hpd_po\"]).astype(int)\n",
    "    \n",
    "    # I. Rare Category Flagging\n",
    "    rare_cat_cols = [\"purch_doc_type_hpd_po\", \"purch_group_hpd_po\", \"vendor_or_creditor_acct_no_hpd_po\"]\n",
    "    for col in rare_cat_cols:\n",
    "        if col in df.columns:\n",
    "            freq_map = df[col].value_counts(normalize=True)\n",
    "            df[f\"{col}_is_rare\"] = df[col].map(freq_map) < 0.01\n",
    "\n",
    "    return df\n",
    "def _prep_invoice(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean and standardize invoice dataset.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # === Drop unwanted ===\n",
    "    drop_cols = [\"ingestion_timestamp\"]\n",
    "    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # === Date columns ===\n",
    "    date_cols = [\n",
    "        \"baseline_date_hpd_invoice\",\n",
    "        \"doc_date_hpd_invoice\",\n",
    "        \"doc_entry_date_hpd_invoice\",\n",
    "        \"entry_time_hpd_invoice\",\n",
    "        \"record_creation_dt\"\n",
    "    ]\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "    # === Fill Vendor & Company ===\n",
    "    df[\"vendor_or_creditor_acct_no_src_invoice\"] = df.get(\"vendor_or_creditor_acct_no_src_invoice\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"vendor_or_creditor_acct_no_hpd_invoice\"] = df.get(\"vendor_or_creditor_acct_no_hpd_invoice\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"company_code_src_invoice\"] = df.get(\"company_code_src_invoice\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "\n",
    "    # === Exchange rate ===\n",
    "    if \"exchange_rate_hpd_invoice\" in df.columns:\n",
    "        df[\"exchange_rate_hpd_invoice\"] = df[\"exchange_rate_hpd_invoice\"].replace({0: np.nan}).fillna(1.0)\n",
    "\n",
    "    # === Numeric cleaning ===\n",
    "    num_cols = [\n",
    "        \"amt_doc_curr_src_invoice\", \"quantity_src_invoice\",\n",
    "        \"po_qty_order_uom_src_invoice\", \"gross_inv_amt_doc_curr_hpd_invoice\",\n",
    "        \"unplanned_dlvry_costs_hpd_invoice\"\n",
    "    ]\n",
    "    for col in num_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def final_parsing_invoice(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Feature engineering for invoice dataset.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # A. Document Age\n",
    "    if \"doc_date_hpd_invoice\" in df.columns and \"baseline_date_hpd_invoice\" in df.columns:\n",
    "        df[\"invoice_doc_age_days\"] = (df[\"baseline_date_hpd_invoice\"] - df[\"doc_date_hpd_invoice\"]).dt.days\n",
    "\n",
    "    # B. Monetary Ratios\n",
    "    if {\"amt_doc_curr_src_invoice\", \"gross_inv_amt_doc_curr_hpd_invoice\"} <= set(df.columns):\n",
    "        df[\"invoice_net_vs_gross_delta\"] = df[\"gross_inv_amt_doc_curr_hpd_invoice\"] - df[\"amt_doc_curr_src_invoice\"]\n",
    "\n",
    "    if {\"amt_doc_curr_src_invoice\", \"quantity_src_invoice\"} <= set(df.columns):\n",
    "        df[\"invoice_price_per_unit\"] = df[\"amt_doc_curr_src_invoice\"] / df[\"quantity_src_invoice\"].replace(0, np.nan)\n",
    "\n",
    "    # C. Linkage Flags\n",
    "    df[\"has_po_link\"] = df[\"purch_doc_no_src_invoice\"].notna().astype(int)\n",
    "    df[\"has_po_item_link\"] = df[\"purch_doc_item_no_src_invoice\"].notna().astype(int)\n",
    "    df[\"has_vendor_link\"] = df[\"vendor_or_creditor_acct_no_src_invoice\"].notna().astype(int)\n",
    "\n",
    "    # D. Missing signal\n",
    "    critical_cols = [\"vendor_or_creditor_acct_no_src_invoice\", \"material_no_src_invoice\", \"amt_doc_curr_src_invoice\"]\n",
    "    df[\"missing_invoice_critical\"] = df[critical_cols].isnull().sum(axis=1)\n",
    "\n",
    "    # E. Rare category\n",
    "    if \"doc_type_hpd_invoice\" in df.columns:\n",
    "        freq_map = df[\"doc_type_hpd_invoice\"].value_counts(normalize=True)\n",
    "        df[\"doc_type_hpd_invoice_is_rare\"] = df[\"doc_type_hpd_invoice\"].map(freq_map) < 0.01\n",
    "\n",
    "    return df\n",
    "\n",
    "##############################################################################\n",
    "#  Cell 3 ‚Äî Create / load features\n",
    "##############################################################################\n",
    "if FEAT_PO.exists():\n",
    "    print(\"‚ö° Loading cached features\")\n",
    "    df = pd.read_pickle(FEAT_PO)\n",
    "else:\n",
    "    print(\"üöß Generating features ‚Ä¶\")\n",
    "    #raw_df  = pd.read_pickle(RAW_PO)\n",
    "    raw_df  = data_load_and_cleaning_invoice()\n",
    "    columns_to_drop = ['main_risk_scenario', 'sub_risks']\n",
    "    raw_df.drop(columns=[col for col in columns_to_drop if col in raw_df.columns], inplace=True)\n",
    "    #invoice_df = _prep_invoice(raw_df)\n",
    "    #invoice_df_2 = final_parsing_invoice(invoice_df)\n",
    "    feat_df = build_features(raw_df)\n",
    "    df=flag_split_po(feat_df)\n",
    "    df=flag_intra_po_split(df)\n",
    "    df = flag_multiple_pos_per_pr_item(df)\n",
    "    # Same vendor price jump\n",
    "    df = flag_same_vendor_price_increase(df, months_range=6, flag_column='same_vendor_price_increase_6m_flag')\n",
    "    df = flag_same_vendor_price_increase(df, months_range=12, flag_column='same_vendor_price_increase_12m_flag')\n",
    "    \n",
    "    # Diff vendor price variance\n",
    "    df = flag_diff_vendor_price_variance(df, months_range=6, flag_column='diff_vendor_price_variance_6m_flag')\n",
    "    df = flag_diff_vendor_price_variance(df, months_range=12, flag_column='diff_vendor_price_variance_12m_flag')\n",
    "    df=final_parsing(df)\n",
    "    df.to_pickle(FEAT_PO)\n",
    "    #print(\"Feature shape:\", df.shape)\n",
    "\n",
    "##############################################################################\n",
    "#  Cell 4 ‚Äî Fast train (no hyper-parameter search, no RFECV)\n",
    "##############################################################################\n",
    "# Keep only numerical features\n",
    "# Set random seeds for reproducibility\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Load and prepare data\n",
    "target = \"rft_by_engine_7\"\n",
    "df[target] = df[target].fillna(0).astype(int)\n",
    "y = df[target]\n",
    "X = df.drop(columns=[target])\n",
    "\n",
    "num_cols = [col for col in X.columns if X[col].dtype.kind in 'if']\n",
    "cat_cols = [col for col in X.columns if X[col].dtype.kind not in 'if']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ]), num_cols),\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, min_frequency=10))\n",
    "    ]), cat_cols)\n",
    "])\n",
    "\n",
    "X_scaled = preprocessor.fit_transform(X)\n",
    "X_train_unsupervised = X_scaled[y == 0]\n",
    "X_test = X_scaled\n",
    "y_test = y.values\n",
    "\n",
    "# Define Autoencoder as a dynamic class\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_1, hidden_2, bottleneck):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_1, hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_2, bottleneck)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck, hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_2, hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_1, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    hidden_1 = trial.suggest_int(\"hidden_1\", 64, 256)\n",
    "    hidden_2 = trial.suggest_int(\"hidden_2\", 16, 128)\n",
    "    bottleneck = trial.suggest_int(\"bottleneck\", 4, 32)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "    patience = 5\n",
    "\n",
    "    model = Autoencoder(X_train_unsupervised.shape[1], hidden_1, hidden_2, bottleneck)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train_unsupervised, dtype=torch.float32)\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "            batch = X_train_tensor[i:i+batch_size]\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / (X_train_tensor.size(0) // batch_size)\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_loss < best_loss - 1e-4:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    # Eval on full test\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(X_test_tensor)\n",
    "        recon_error = torch.mean((X_test_tensor - reconstructed) ** 2, dim=1).numpy()\n",
    "\n",
    "    threshold = np.percentile(recon_error[y_test == 0], 95)\n",
    "    preds = (recon_error > threshold).astype(int)\n",
    "    report = classification_report(y_test, preds, output_dict=True)\n",
    "    return report['1']['f1-score']  # Maximize fraud class F1\n",
    "\n",
    "# Run Optuna search\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683cea5f-817a-4e70-83a6-85a6ff9ee0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (py312env)",
   "language": "python",
   "name": "py312env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
