{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4c074-a951-456d-a0c1-eae6775de746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "##############################################################################\n",
    "#  Cell 1 â€” Imports & common paths\n",
    "##############################################################################\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import joblib\n",
    "import logging\n",
    "from typing import List\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import logging\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set options to show full DataFrame output\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import joblib, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from typing import Any, Dict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "RAW_PO      = Path(\"po_output.pkl\") #(\"../Po_Invoice_Data/po_output_tushar.pkl\")      # raw PO data (input)\n",
    "FEAT_PO     = Path(\"Po_Invoice_Data/po_output_features_df_risk.pkl\")         # engineered features\n",
    "#MODEL_PKL   = Path(\"Po_Invoice_Data/po_gbdt_model.pkl\")         # tuned model file\n",
    "#CV_REPORT   = Path(\"Po_Invoice_Data/cv_results.csv\")            # param grid results\n",
    "#SCORING_OUT = Path(\"Po_Invoice_Data/scored_po.pkl\")             # predictions file\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Column standardisation helper\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Final rule to sub-risk mapping\n",
    "def get_sub_risks(row):\n",
    "    rule_to_subrisk = {\n",
    "        \"P2P02067\": \"Price Variance Risk\",\n",
    "        \"P2P02068\": \"Price Variance Risk\",\n",
    "        \"P2P02070\": \"Split PO\",\n",
    "        \"P2P02072\": \"Split PO\"\n",
    "    }\n",
    "    risks = {rule_to_subrisk[rule] for rule in rule_to_subrisk if row.get(rule, 0) == 1}\n",
    "    return list(risks) if risks else [\"No Risk\"]\n",
    "def sub_risk(df):    \n",
    "    \n",
    "    # Step 1: Assign Main Risk Scenario\n",
    "    df[\"Main Risk Scenario\"] = df[\"rft_by_engine_po\"].apply(\n",
    "        lambda x: \"Procurement Risk\" if x else \"No Risk\"\n",
    "    )\n",
    "    \n",
    "    # Step 2: Generate clean list of sub risks\n",
    "    #def get_sub_risks(row):\n",
    "        #risks = {rule_to_subrisk[rule] for rule in rule_to_subrisk if row.get(rule, 0) == 1}\n",
    "        #return list(risks) if risks else [\"No Risk\"]\n",
    "    \n",
    "    df[\"Sub Risks\"] = df.apply(get_sub_risks, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "COL_MAP: Dict[str, str] = {\n",
    "    # raw_column                          # internal name\n",
    "    \"vendor_or_creditor_acct_no_hpd_po\": \"vendor_id\",\n",
    "    \"material_no_src_po\": \"material_id\",\n",
    "    \"purch_doc_date_hpd_po\": \"po_date\",\n",
    "    \"doc_change_date_src_po\": \"po_change_date\",\n",
    "    \"net_price_doc_curr_src_po\": \"net_price\",\n",
    "    \"gross_val_po_curr_src_po\": \"gross_val\",\n",
    "    \"exchange_rate_hpd_po\": \"exch_rate\",\n",
    "    \"requester_name_src_po\": \"requester\",\n",
    "    # unitâ€‘conversion numerators / denominators\n",
    "    \"p2o_unit_conv_num_src_po\": \"p2o_num\",\n",
    "    \"p2o_unit_conv_denom_src_po\": \"p2o_den\",\n",
    "    \"o2b_unit_conv_num_src_po\": \"o2b_num\",\n",
    "    \"o2b_unit_conv_denom_src_po\": \"o2b_den\",\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Feature engineering functions (pure, chainable)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _prep(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Rename key columns, parse dates & fill obvious nulls.\"\"\"\n",
    "    #df = df.rename(columns={k: v for k, v in COL_MAP.items() if k in df.columns})\n",
    "    df[\"purch_doc_date_hpd_po\"] = pd.to_datetime(df[\"purch_doc_date_hpd_po\"], errors=\"coerce\")\n",
    "    df[\"doc_change_date_src_po\"] = pd.to_datetime(df.get(\"doc_change_date_src_po\"), errors=\"coerce\")\n",
    "    df[\"vendor_or_creditor_acct_no_hpd_po\"] = df.get(\"vendor_or_creditor_acct_no_hpd_po\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"requester_name_src_po\"] = df.get(\"requester_name_src_po\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"exchange_rate_hpd_po\"] = df.get(\"exchange_rate_hpd_po\", 1.0).replace({0: np.nan}).fillna(1.0)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2a) Ruleâ€‘based features (rulesÂ 1,2,3,5)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def add_rule_metrics(df: pd.DataFrame,\n",
    "                     split_days: int = 60,\n",
    "                     price_var_days: int = 365) -> pd.DataFrame:\n",
    "    \"\"\"Add features mirroring Baldota P2P rules.\n",
    "\n",
    "    * vm_count/value = aggregation for **same vendor+material** within *split_days*\n",
    "    * vm_price_var_pct = price deviation (%) vs mean of past *price_var_days*\n",
    "    * mat_vendor_cnt & mat_price_var_pct analogues for material across vendors\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.sort_values(\"purch_doc_date_hpd_po\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Initialise\n",
    "    df[\"vm_count_%dd\" % split_days] = 0\n",
    "    df[\"vm_value_%dd\" % split_days] = 0.0\n",
    "    df[\"vm_price_var_pct_%dd\" % price_var_days] = 0.0\n",
    "    df[\"mat_vendor_cnt_%dd\" % split_days] = 0\n",
    "    df[\"mat_price_var_pct_%dd\" % price_var_days] = 0.0\n",
    "\n",
    "    # Preâ€‘extract convenient arrays for speed\n",
    "    po_dates = df[\"purch_doc_date_hpd_po\"].values\n",
    "    prices = df.get(\"net_price_doc_curr_src_po\").astype(float).values\n",
    "    vals = df.get(\"gross_val_po_curr_src_po\").astype(float).values\n",
    "\n",
    "    # --- Same vendor + material group logic ----------------------------------\n",
    "    for (v, m), idx in df.groupby([\"vendor_or_creditor_acct_no_hpd_po\", \"material_no_src_po\"]).groups.items():\n",
    "        d = po_dates[idx]\n",
    "        p = prices[idx]\n",
    "        vls = vals[idx]\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = d[loc]\n",
    "            # split window\n",
    "            win_mask = (d >= cur - np.timedelta64(split_days, \"D\")) & (d <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(\"vm_count_%dd\" % split_days)] = int(win_mask.sum())\n",
    "            df.iat[ridx, df.columns.get_loc(\"vm_value_%dd\" % split_days)] = float(vls[win_mask].sum())\n",
    "            # price variance window\n",
    "            var_mask = (d >= cur - np.timedelta64(price_var_days, \"D\")) & (d <= cur)\n",
    "            if var_mask.sum() > 1:\n",
    "                mean_price = p[var_mask].mean()\n",
    "                if mean_price:\n",
    "                    pct = abs(p[loc] - mean_price) / mean_price * 100\n",
    "                    df.iat[ridx, df.columns.get_loc(\"vm_price_var_pct_%dd\" % price_var_days)] = pct\n",
    "\n",
    "    # --- Materialâ€‘only group logic -------------------------------------------\n",
    "    for m, idx in df.groupby(\"material_no_src_po\").groups.items():\n",
    "        d = po_dates[idx]\n",
    "        p = prices[idx]\n",
    "        vendors = df.loc[idx, \"vendor_or_creditor_acct_no_hpd_po\"].values\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = d[loc]\n",
    "            win_mask = (d >= cur - np.timedelta64(split_days, \"D\")) & (d <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(\"mat_vendor_cnt_%dd\" % split_days)] = int(len(set(vendors[win_mask])))\n",
    "            var_mask = (d >= cur - np.timedelta64(price_var_days, \"D\")) & (d <= cur)\n",
    "            if var_mask.sum() > 1:\n",
    "                mean_price = p[var_mask].mean()\n",
    "                if mean_price:\n",
    "                    pct = abs(p[loc] - mean_price) / mean_price * 100\n",
    "                    df.iat[ridx, df.columns.get_loc(\"mat_price_var_pct_%dd\" % price_var_days)] = pct\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2b) Value & process metrics\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def add_value_and_timing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"conv_factor_p2o\"] = (\n",
    "        df.get(\"p2o_unit_conv_num_src_po\") / df.get(\"p2o_unit_conv_denom_src_po\")\n",
    "    ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "    df[\"conv_factor_o2b\"] = (\n",
    "        df.get(\"o2b_unit_conv_num_src_po\") / df.get(\"o2b_unit_conv_denom_src_po\")\n",
    "    ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "    df[\"po_change_lag_days\"] = (df.get(\"doc_change_date_src_po\") - df[\"purch_doc_date_hpd_po\"]).dt.days\n",
    "    df[\"base_value\"] = df.get(\"gross_val_po_curr_src_po\") * df.get(\"exchange_rate_hpd_po\")\n",
    "    p95 = df[\"gross_val_po_curr_src_po\"].quantile(0.95)\n",
    "    df[\"high_value_flag\"] = (df[\"gross_val_po_curr_src_po\"] >= p95).astype(int)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2c) Behavioural rolling windows\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _rolling_stats(df: pd.DataFrame, group_col: str, days: int,\n",
    "                   count_col: str, sum_col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[count_col] = 0\n",
    "    df[sum_col] = 0.0\n",
    "\n",
    "    po_dates = df[\"purch_doc_date_hpd_po\"].values\n",
    "    gross_vals = df[\"gross_val_po_curr_src_po\"].values\n",
    "\n",
    "    for key, idx in df.groupby(group_col).groups.items():\n",
    "        dates = po_dates[idx]\n",
    "        vals = gross_vals[idx]\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = dates[loc]\n",
    "            mask = (dates >= cur - np.timedelta64(days, \"D\")) & (dates <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(count_col)] = int(mask.sum())\n",
    "            df.iat[ridx, df.columns.get_loc(sum_col)] = float(vals[mask].sum())\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_behavioural_stats(df: pd.DataFrame, window_days: int = 30) -> pd.DataFrame:\n",
    "    df = _rolling_stats(df, \"requester_name_src_po\", window_days,\n",
    "                        \"req_po_count_%dd\" % window_days,\n",
    "                        \"req_val_sum_%dd\" % window_days)\n",
    "    df = _rolling_stats(df, \"vendor_or_creditor_acct_no_hpd_po\", window_days,\n",
    "                        \"vendor_po_count_%dd\" % window_days,\n",
    "                        \"vendor_val_sum_%dd\" % window_days)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Public orchestrator\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def build_features(raw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Endâ€‘toâ€‘end feature generator (no target leakage).\"\"\"\n",
    "    df = _prep(raw_df)\n",
    "    df = add_rule_metrics(df)\n",
    "    df = add_value_and_timing(df)\n",
    "    df = add_behavioural_stats(df)\n",
    "    return df\n",
    "\n",
    "def flag_split_po(df):\n",
    "    df = df.copy()\n",
    "    df['split_po_flag'] = 0  # Default 0\n",
    "\n",
    "    # Apply exclusion filters only for computation\n",
    "    exclude_doc_types = [\"AN\", \"AR\", \"MN\", \"QC\", \"QI\", \"QS\", \"RS\", \"SC\", \"SG\", \"SR\", \"SS\", \"ST\", \"TP\", \"TR\", \"UB\", \"WK\"]\n",
    "\n",
    "    filtered = df[\n",
    "        (~df['purch_doc_type_hpd_po'].isin(exclude_doc_types)) &\n",
    "        (df['purch_doc_type_hpd_po'].notna()) &\n",
    "        (~(df['po_item_del_flag_src_po'] == 'L')) &\n",
    "        (~df['plant_src_po'].fillna(\"\").astype(str).str.startswith(\"4\")) &\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['gross_val_po_curr_src_po'] >= 10) &\n",
    "        (df['purch_doc_date_hpd_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['company_code_src_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    filtered['purch_doc_date_hpd_po'] = pd.to_datetime(filtered['purch_doc_date_hpd_po'])\n",
    "\n",
    "    def get_group_key(row):\n",
    "        if pd.notna(row['material_no_src_po']):\n",
    "            return f\"{row['vendor_or_creditor_acct_no_hpd_po']}__{row['material_no_src_po']}\"\n",
    "        elif pd.notna(row['short_text_src_po']):\n",
    "            return f\"{row['vendor_or_creditor_acct_no_hpd_po']}__{row['short_text_src_po']}\"\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    filtered['split_key'] = filtered.apply(get_group_key, axis=1)\n",
    "    filtered = filtered[filtered['split_key'].notna()].copy()\n",
    "    filtered.sort_values(['split_key', 'purch_doc_date_hpd_po'], inplace=True)\n",
    "\n",
    "    flagged_po_set = set()\n",
    "\n",
    "    for key, group in filtered.groupby('split_key'):\n",
    "        dates = group['purch_doc_date_hpd_po'].reset_index(drop=True)\n",
    "        po_nos = group['purch_doc_no_src_po'].reset_index(drop=True)\n",
    "\n",
    "        for i in range(len(dates)):\n",
    "            date_i = dates[i]\n",
    "            po_i = po_nos[i]\n",
    "            mask = (\n",
    "                (dates >= date_i - pd.Timedelta(days=14)) &\n",
    "                (dates <= date_i + pd.Timedelta(days=14)) &\n",
    "                (po_nos != po_i)\n",
    "            )\n",
    "            if mask.sum() > 0:\n",
    "                flagged_po_set.add(po_i)\n",
    "\n",
    "    # Assign flag only to matching rows in original df\n",
    "    df['split_po_flag'] = df['purch_doc_no_src_po'].isin(flagged_po_set).astype(int)\n",
    "    return df\n",
    "def flag_intra_po_split(df, gross_threshold=10):\n",
    "    df = df.copy()\n",
    "    df['intra_po_split_flag'] = 0  # Default\n",
    "\n",
    "    df_valid = df[\n",
    "        df['gross_val_po_curr_src_po'].notna() &\n",
    "        df['vendor_or_creditor_acct_no_hpd_po'].notna() &\n",
    "        df['material_no_src_po'].notna() &\n",
    "        df['purch_doc_no_src_po'].notna()\n",
    "    ].copy()\n",
    "\n",
    "    group_cols = ['purch_doc_no_src_po', 'vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po']\n",
    "    grouped = df_valid.groupby(group_cols)\n",
    "\n",
    "    flagged_indexes = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        total_gross = group['gross_val_po_curr_src_po'].sum()\n",
    "        num_items = len(group)\n",
    "        all_below_threshold = group['gross_val_po_curr_src_po'].all()\n",
    "\n",
    "        if total_gross >= gross_threshold and num_items > 1 and all_below_threshold:\n",
    "            flagged_indexes.extend(group.index.tolist())\n",
    "\n",
    "    df.loc[flagged_indexes, 'intra_po_split_flag'] = 1\n",
    "    return df\n",
    "def flag_multiple_pos_per_pr_item(df):\n",
    "    df = df.copy()\n",
    "    df['multi_po_per_pr_flag'] = 0  # Default\n",
    "\n",
    "    # Only consider approved POs with valid PR and PR item\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        df['pr_no_src_po'].notna() &\n",
    "        df['pr_item_no_src_po'].notna() &\n",
    "        df['purch_doc_no_src_po'].notna()\n",
    "    ][['pr_no_src_po', 'pr_item_no_src_po', 'purch_doc_no_src_po']].drop_duplicates()\n",
    "\n",
    "    # Count number of unique POs per PR+Item\n",
    "    po_counts = df_valid.groupby(['pr_no_src_po', 'pr_item_no_src_po'])['purch_doc_no_src_po'].nunique()\n",
    "\n",
    "    # Identify PR+Items linked to more than one PO\n",
    "    multi_po_keys = po_counts[po_counts > 1].index.tolist()\n",
    "\n",
    "    # Create a set for fast lookup\n",
    "    multi_po_set = set(multi_po_keys)\n",
    "\n",
    "    # Flag in the main DataFrame\n",
    "    df['multi_po_per_pr_flag'] = df.apply(\n",
    "        lambda row: 1 if (row['pr_no_src_po'], row['pr_item_no_src_po']) in multi_po_set else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "def flag_same_vendor_price_increase(df, price_increase_threshold=0.05, months_range=6, flag_column='same_vendor_price_increase_flag'):\n",
    "    df = df.copy()\n",
    "    df[flag_column] = 0\n",
    "\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['material_no_src_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['net_price_doc_curr_src_po'].notna()) &\n",
    "        (df['purch_doc_date_hpd_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    df_valid['purch_doc_date_hpd_po'] = pd.to_datetime(df_valid['purch_doc_date_hpd_po'])\n",
    "    df_valid.sort_values(['vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po', 'purch_doc_date_hpd_po'], inplace=True)\n",
    "\n",
    "    group_cols = ['vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po']\n",
    "    flagged_indices = []\n",
    "\n",
    "    for _, group in df_valid.groupby(group_cols):\n",
    "        group = group.sort_values('purch_doc_date_hpd_po').reset_index()\n",
    "\n",
    "        for i in range(1, len(group)):\n",
    "            current_row = group.loc[i]\n",
    "            current_date = current_row['purch_doc_date_hpd_po']\n",
    "            current_price = current_row['net_price_doc_curr_src_po']\n",
    "\n",
    "            mask = group.loc[:i-1, 'purch_doc_date_hpd_po'] >= current_date - pd.DateOffset(months=months_range)\n",
    "            past_group = group.loc[:i-1][mask]\n",
    "\n",
    "            if not past_group.empty:\n",
    "                last_price = past_group['net_price_doc_curr_src_po'].iloc[-1]\n",
    "                if last_price > 0 and ((current_price - last_price) / last_price) >= price_increase_threshold:\n",
    "                    flagged_indices.append(current_row['index'])\n",
    "\n",
    "    df.loc[flagged_indices, flag_column] = 1\n",
    "    return df\n",
    "def flag_diff_vendor_price_variance(df, price_variance_threshold=0.05, months_range=6, flag_column='diff_vendor_price_variance_flag'):\n",
    "    df = df.copy()\n",
    "    df[flag_column] = 0\n",
    "\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['material_no_src_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['net_price_doc_curr_src_po'].notna()) &\n",
    "        (df['purch_doc_date_hpd_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    df_valid['purch_doc_date_hpd_po'] = pd.to_datetime(df_valid['purch_doc_date_hpd_po'])\n",
    "    df_valid = df_valid.sort_values(['material_no_src_po', 'purch_doc_date_hpd_po'])\n",
    "\n",
    "    for material, mat_group in df_valid.groupby('material_no_src_po'):\n",
    "        mat_group = mat_group.sort_values('purch_doc_date_hpd_po').reset_index()\n",
    "\n",
    "        for i in range(len(mat_group)):\n",
    "            current_row = mat_group.loc[i]\n",
    "            current_date = current_row['purch_doc_date_hpd_po']\n",
    "            current_price = current_row['net_price_doc_curr_src_po']\n",
    "\n",
    "            past_window = mat_group[\n",
    "                (mat_group['purch_doc_date_hpd_po'] < current_date) &\n",
    "                (mat_group['purch_doc_date_hpd_po'] >= current_date - pd.DateOffset(months=months_range))\n",
    "            ]\n",
    "\n",
    "            vendor_prices = past_window.groupby('vendor_or_creditor_acct_no_hpd_po')['net_price_doc_curr_src_po'].mean()\n",
    "            if not vendor_prices.empty:\n",
    "                max_price = vendor_prices.max()\n",
    "                min_price = vendor_prices.min()\n",
    "                if max_price > 0 and (max_price - min_price) / max_price >= price_variance_threshold:\n",
    "                    df.loc[current_row['index'], flag_column] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def final_parsing(df):\n",
    "        # === Parse Date Columns ===\n",
    "    date_cols = [\n",
    "        \"doc_change_date_src_po\",\n",
    "        \"doc_change_date_hpd_po\",\n",
    "        \"purch_doc_date_hpd_po\"\n",
    "    ]\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    \n",
    "    # === Feature Engineering ===\n",
    "    \n",
    "    # A. Date Features\n",
    "    df[\"po_doc_age_days\"] = (df[\"doc_change_date_hpd_po\"] - df[\"purch_doc_date_hpd_po\"]).dt.days\n",
    "    df[\"lead_time_po_vs_pr\"] = (df[\"purch_doc_date_hpd_po\"] - df[\"doc_change_date_src_po\"]).dt.days\n",
    "    df[\"po_day_of_week\"] = df[\"purch_doc_date_hpd_po\"].dt.dayofweek\n",
    "    df[\"po_day_of_month\"] = df[\"purch_doc_date_hpd_po\"].dt.day\n",
    "    df[\"po_month\"] = df[\"purch_doc_date_hpd_po\"].dt.month\n",
    "    \n",
    "    # B. Price & Value Features\n",
    "    df[\"price_per_unit\"] = df[\"net_val_po_curr_src_po\"] / df[\"quantity_src_po\"].replace(0, np.nan)\n",
    "    df[\"net_vs_gross_delta\"] = df[\"gross_val_po_curr_src_po\"] - df[\"net_val_po_curr_src_po\"]\n",
    "    df[\"price_variance_percent\"] = (df[\"net_val_po_curr_src_po\"] - df[\"gross_val_po_curr_src_po\"]) / df[\"gross_val_po_curr_src_po\"].replace(0, np.nan)\n",
    "    df[\"outline_agrmt_coverage\"] = df[\"outline_agrmt_tgt_val_doc_curr_src_po\"] / df[\"net_val_po_curr_src_po\"].replace(0, np.nan)\n",
    "    \n",
    "    # C. PO-PR Linkage Flags\n",
    "    df[\"has_pr_link\"] = df[\"pr_no_src_po\"].notna().astype(int)\n",
    "    df[\"has_pr_item_link\"] = df[\"pr_item_no_src_po\"].notna().astype(int)\n",
    "    \n",
    "    # D. Flags & Indicators\n",
    "    binary_cols = [\n",
    "        \"gr_indicator_src_po\", \"gr_invoice_verif_flag_src_po\",\n",
    "        \"inv_receipt_indicator_src_po\", \"release_indicator_hpd_po\",\n",
    "        \"release_status_hpd_po\", \"doc_release_incompl_flag_hpd_po\"\n",
    "    ]\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            df[col + \"_flag\"] = df[col].notna().astype(int)\n",
    "    \n",
    "    # E. Missing Signal\n",
    "    critical_cols = [\n",
    "        \"material_type_src_po\", \"material_no_src_po\",\n",
    "        \"vendor_or_creditor_acct_no_hpd_po\", \"gross_val_po_curr_src_po\",\n",
    "        \"net_price_doc_curr_src_po\"\n",
    "    ]\n",
    "    df[\"missing_critical_fields\"] = df[critical_cols].isnull().sum(axis=1)\n",
    "    \n",
    "    # F. Currency & Exchange Rate\n",
    "    df[\"has_exchange_rate\"] = df[\"exchange_rate_hpd_po\"].notna().astype(int)\n",
    "    df[\"log_exchange_rate\"] = np.log1p(df[\"exchange_rate_hpd_po\"].fillna(0))\n",
    "    \n",
    "    # G. Unit Conversion Features\n",
    "    df[\"p2o_conversion_ratio\"] = df[\"p2o_unit_conv_num_src_po\"] / df[\"p2o_unit_conv_denom_src_po\"].replace(0, np.nan)\n",
    "    df[\"o2b_conversion_ratio\"] = df[\"o2b_unit_conv_num_src_po\"] / df[\"o2b_unit_conv_denom_src_po\"].replace(0, np.nan)\n",
    "    \n",
    "    # H. Behavioral Flags\n",
    "    df[\"is_same_vendor_pr_po\"] = (\n",
    "        df[\"vendor_or_creditor_acct_no_hpd_po\"].notna() & df[\"base_id_src_po\"].notna()\n",
    "    ).astype(int)\n",
    "    df[\"has_rfq_status\"] = df[\"rfq_status_hpd_po\"].notna().astype(int)\n",
    "    df[\"purch_group_org_same\"] = (df[\"purch_group_hpd_po\"] == df[\"purch_org_hpd_po\"]).astype(int)\n",
    "    \n",
    "    # I. Rare Category Flagging\n",
    "    rare_cat_cols = [\"purch_doc_type_hpd_po\", \"purch_group_hpd_po\", \"vendor_or_creditor_acct_no_hpd_po\"]\n",
    "    for col in rare_cat_cols:\n",
    "        if col in df.columns:\n",
    "            freq_map = df[col].value_counts(normalize=True)\n",
    "            df[f\"{col}_is_rare\"] = df[col].map(freq_map) < 0.01\n",
    "\n",
    "    return df\n",
    "##############################################################################\n",
    "#  Cell 3 â€” Create / load features\n",
    "##############################################################################\n",
    "if FEAT_PO.exists():\n",
    "    print(\"âš¡ Loading cached features\")\n",
    "    df = pd.read_pickle(FEAT_PO)\n",
    "else:\n",
    "    print(\"ðŸš§ Generating features â€¦\")\n",
    "    raw_df  = pd.read_pickle(RAW_PO)\n",
    "    raw_df_2= sub_risk(raw_df)\n",
    "    feat_df = build_features(raw_df_2)\n",
    "    df=flag_split_po(feat_df)\n",
    "    df=flag_intra_po_split(df)\n",
    "    df = flag_multiple_pos_per_pr_item(df)\n",
    "    # Same vendor price jump\n",
    "    df = flag_same_vendor_price_increase(df, months_range=6, flag_column='same_vendor_price_increase_6m_flag')\n",
    "    df = flag_same_vendor_price_increase(df, months_range=12, flag_column='same_vendor_price_increase_12m_flag')\n",
    "    \n",
    "    # Diff vendor price variance\n",
    "    df = flag_diff_vendor_price_variance(df, months_range=6, flag_column='diff_vendor_price_variance_6m_flag')\n",
    "    df = flag_diff_vendor_price_variance(df, months_range=12, flag_column='diff_vendor_price_variance_12m_flag')\n",
    "    df=final_parsing(df)\n",
    "    df.to_pickle(FEAT_PO)\n",
    "    print(\"Feature shape:\", df.shape)\n",
    "\n",
    "columns_to_drop = [\n",
    "    # IDs and references\n",
    "    \"purch_doc_no_src_po\", \"purch_doc_item_no_src_po\", \"pr_no_src_po\", \"pr_item_no_src_po\",\n",
    "    \"base_id_src_po\", \"principal_purch_agrmt_item_no_src_po\", \"principal_purch_agrmt_no_hpd_po\",\n",
    "\n",
    "    # Text\n",
    "    \"short_text_src_po\", \"requester_name_src_po\", \"resp_vendor_salesperson_hpd_po\",\n",
    "\n",
    "    # Dates (used to create features)\n",
    "    \"doc_change_date_src_po\", \"doc_change_date_hpd_po\", \"purch_doc_date_hpd_po\",\n",
    "\n",
    "    # Sparse or incomplete\n",
    "    \"po_item_del_flag_src_po\", \"doc_release_incompl_flag_hpd_po\", \"control_indicator_hpd_po\",\n",
    "\n",
    "    # Replaced with ratios / logs / engineered\n",
    "    \"p2o_unit_conv_denom_src_po\", \"p2o_unit_conv_num_src_po\",\n",
    "    \"o2b_unit_conv_denom_src_po\", \"o2b_unit_conv_num_src_po\",\n",
    "    \"gross_val_po_curr_src_po\", \"net_val_po_curr_src_po\",\n",
    "    \"outline_agrmt_tgt_val_doc_curr_src_po\", \"quantity_src_po\",\n",
    "    \"exchange_rate_hpd_po\", \"net_price_doc_curr_src_po\",'base_id_po','rule_ids_po','P2P02067','P2P02068','P2P02070','P2P02072',\n",
    "       'Main Risk Scenario',]\n",
    "df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "# === Final Dataset ===\n",
    "\n",
    "df[\"rft_by_engine_po\"] = df[\"rft_by_engine_po\"].astype(int)\n",
    "## renaming rft_by_engine_po to model_flag\n",
    "df.rename(columns={\"rft_by_engine_po\": \"model_flag\"}, inplace=True)\n",
    "#df.drop(columns=[\"model_flag\"],inplace=True)\n",
    "# === Encode Remaining Categorical Columns ===\n",
    "categorical_cols = df.select_dtypes(include='object').columns.difference(['Sub Risks'])\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = df[col].astype(str).fillna(\"Unknown\")\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# === Impute Any Remaining Numeric Columns ===\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "\n",
    "\n",
    "#X = df.drop(columns=[\"rft_by_engine_po\"])\n",
    "#y = df[\"rft_by_engine_po\"]\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_multi = mlb.fit_transform(df[\"Sub Risks\"])\n",
    "label_names = mlb.classes_\n",
    "print(\"Risk classes:\", mlb.classes_)\n",
    "\n",
    "# Drop old risk columns and Sub Risks list\n",
    "X = df.drop(columns=[\"Sub Risks\"])  \n",
    "y = y_multi  # Multi-hot encoded matrix\n",
    "joblib.dump(X.columns.tolist(), \"Po_Invoice_Data/trained_feature_columns.pkl\")\n",
    "# Split X and multi-label y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=None  # cannot use stratify for multi-label\n",
    ")\n",
    "\n",
    "# === STEP 5: Train Multi-label Model ===\n",
    "model = OneVsRestClassifier(RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === STEP 6: Predict ===\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# === STEP 7: Evaluation ===\n",
    "print(\"ðŸ“Š Classification Report (per label):\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_names))\n",
    "\n",
    "# === STEP 8: Multilabel Confusion Matrices ===\n",
    "mcm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix for each label\n",
    "for i, label in enumerate(label_names):\n",
    "    cm = mcm[i]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[f\"Not {label}\", label])\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix for '{label}'\")\n",
    "    plt.show()\n",
    "\n",
    "# === STEP 9: Convert predictions to label names ===\n",
    "predicted_labels = mlb.inverse_transform(y_pred)\n",
    "\n",
    "# Optional: if you want to keep X_test with index\n",
    "X_test_copy = X_test.copy()\n",
    "X_test_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add predicted labels as a column\n",
    "results_df = X_test_copy.copy()\n",
    "results_df[\"Predicted Risks\"] = predicted_labels\n",
    "\n",
    "# View sample\n",
    "print(results_df[[\"Predicted Risks\"]].head())\n",
    "\n",
    "# === STEP 10: Save Model and Preprocessing Artifacts ===\n",
    "joblib.dump(model, \"Po_Invoice_Data/rf_multilabel_model_sub_risks.pkl\")\n",
    "joblib.dump(mlb, \"Po_Invoice_Data/mlb.pkl\")\n",
    "joblib.dump(label_encoders, \"Po_Invoice_Data/label_encoders.pkl\")\n",
    "joblib.dump(imputer, \"Po_Invoice_Data/imputer.pkl\")\n",
    "print(\"âœ… Model and artifacts saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
