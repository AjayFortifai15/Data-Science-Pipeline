{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d6a04-b498-4ed6-8156-1ae0cc21e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "##############################################################################\n",
    "#  Cell 1 â€” Imports & common paths\n",
    "##############################################################################\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import joblib\n",
    "import logging\n",
    "from typing import List\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import logging\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set options to show full DataFrame output\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import joblib, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from typing import Any, Dict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "RAW_PO      = Path(\"po_output.pkl\") #(\"../Po_Invoice_Data/po_output_tushar.pkl\")      # raw PO data (input)\n",
    "FEAT_PO     = Path(\"Po_Invoice_Data/po_output_features_df_model.pkl\")         # engineered features\n",
    "MODEL_PKL   = Path(\"Po_Invoice_Data/po_gbdt_model.pkl\")         # tuned model file\n",
    "CV_REPORT   = Path(\"Po_Invoice_Data/cv_results.csv\")            # param grid results\n",
    "SCORING_OUT = Path(\"Po_Invoice_Data/scored_po.pkl\")             # predictions file\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Column standardisation helper\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "COL_MAP: Dict[str, str] = {\n",
    "    # raw_column                          # internal name\n",
    "    \"vendor_or_creditor_acct_no_hpd_po\": \"vendor_id\",\n",
    "    \"material_no_src_po\": \"material_id\",\n",
    "    \"purch_doc_date_hpd_po\": \"po_date\",\n",
    "    \"doc_change_date_src_po\": \"po_change_date\",\n",
    "    \"net_price_doc_curr_src_po\": \"net_price\",\n",
    "    \"gross_val_po_curr_src_po\": \"gross_val\",\n",
    "    \"exchange_rate_hpd_po\": \"exch_rate\",\n",
    "    \"requester_name_src_po\": \"requester\",\n",
    "    # unitâ€‘conversion numerators / denominators\n",
    "    \"p2o_unit_conv_num_src_po\": \"p2o_num\",\n",
    "    \"p2o_unit_conv_denom_src_po\": \"p2o_den\",\n",
    "    \"o2b_unit_conv_num_src_po\": \"o2b_num\",\n",
    "    \"o2b_unit_conv_denom_src_po\": \"o2b_den\",\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Feature engineering functions (pure, chainable)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _prep(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Rename key columns, parse dates & fill obvious nulls.\"\"\"\n",
    "    #df = df.rename(columns={k: v for k, v in COL_MAP.items() if k in df.columns})\n",
    "    df[\"purch_doc_date_hpd_po\"] = pd.to_datetime(df[\"purch_doc_date_hpd_po\"], errors=\"coerce\")\n",
    "    df[\"doc_change_date_src_po\"] = pd.to_datetime(df.get(\"doc_change_date_src_po\"), errors=\"coerce\")\n",
    "    df[\"vendor_or_creditor_acct_no_hpd_po\"] = df.get(\"vendor_or_creditor_acct_no_hpd_po\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"requester_name_src_po\"] = df.get(\"requester_name_src_po\", \"UNKNOWN\").fillna(\"UNKNOWN\")\n",
    "    df[\"exchange_rate_hpd_po\"] = df.get(\"exchange_rate_hpd_po\", 1.0).replace({0: np.nan}).fillna(1.0)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2a) Ruleâ€‘based features (rulesÂ 1,2,3,5)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def add_rule_metrics(df: pd.DataFrame,\n",
    "                     split_days: int = 60,\n",
    "                     price_var_days: int = 365) -> pd.DataFrame:\n",
    "    \"\"\"Add features mirroring Baldota P2P rules.\n",
    "\n",
    "    * vm_count/value = aggregation for **same vendor+material** within *split_days*\n",
    "    * vm_price_var_pct = price deviation (%) vs mean of past *price_var_days*\n",
    "    * mat_vendor_cnt & mat_price_var_pct analogues for material across vendors\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.sort_values(\"purch_doc_date_hpd_po\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Initialise\n",
    "    df[\"vm_count_%dd\" % split_days] = 0\n",
    "    df[\"vm_value_%dd\" % split_days] = 0.0\n",
    "    df[\"vm_price_var_pct_%dd\" % price_var_days] = 0.0\n",
    "    df[\"mat_vendor_cnt_%dd\" % split_days] = 0\n",
    "    df[\"mat_price_var_pct_%dd\" % price_var_days] = 0.0\n",
    "\n",
    "    # Preâ€‘extract convenient arrays for speed\n",
    "    po_dates = df[\"purch_doc_date_hpd_po\"].values\n",
    "    prices = df.get(\"net_price_doc_curr_src_po\").astype(float).values\n",
    "    vals = df.get(\"gross_val_po_curr_src_po\").astype(float).values\n",
    "\n",
    "    # --- Same vendor + material group logic ----------------------------------\n",
    "    for (v, m), idx in df.groupby([\"vendor_or_creditor_acct_no_hpd_po\", \"material_no_src_po\"]).groups.items():\n",
    "        d = po_dates[idx]\n",
    "        p = prices[idx]\n",
    "        vls = vals[idx]\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = d[loc]\n",
    "            # split window\n",
    "            win_mask = (d >= cur - np.timedelta64(split_days, \"D\")) & (d <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(\"vm_count_%dd\" % split_days)] = int(win_mask.sum())\n",
    "            df.iat[ridx, df.columns.get_loc(\"vm_value_%dd\" % split_days)] = float(vls[win_mask].sum())\n",
    "            # price variance window\n",
    "            var_mask = (d >= cur - np.timedelta64(price_var_days, \"D\")) & (d <= cur)\n",
    "            if var_mask.sum() > 1:\n",
    "                mean_price = p[var_mask].mean()\n",
    "                if mean_price:\n",
    "                    pct = abs(p[loc] - mean_price) / mean_price * 100\n",
    "                    df.iat[ridx, df.columns.get_loc(\"vm_price_var_pct_%dd\" % price_var_days)] = pct\n",
    "\n",
    "    # --- Materialâ€‘only group logic -------------------------------------------\n",
    "    for m, idx in df.groupby(\"material_no_src_po\").groups.items():\n",
    "        d = po_dates[idx]\n",
    "        p = prices[idx]\n",
    "        vendors = df.loc[idx, \"vendor_or_creditor_acct_no_hpd_po\"].values\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = d[loc]\n",
    "            win_mask = (d >= cur - np.timedelta64(split_days, \"D\")) & (d <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(\"mat_vendor_cnt_%dd\" % split_days)] = int(len(set(vendors[win_mask])))\n",
    "            var_mask = (d >= cur - np.timedelta64(price_var_days, \"D\")) & (d <= cur)\n",
    "            if var_mask.sum() > 1:\n",
    "                mean_price = p[var_mask].mean()\n",
    "                if mean_price:\n",
    "                    pct = abs(p[loc] - mean_price) / mean_price * 100\n",
    "                    df.iat[ridx, df.columns.get_loc(\"mat_price_var_pct_%dd\" % price_var_days)] = pct\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2b) Value & process metrics\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def add_value_and_timing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"conv_factor_p2o\"] = (\n",
    "        df.get(\"p2o_unit_conv_num_src_po\") / df.get(\"p2o_unit_conv_denom_src_po\")\n",
    "    ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "    df[\"conv_factor_o2b\"] = (\n",
    "        df.get(\"o2b_unit_conv_num_src_po\") / df.get(\"o2b_unit_conv_denom_src_po\")\n",
    "    ).replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "    df[\"po_change_lag_days\"] = (df.get(\"doc_change_date_src_po\") - df[\"purch_doc_date_hpd_po\"]).dt.days\n",
    "    df[\"base_value\"] = df.get(\"gross_val_po_curr_src_po\") * df.get(\"exchange_rate_hpd_po\")\n",
    "    p95 = df[\"gross_val_po_curr_src_po\"].quantile(0.95)\n",
    "    df[\"high_value_flag\"] = (df[\"gross_val_po_curr_src_po\"] >= p95).astype(int)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2c) Behavioural rolling windows\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _rolling_stats(df: pd.DataFrame, group_col: str, days: int,\n",
    "                   count_col: str, sum_col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[count_col] = 0\n",
    "    df[sum_col] = 0.0\n",
    "\n",
    "    po_dates = df[\"purch_doc_date_hpd_po\"].values\n",
    "    gross_vals = df[\"gross_val_po_curr_src_po\"].values\n",
    "\n",
    "    for key, idx in df.groupby(group_col).groups.items():\n",
    "        dates = po_dates[idx]\n",
    "        vals = gross_vals[idx]\n",
    "        for loc, ridx in enumerate(idx):\n",
    "            cur = dates[loc]\n",
    "            mask = (dates >= cur - np.timedelta64(days, \"D\")) & (dates <= cur)\n",
    "            df.iat[ridx, df.columns.get_loc(count_col)] = int(mask.sum())\n",
    "            df.iat[ridx, df.columns.get_loc(sum_col)] = float(vals[mask].sum())\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_behavioural_stats(df: pd.DataFrame, window_days: int = 30) -> pd.DataFrame:\n",
    "    df = _rolling_stats(df, \"requester_name_src_po\", window_days,\n",
    "                        \"req_po_count_%dd\" % window_days,\n",
    "                        \"req_val_sum_%dd\" % window_days)\n",
    "    df = _rolling_stats(df, \"vendor_or_creditor_acct_no_hpd_po\", window_days,\n",
    "                        \"vendor_po_count_%dd\" % window_days,\n",
    "                        \"vendor_val_sum_%dd\" % window_days)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Public orchestrator\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def build_features(raw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Endâ€‘toâ€‘end feature generator (no target leakage).\"\"\"\n",
    "    df = _prep(raw_df)\n",
    "    df = add_rule_metrics(df)\n",
    "    df = add_value_and_timing(df)\n",
    "    df = add_behavioural_stats(df)\n",
    "    return df\n",
    "def flag_split_po(df):\n",
    "    df = df.copy()\n",
    "    df['split_po_flag'] = 0  # Default 0\n",
    "\n",
    "    # Apply exclusion filters only for computation\n",
    "    exclude_doc_types = [\"AN\", \"AR\", \"MN\", \"QC\", \"QI\", \"QS\", \"RS\", \"SC\", \"SG\", \"SR\", \"SS\", \"ST\", \"TP\", \"TR\", \"UB\", \"WK\"]\n",
    "\n",
    "    filtered = df[\n",
    "        (~df['purch_doc_type_hpd_po'].isin(exclude_doc_types)) &\n",
    "        (df['purch_doc_type_hpd_po'].notna()) &\n",
    "        (~(df['po_item_del_flag_src_po'] == 'L')) &\n",
    "        (~df['plant_src_po'].fillna(\"\").astype(str).str.startswith(\"4\")) &\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['gross_val_po_curr_src_po'] >= 10) &\n",
    "        (df['purch_doc_date_hpd_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['company_code_src_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    filtered['purch_doc_date_hpd_po'] = pd.to_datetime(filtered['purch_doc_date_hpd_po'])\n",
    "\n",
    "    def get_group_key(row):\n",
    "        if pd.notna(row['material_no_src_po']):\n",
    "            return f\"{row['vendor_or_creditor_acct_no_hpd_po']}__{row['material_no_src_po']}\"\n",
    "        elif pd.notna(row['short_text_src_po']):\n",
    "            return f\"{row['vendor_or_creditor_acct_no_hpd_po']}__{row['short_text_src_po']}\"\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    filtered['split_key'] = filtered.apply(get_group_key, axis=1)\n",
    "    filtered = filtered[filtered['split_key'].notna()].copy()\n",
    "    filtered.sort_values(['split_key', 'purch_doc_date_hpd_po'], inplace=True)\n",
    "\n",
    "    flagged_po_set = set()\n",
    "\n",
    "    for key, group in filtered.groupby('split_key'):\n",
    "        dates = group['purch_doc_date_hpd_po'].reset_index(drop=True)\n",
    "        po_nos = group['purch_doc_no_src_po'].reset_index(drop=True)\n",
    "\n",
    "        for i in range(len(dates)):\n",
    "            date_i = dates[i]\n",
    "            po_i = po_nos[i]\n",
    "            mask = (\n",
    "                (dates >= date_i - pd.Timedelta(days=14)) &\n",
    "                (dates <= date_i + pd.Timedelta(days=14)) &\n",
    "                (po_nos != po_i)\n",
    "            )\n",
    "            if mask.sum() > 0:\n",
    "                flagged_po_set.add(po_i)\n",
    "\n",
    "    # Assign flag only to matching rows in original df\n",
    "    df['split_po_flag'] = df['purch_doc_no_src_po'].isin(flagged_po_set).astype(int)\n",
    "    return df\n",
    "def flag_intra_po_split(df, gross_threshold=10):\n",
    "    df = df.copy()\n",
    "    df['intra_po_split_flag'] = 0  # Default\n",
    "\n",
    "    df_valid = df[\n",
    "        df['gross_val_po_curr_src_po'].notna() &\n",
    "        df['vendor_or_creditor_acct_no_hpd_po'].notna() &\n",
    "        df['material_no_src_po'].notna() & \n",
    "        df['purch_doc_no_src_po'].notna() \n",
    "    \n",
    "    ].copy()\n",
    "\n",
    "    group_cols = ['purch_doc_no_src_po', 'vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po']\n",
    "    grouped = df_valid.groupby(group_cols)\n",
    "\n",
    "    flagged_indexes = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        total_gross = group['gross_val_po_curr_src_po'].sum()\n",
    "        num_items = len(group)\n",
    "        all_below_threshold = group['gross_val_po_curr_src_po'].all()\n",
    "\n",
    "        if total_gross >= gross_threshold and num_items > 1 and all_below_threshold:\n",
    "            flagged_indexes.extend(group.index.tolist())\n",
    "\n",
    "    df.loc[flagged_indexes, 'intra_po_split_flag'] = 1\n",
    "    return df\n",
    "def flag_multiple_pos_per_pr_item(df):\n",
    "    df = df.copy()\n",
    "    df['multi_po_per_pr_flag'] = 0  # Default\n",
    "\n",
    "    # Only consider approved POs with valid PR and PR item\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        df['pr_no_src_po'].notna() &\n",
    "        df['pr_item_no_src_po'].notna() &\n",
    "        df['purch_doc_no_src_po'].notna()\n",
    "    ][['pr_no_src_po', 'pr_item_no_src_po', 'purch_doc_no_src_po']].drop_duplicates()\n",
    "\n",
    "    # Count number of unique POs per PR+Item\n",
    "    po_counts = df_valid.groupby(['pr_no_src_po', 'pr_item_no_src_po'])['purch_doc_no_src_po'].nunique()\n",
    "\n",
    "    # Identify PR+Items linked to more than one PO\n",
    "    multi_po_keys = po_counts[po_counts > 1].index.tolist()\n",
    "\n",
    "    # Create a set for fast lookup\n",
    "    multi_po_set = set(multi_po_keys)\n",
    "\n",
    "    # Flag in the main DataFrame\n",
    "    df['multi_po_per_pr_flag'] = df.apply(\n",
    "        lambda row: 1 if (row['pr_no_src_po'], row['pr_item_no_src_po']) in multi_po_set else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "def flag_same_vendor_price_increase(df, price_increase_threshold=0.05, months_range=6, flag_column='same_vendor_price_increase_flag'):\n",
    "    df = df.copy()\n",
    "    df[flag_column] = 0\n",
    "\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['material_no_src_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['net_price_doc_curr_src_po'].notna()) &\n",
    "        (df['purch_doc_date_hpd_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    df_valid['purch_doc_date_hpd_po'] = pd.to_datetime(df_valid['purch_doc_date_hpd_po'])\n",
    "    df_valid.sort_values(['vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po', 'purch_doc_date_hpd_po'], inplace=True)\n",
    "\n",
    "    group_cols = ['vendor_or_creditor_acct_no_hpd_po', 'material_no_src_po']\n",
    "    flagged_indices = []\n",
    "\n",
    "    for _, group in df_valid.groupby(group_cols):\n",
    "        group = group.sort_values('purch_doc_date_hpd_po').reset_index()\n",
    "\n",
    "        for i in range(1, len(group)):\n",
    "            current_row = group.loc[i]\n",
    "            current_date = current_row['purch_doc_date_hpd_po']\n",
    "            current_price = current_row['net_price_doc_curr_src_po']\n",
    "\n",
    "            mask = group.loc[:i-1, 'purch_doc_date_hpd_po'] >= current_date - pd.DateOffset(months=months_range)\n",
    "            past_group = group.loc[:i-1][mask]\n",
    "\n",
    "            if not past_group.empty:\n",
    "                last_price = past_group['net_price_doc_curr_src_po'].iloc[-1]\n",
    "                if last_price > 0 and ((current_price - last_price) / last_price) >= price_increase_threshold:\n",
    "                    flagged_indices.append(current_row['index'])\n",
    "\n",
    "    df.loc[flagged_indices, flag_column] = 1\n",
    "    return df\n",
    "def flag_diff_vendor_price_variance(df, price_variance_threshold=0.05, months_range=6, flag_column='diff_vendor_price_variance_flag'):\n",
    "    df = df.copy()\n",
    "    df[flag_column] = 0\n",
    "\n",
    "    df_valid = df[\n",
    "        (df['release_indicator_hpd_po'] == 'R') &\n",
    "        (df['material_no_src_po'].notna()) &\n",
    "        (df['vendor_or_creditor_acct_no_hpd_po'].notna()) &\n",
    "        (df['net_price_doc_curr_src_po'].notna()) &\n",
    "        (df['purch_doc_date_hpd_po'].notna())\n",
    "    ].copy()\n",
    "\n",
    "    df_valid['purch_doc_date_hpd_po'] = pd.to_datetime(df_valid['purch_doc_date_hpd_po'])\n",
    "    df_valid = df_valid.sort_values(['material_no_src_po', 'purch_doc_date_hpd_po'])\n",
    "\n",
    "    for material, mat_group in df_valid.groupby('material_no_src_po'):\n",
    "        mat_group = mat_group.sort_values('purch_doc_date_hpd_po').reset_index()\n",
    "\n",
    "        for i in range(len(mat_group)):\n",
    "            current_row = mat_group.loc[i]\n",
    "            current_date = current_row['purch_doc_date_hpd_po']\n",
    "            current_price = current_row['net_price_doc_curr_src_po']\n",
    "\n",
    "            past_window = mat_group[\n",
    "                (mat_group['purch_doc_date_hpd_po'] < current_date) &\n",
    "                (mat_group['purch_doc_date_hpd_po'] >= current_date - pd.DateOffset(months=months_range))\n",
    "            ]\n",
    "\n",
    "            vendor_prices = past_window.groupby('vendor_or_creditor_acct_no_hpd_po')['net_price_doc_curr_src_po'].mean()\n",
    "            if not vendor_prices.empty:\n",
    "                max_price = vendor_prices.max()\n",
    "                min_price = vendor_prices.min()\n",
    "                if max_price > 0 and (max_price - min_price) / max_price >= price_variance_threshold:\n",
    "                    df.loc[current_row['index'], flag_column] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def final_parsing(df):\n",
    "        # === Parse Date Columns ===\n",
    "    date_cols = [\n",
    "        \"doc_change_date_src_po\",\n",
    "        \"doc_change_date_hpd_po\",\n",
    "        \"purch_doc_date_hpd_po\"\n",
    "    ]\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    \n",
    "    # === Feature Engineering ===\n",
    "    \n",
    "    # A. Date Features\n",
    "    df[\"po_doc_age_days\"] = (df[\"doc_change_date_hpd_po\"] - df[\"purch_doc_date_hpd_po\"]).dt.days\n",
    "    df[\"lead_time_po_vs_pr\"] = (df[\"purch_doc_date_hpd_po\"] - df[\"doc_change_date_src_po\"]).dt.days\n",
    "    df[\"po_day_of_week\"] = df[\"purch_doc_date_hpd_po\"].dt.dayofweek\n",
    "    df[\"po_day_of_month\"] = df[\"purch_doc_date_hpd_po\"].dt.day\n",
    "    df[\"po_month\"] = df[\"purch_doc_date_hpd_po\"].dt.month\n",
    "    \n",
    "    # B. Price & Value Features\n",
    "    df[\"price_per_unit\"] = df[\"net_val_po_curr_src_po\"] / df[\"quantity_src_po\"].replace(0, np.nan)\n",
    "    df[\"net_vs_gross_delta\"] = df[\"gross_val_po_curr_src_po\"] - df[\"net_val_po_curr_src_po\"]\n",
    "    df[\"price_variance_percent\"] = (df[\"net_val_po_curr_src_po\"] - df[\"gross_val_po_curr_src_po\"]) / df[\"gross_val_po_curr_src_po\"].replace(0, np.nan)\n",
    "    df[\"outline_agrmt_coverage\"] = df[\"outline_agrmt_tgt_val_doc_curr_src_po\"] / df[\"net_val_po_curr_src_po\"].replace(0, np.nan)\n",
    "    \n",
    "    # C. PO-PR Linkage Flags\n",
    "    df[\"has_pr_link\"] = df[\"pr_no_src_po\"].notna().astype(int)\n",
    "    df[\"has_pr_item_link\"] = df[\"pr_item_no_src_po\"].notna().astype(int)\n",
    "    \n",
    "    # D. Flags & Indicators\n",
    "    binary_cols = [\n",
    "        \"gr_indicator_src_po\", \"gr_invoice_verif_flag_src_po\",\n",
    "        \"inv_receipt_indicator_src_po\", \"release_indicator_hpd_po\",\n",
    "        \"release_status_hpd_po\", \"doc_release_incompl_flag_hpd_po\"\n",
    "    ]\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            df[col + \"_flag\"] = df[col].notna().astype(int)\n",
    "    \n",
    "    # E. Missing Signal\n",
    "    critical_cols = [\n",
    "        \"material_type_src_po\", \"material_no_src_po\",\n",
    "        \"vendor_or_creditor_acct_no_hpd_po\", \"gross_val_po_curr_src_po\",\n",
    "        \"net_price_doc_curr_src_po\"\n",
    "    ]\n",
    "    df[\"missing_critical_fields\"] = df[critical_cols].isnull().sum(axis=1)\n",
    "    \n",
    "    # F. Currency & Exchange Rate\n",
    "    df[\"has_exchange_rate\"] = df[\"exchange_rate_hpd_po\"].notna().astype(int)\n",
    "    df[\"log_exchange_rate\"] = np.log1p(df[\"exchange_rate_hpd_po\"].fillna(0))\n",
    "    \n",
    "    # G. Unit Conversion Features\n",
    "    df[\"p2o_conversion_ratio\"] = df[\"p2o_unit_conv_num_src_po\"] / df[\"p2o_unit_conv_denom_src_po\"].replace(0, np.nan)\n",
    "    df[\"o2b_conversion_ratio\"] = df[\"o2b_unit_conv_num_src_po\"] / df[\"o2b_unit_conv_denom_src_po\"].replace(0, np.nan)\n",
    "    \n",
    "    # H. Behavioral Flags\n",
    "    df[\"is_same_vendor_pr_po\"] = (\n",
    "        df[\"vendor_or_creditor_acct_no_hpd_po\"].notna() & df[\"base_id_src_po\"].notna()\n",
    "    ).astype(int)\n",
    "    df[\"has_rfq_status\"] = df[\"rfq_status_hpd_po\"].notna().astype(int)\n",
    "    df[\"purch_group_org_same\"] = (df[\"purch_group_hpd_po\"] == df[\"purch_org_hpd_po\"]).astype(int)\n",
    "    \n",
    "    # I. Rare Category Flagging\n",
    "    rare_cat_cols = [\"purch_doc_type_hpd_po\", \"purch_group_hpd_po\", \"vendor_or_creditor_acct_no_hpd_po\"]\n",
    "    for col in rare_cat_cols:\n",
    "        if col in df.columns:\n",
    "            freq_map = df[col].value_counts(normalize=True)\n",
    "            df[f\"{col}_is_rare\"] = df[col].map(freq_map) < 0.01\n",
    "\n",
    "    return df\n",
    "\n",
    "##############################################################################\n",
    "#  Cell 3 â€” Create / load features\n",
    "##############################################################################\n",
    "if FEAT_PO.exists():\n",
    "    print(\"âš¡ Loading cached features\")\n",
    "    df = pd.read_pickle(FEAT_PO)\n",
    "else:\n",
    "    print(\"ðŸš§ Generating features â€¦\")\n",
    "    raw_df  = pd.read_pickle(RAW_PO)\n",
    "    columns_to_drop = [\n",
    "    'base_id_po','rule_ids_po','P2P02067','P2P02068','P2P02070','P2P02072']\n",
    "    raw_df.drop(columns=[col for col in columns_to_drop if col in raw_df.columns], inplace=True)\n",
    "    feat_df = build_features(raw_df)\n",
    "    df=flag_split_po(feat_df)\n",
    "    df=flag_intra_po_split(df)\n",
    "    df = flag_multiple_pos_per_pr_item(df)\n",
    "    # Same vendor price jump\n",
    "    df = flag_same_vendor_price_increase(df, months_range=6, flag_column='same_vendor_price_increase_6m_flag')\n",
    "    df = flag_same_vendor_price_increase(df, months_range=12, flag_column='same_vendor_price_increase_12m_flag')\n",
    "    \n",
    "    # Diff vendor price variance\n",
    "    df = flag_diff_vendor_price_variance(df, months_range=6, flag_column='diff_vendor_price_variance_6m_flag')\n",
    "    df = flag_diff_vendor_price_variance(df, months_range=12, flag_column='diff_vendor_price_variance_12m_flag')\n",
    "    df=final_parsing(df)\n",
    "    df.to_pickle(FEAT_PO)\n",
    "    print(\"Feature shape:\", df.shape)\n",
    "\n",
    "##############################################################################\n",
    "#  Cell 4 â€” Fast train (no hyper-parameter search, no RFECV)\n",
    "##############################################################################\n",
    "target = \"rft_by_engine_po\"\n",
    "y = df[target].fillna(0).astype(int)\n",
    "X = df.drop(columns=[target])\n",
    "\n",
    "# ---------- Pre-processor ----------\n",
    "num_cols = [c for c in X if X[c].dtype.kind in \"if\"]\n",
    "cat_cols = [c for c in X if X[c].dtype.kind not in \"if\"]\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                      (\"scaler\",  StandardScaler(with_mean=False))]), num_cols),\n",
    "    (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                      (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\",\n",
    "                                            min_frequency=10))]), cat_cols)\n",
    "])\n",
    "\n",
    "# ---------- Baseline model ----------\n",
    "gbc = GradientBoostingClassifier(\n",
    "    n_estimators = 300,     # balanced speed vs. accuracy\n",
    "    learning_rate = 0.05,\n",
    "    max_depth = 3,\n",
    "    subsample = 0.8,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"pre\",  pre),\n",
    "    (\"clf\",  gbc)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Simple hold-out evaluation (20 % split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, stratify=y,\n",
    "                                          test_size=0.2, random_state=99)\n",
    "\n",
    "pipe.fit(X_tr, y_tr)\n",
    "\n",
    "# ---------- Save & quick metrics ----------\n",
    "#MODEL_PKL.parent.mkdir(exist_ok=True)\n",
    "#joblib.dump(pipe, MODEL_PKL)\n",
    "#print(\"âœ… Baseline model saved:\", MODEL_PKL)\n",
    "\n",
    "y_pred = pipe.predict(X_te)\n",
    "y_prob = pipe.predict_proba(X_te)[:, 1]\n",
    "print(classification_report(y_te, y_pred))\n",
    "print(\"Hold-out ROC-AUC:\", roc_auc_score(y_te, y_prob))\n",
    "\n",
    "\n",
    "# === STEP 8: Confusion Matrix ===\n",
    "cm = confusion_matrix(y_te, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e754a1-1dc6-47ef-8b42-3cba0d854bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X)\n",
    "y_prob = pipe.predict_proba(X)[:, 1]\n",
    "print(classification_report(y, y_pred))\n",
    "print(\"Hold-out ROC-AUC:\", roc_auc_score(y, y_prob))\n",
    "\n",
    "# === STEP 8: Confusion Matrix ===\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc63618-f3c4-48b4-a2c4-4fbef1ac5ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Cell 7 â€” Feature-importance inspection\n",
    "##############################################################################\n",
    "from pathlib import Path\n",
    "import joblib, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MODEL_PKL = Path(\"Po_Invoice_Data/po_gbdt_model.pkl\")\n",
    "FEATURE_PKL = Path(\"Po_Invoice_Data/po_output_features_df_model.pkl\")\n",
    "TARGET = \"rft_by_engine_po\"\n",
    "\n",
    "# 1. Load model & feature frame\n",
    "pipe = joblib.load(MODEL_PKL)\n",
    "feat_df = pd.read_pickle(FEATURE_PKL)\n",
    "\n",
    "# 2. Reconstruct original column lists\n",
    "X = feat_df.drop(columns=[TARGET], errors=\"ignore\")\n",
    "num_cols = [c for c in X if X[c].dtype.kind in \"if\"]\n",
    "cat_cols = [c for c in X if X[c].dtype.kind not in \"if\"]\n",
    "\n",
    "# 3. Get names after preprocessing\n",
    "pre = pipe.named_steps[\"pre\"]\n",
    "ohe = pre.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "ohe_feature_names = ohe.get_feature_names_out(cat_cols)\n",
    "\n",
    "feature_names = num_cols + list(ohe_feature_names)\n",
    "\n",
    "# 4. Retrieve importances from the tree model\n",
    "importances = pipe.named_steps[\"clf\"].feature_importances_\n",
    "\n",
    "imp_df = (\n",
    "    pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "      .sort_values(\"importance\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "display(imp_df.head(25))     # top-25 table\n",
    "\n",
    "# 5. Quick bar chart for visual feel\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(imp_df.head(15)[\"feature\"][::-1],\n",
    "         imp_df.head(15)[\"importance\"][::-1])\n",
    "plt.title(\"Top-15 Feature Importances\")\n",
    "plt.xlabel(\"GBDT importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
